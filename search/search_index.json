{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Kolena is a comprehensive machine learning testing and debugging platform to surface hidden model behaviors and take the mystery out of model development. Kolena helps you:</p> <ul> <li>Perform high-resolution model evaluation</li> <li>Understand and track behavioral improvements and regressions</li> <li>Meaningfully communicate model capabilities</li> <li>Automate model testing and deployment workflows</li> </ul> <p>This <code>kolena</code> package contains the Python client library for programmatic interaction with the Kolena ML testing platform.</p>"},{"location":"reference/errors/","title":"<code>kolena.errors</code>","text":""},{"location":"reference/errors/#kolena.errors.KolenaError","title":"<code>KolenaError</code>","text":"<p>         Bases: <code>Exception</code></p> <p>Base error for all Kolena errors to extend. Allows consumers to catch Kolena specific errors.</p>"},{"location":"reference/errors/#kolena.errors.InputValidationError","title":"<code>InputValidationError</code>","text":"<p>         Bases: <code>ValueError</code>, <code>KolenaError</code></p> <p>Exception indicating that provided input data failed validation.</p>"},{"location":"reference/errors/#kolena.errors.IncorrectUsageError","title":"<code>IncorrectUsageError</code>","text":"<p>         Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that the user performed a disallowed action with the client.</p>"},{"location":"reference/errors/#kolena.errors.InvalidTokenError","title":"<code>InvalidTokenError</code>","text":"<p>         Bases: <code>ValueError</code>, <code>KolenaError</code></p> <p>Exception indicating that provided token value was invalid.</p>"},{"location":"reference/errors/#kolena.errors.InvalidClientStateError","title":"<code>InvalidClientStateError</code>","text":"<p>         Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that client state was invalid.</p>"},{"location":"reference/errors/#kolena.errors.UninitializedError","title":"<code>UninitializedError</code>","text":"<p>         Bases: <code>InvalidClientStateError</code></p> <p>Exception indicating that the client has not been properly initialized before usage.</p>"},{"location":"reference/errors/#kolena.errors.DirectInstantiationError","title":"<code>DirectInstantiationError</code>","text":"<p>         Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that the default constructor was used for a class that does not support direct instantiation. Available static constructors should be used when this exception is encountered.</p>"},{"location":"reference/errors/#kolena.errors.FrozenObjectError","title":"<code>FrozenObjectError</code>","text":"<p>         Bases: <code>RuntimeError</code>, <code>KolenaError</code></p> <p>Exception indicating that the user attempted to modify a frozen object.</p>"},{"location":"reference/errors/#kolena.errors.UnauthenticatedError","title":"<code>UnauthenticatedError</code>","text":"<p>         Bases: <code>HTTPError</code>, <code>KolenaError</code></p> <p>Exception indicating unauthenticated usage of the client.</p>"},{"location":"reference/errors/#kolena.errors.RemoteError","title":"<code>RemoteError</code>","text":"<p>         Bases: <code>HTTPError</code>, <code>KolenaError</code></p> <p>Exception indicating that a remote error occurred in communications between the Kolena client and server.</p>"},{"location":"reference/errors/#kolena.errors.CustomMetricsException","title":"<code>CustomMetricsException</code>","text":"<p>         Bases: <code>KolenaError</code></p> <p>Exception indicating that there's an error when computing custom metrics.</p>"},{"location":"reference/errors/#kolena.errors.WorkflowMismatchError","title":"<code>WorkflowMismatchError</code>","text":"<p>         Bases: <code>KolenaError</code></p> <p>Exception indicating a workflow mismatch.</p>"},{"location":"reference/errors/#kolena.errors.NotFoundError","title":"<code>NotFoundError</code>","text":"<p>         Bases: <code>RemoteError</code></p> <p>Exception indicating an entity is not found</p>"},{"location":"reference/errors/#kolena.errors.NameConflictError","title":"<code>NameConflictError</code>","text":"<p>         Bases: <code>RemoteError</code></p> <p>Exception indicating the name of an entity is conflict</p>"},{"location":"reference/initialize/","title":"<code>kolena.initialize</code>","text":""},{"location":"reference/initialize/#kolena.initialize.initialize","title":"<code>initialize(api_token, *args, verbose=False, proxies=None, **kwargs)</code>","text":"<p>Initialize a client session.</p> <p>A session has a global scope and remains active until interpreter shutdown.</p> <p>Note</p> <p>As of version 0.29.0: the entity param is no longer needed; <code>initialize(entity, token)</code> is deprecated and replaced by <code>initialize(token)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>api_token</code> <code>str</code> <p>Provided API token. This token is a secret and should be treated with caution.</p> required <code>verbose</code> <code>bool</code> <p>Optionally configure client to run in verbose mode, providing more information about execution. All logging events are emitted as Python standard library <code>logging</code> events from the <code>\"kolena\"</code> logger as well as to stdout/stderr directly.</p> <code>False</code> <code>proxies</code> <code>Optional[Dict[str, str]]</code> <p>Optionally configure client to run with <code>http</code> or <code>https</code> proxies. The <code>proxies</code> parameter is passed through to the <code>requests</code> package and can be configured accordingly.</p> <code>None</code> <p>Raises:</p> Type Description <code>InvalidTokenError</code> <p>The provided <code>api_token</code> is not valid.</p> <code>InputValidationError</code> <p>The provided combination or number of args is not valid.</p> Source code in <code>kolena/initialize.py</code> <pre><code>def initialize(\napi_token: str,\n*args: Any,\nverbose: bool = False,\nproxies: Optional[Dict[str, str]] = None,\n**kwargs: Any,\n) -&gt; None:\n\"\"\"\n    Initialize a client session.\n    A session has a global scope and remains active until interpreter shutdown.\n    !!! note\n        As of version 0.29.0: the entity param is no longer needed; `initialize(entity, token)` is\n        **deprecated** and replaced by `initialize(token)`.\n    :param api_token: Provided API token. This token is a secret and should be treated with caution.\n    :param verbose: Optionally configure client to run in verbose mode, providing more information about execution. All\n        logging events are emitted as Python standard library `logging` events from the `\"kolena\"` logger as well as\n        to stdout/stderr directly.\n    :param proxies: Optionally configure client to run with `http` or `https` proxies. The `proxies` parameter\n        is passed through to the `requests` package and can be\n        [configured accordingly](https://requests.readthedocs.io/en/latest/user/advanced/#proxies).\n    :raises InvalidTokenError: The provided `api_token` is not valid.\n    :raises InputValidationError: The provided combination or number of args is not valid.\n    \"\"\"\nused_deprecated_signature = False\nif len(args) &gt; 1:\nraise InputValidationError(f\"Too many args. Expected 0 or 1 but got {len(args)} Check docs for usage.\")\nelif len(args) == 1:\n# overwrite the originally passed api_token since we are supporting backward compatability with entity\napi_token = args[0]\nif len(args) == 1 or \"entity\" in kwargs:\nused_deprecated_signature = True\nwarnings.warn(\n\"The signature initialize(entity, token) is deprecated. Please update to initialize(token).\",\ncategory=DeprecationWarning,\nstacklevel=2,\n)\ninit_response = state.get_token(api_token, proxies=proxies)\nderived_telemetry = init_response.tenant_telemetry\n_client_state.update(\napi_token=api_token,\njwt_token=init_response.access_token,\ntenant=init_response.tenant,\nverbose=verbose,\ntelemetry=derived_telemetry,\nproxies=proxies,\n)\nif used_deprecated_signature:\nupload_log(\"Client attempted to use deprecated entity auth signature.\", \"warn\")\nlog.info(\"initialized\")\nif verbose:\n# Configure third party logging based on verbosity\npd.set_option(\"display.max_colwidth\", None)\nlog.info(f\"connected to {get_platform_url()}\")\n</code></pre>"},{"location":"reference/built-in/base/","title":"Base Definitions","text":"<p>Legacy Warning: Deprecated Module</p> <p>This module has been deprecated. Consider using <code>kolena.workflow</code> instead.</p> <p>Base definitions shared between <code>kolena.classification</code> and <code>kolena.detection</code>.</p>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase","title":"<code>BaseTestCase(name, workflow, version=None, description=None, images=None, reset=False)</code>","text":"<p>         Bases: <code>ABC</code>, <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A test case holds a set of images to compute performance metrics against.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the test case to create or load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify the version of the test case to load. When absent, the latest version is loaded. Ignored when creating new test cases.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Optionally specify a description for a newly created test case. For existing test cases, this description can be edited via <code>TestCase.edit</code>.</p> <code>None</code> <code>images</code> <code>Optional[List[_TestImageClass]]</code> <p>Optionally provide a list of images and associated ground truths to populate a new test case. For existing test cases, images can be edited via <code>TestCase.edit</code>.</p> <code>None</code> Source code in <code>kolena/detection/_internal/test_case.py</code> <pre><code>def __init__(\nself,\nname: str,\nworkflow: WorkflowType,\nversion: Optional[int] = None,\ndescription: Optional[str] = None,\nimages: Optional[List[_TestImageClass]] = None,\nreset: bool = False,\n):\ntry:\nself._populate_from_other(self.load(name, version))\nif description is not None and self.description != description and not reset:\nlog.warn(\"test case already exists, not updating description when reset=False\")\nif images is not None:\nif self.version &gt; 0 and not reset:\nlog.warn(\"not updating images for test case that has already been edited when reset=False\")\nelse:\nself._hydrate(images, description)\nexcept NotFoundError:\nif version is not None:\nlog.warn(f\"creating new test case '{name}', ignoring provided version\")\nself._populate_from_other(self._create(workflow, name, description, images))\nself._freeze()\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>The unique name of this test case.</p>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of this test case. A test case's version is automatically incremented whenever it is edited via <code>TestCase.edit</code>.</p>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form, human-readable description of this test case. Can be edited at any time via <code>TestCase.edit</code>.</p>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase.Editor","title":"<code>Editor(description, reset)</code>","text":"<p>Interface to edit a test case. Create with <code>TestCase.edit</code>.</p> Source code in <code>kolena/detection/_internal/test_case.py</code> <pre><code>def __init__(self, description: str, reset: bool) -&gt; None:\nself._edited = False\nself._reset = reset\nself._description = description\nself._initial_description = description\nself._images: Dict[str, BaseTestImage] = OrderedDict()\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase.Editor.description","title":"<code>description(description)</code>","text":"<p>Update the description of this test case.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>The new test case description.</p> required Source code in <code>kolena/detection/_internal/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef description(self, description: str) -&gt; None:\n\"\"\"\n    Update the description of this test case.\n    :param description: The new test case description.\n    \"\"\"\nif self._description == description:\nreturn\nself._description = description\nself._edited = True\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase.Editor.add","title":"<code>add(image)</code>","text":"<p>Add a test image to the test case, targeting the <code>ground_truths</code> held by the image. When the test image already exists in the test case, its ground truth is overwritten.</p> <p>To filter the ground truths associated with a test image, see <code>TestImage.filter</code>.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>_TestImageClass</code> <p>The test image to add to the test case, holding corresponding ground truths.</p> required Source code in <code>kolena/detection/_internal/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef add(self, image: _TestImageClass) -&gt; None:\n\"\"\"\n    Add a test image to the test case, targeting the `ground_truths` held by the image. When the test image\n    already exists in the test case, its ground truth is overwritten.\n    To filter the ground truths associated with a test image, see\n    [`TestImage.filter`][kolena.detection.TestImage.filter].\n    :param image: The test image to add to the test case, holding corresponding ground truths.\n    \"\"\"\nif image == self._images.get(image.locator, None):\nlog.info(f\"no op: {image.locator} already in test case\")\nreturn\nself._images[image.locator] = image\nself._edited = True\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase.Editor.remove","title":"<code>remove(image)</code>","text":"<p>Remove the image from the test case.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>_TestImageClass</code> <p>The image to remove.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>The image is not in the test case.</p> Source code in <code>kolena/detection/_internal/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef remove(self, image: _TestImageClass) -&gt; None:\n\"\"\"\n    Remove the image from the test case.\n    :param image: The image to remove.\n    :raises KeyError: The image is not in the test case.\n    \"\"\"\nif image.locator not in self._images.keys():\nraise KeyError(f\"unrecognized image: '{image.locator}' not in test case\")\nself._images.pop(image.locator)\nself._edited = True\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase.load_images","title":"<code>load_images()</code>","text":"<p>Load all test images with their associated ground truths in this test case.</p> Source code in <code>kolena/detection/_internal/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef load_images(self) -&gt; List[_TestImageClass]:\n\"\"\"Load all test images with their associated ground truths in this test case.\"\"\"\nreturn list(self.iter_images())\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase.iter_images","title":"<code>iter_images()</code>","text":"<p>Iterate through all images with their associated ground truths in this test case.</p> Source code in <code>kolena/detection/_internal/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef iter_images(self) -&gt; Iterator[_TestImageClass]:\n\"\"\"Iterate through all images with their associated ground truths in this test case.\"\"\"\nlog.info(f\"loading test images for test case '{self.name}' (v{self.version})\")\ninit_request = CoreAPI.InitLoadContentsRequest(batch_size=BatchSize.LOAD_SAMPLES.value, test_case_id=self._id)\nfor df in _BatchedLoader.iter_data(\ninit_request=init_request,\nendpoint_path=API.Path.INIT_LOAD_IMAGES.value,\ndf_class=self._TestImageDataFrameClass,\n):\nfor record in df.itertuples():\nyield self._TestImageClass._from_record(record)\nlog.info(f\"loaded test images for test case '{self.name}' (v{self.version})\")\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase.create","title":"<code>create(name, description=None, images=None)</code>  <code>classmethod</code>","text":"<p>Create a new test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new test case to create.</p> required <code>description</code> <code>Optional[str]</code> <p>Optional free-form description of the test case to create.</p> <code>None</code> <code>images</code> <code>Optional[List[_TestImageClass]]</code> <p>Optionally specify a set of images to populate the test case.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseTestCase</code> <p>The newly created test case.</p> Source code in <code>kolena/detection/_internal/test_case.py</code> <pre><code>@classmethod\ndef create(\ncls,\nname: str,\ndescription: Optional[str] = None,\nimages: Optional[List[_TestImageClass]] = None,\n) -&gt; \"BaseTestCase\":\n\"\"\"\n    Create a new test case with the provided name.\n    :param name: The name of the new test case to create.\n    :param description: Optional free-form description of the test case to create.\n    :param images: Optionally specify a set of images to populate the test case.\n    :return: The newly created test case.\n    \"\"\"\nreturn cls._create(cls._workflow, name, description, images)\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase.load","title":"<code>load(name, version=None)</code>  <code>classmethod</code>","text":"<p>Load an existing test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test case to load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify a particular version of the test case to load. Defaults to the latest version when unset.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseTestCase</code> <p>The loaded test case.</p> Source code in <code>kolena/detection/_internal/test_case.py</code> <pre><code>@classmethod\ndef load(cls, name: str, version: Optional[int] = None) -&gt; \"BaseTestCase\":\n\"\"\"\n    Load an existing test case with the provided name.\n    :param name: The name of the test case to load.\n    :param version: Optionally specify a particular version of the test case to load. Defaults to the latest version\n        when unset.\n    :return: The loaded test case.\n    \"\"\"\ndata = cls._load_by_name(name, version)\nreturn cls._create_from_data(data)\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_case.BaseTestCase.edit","title":"<code>edit(reset=False)</code>","text":"<p>Edit this test case in a context:</p> <pre><code>with test_case.edit() as editor:\n# perform as many editing actions as desired\neditor.add(...)\neditor.remove(...)\n</code></pre> <p>Changes are committed to Kolena when the context is exited.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>Clear any and all test samples currently in the test case.</p> <code>False</code> Source code in <code>kolena/detection/_internal/test_case.py</code> <pre><code>@contextmanager\ndef edit(self, reset: bool = False) -&gt; Iterator[Editor]:\n\"\"\"\n    Edit this test case in a context:\n    ```python\n    with test_case.edit() as editor:\n        # perform as many editing actions as desired\n        editor.add(...)\n        editor.remove(...)\n    ```\n    Changes are committed to Kolena when the context is exited.\n    :param reset: Clear any and all test samples currently in the test case.\n    \"\"\"\neditor = self.Editor(self.description, reset)\neditor._TestImageClass = self._TestImageClass\nif not reset:\nfor image in self.iter_images():\neditor.add(image)\neditor._edited = False\nyield editor\n# no-op contexts have no effect, do not bump version\nif not editor._edited:\nreturn\nlog.info(f\"editing test case '{self.name}' (v{self.version})\")\ninit_response = init_upload()\ndf = self._to_data_frame(list(editor._images.values()))\ndf_serialized = df.as_serializable()\nupload_data_frame(df=df_serialized, batch_size=BatchSize.UPLOAD_RECORDS.value, load_uuid=init_response.uuid)\nrequest = CoreAPI.CompleteEditRequest(\ntest_case_id=self._id,\ncurrent_version=self.version,\ndescription=editor._description,\nreset=editor._reset,\nuuid=init_response.uuid,\n)\ncomplete_res = krequests.put(\nendpoint_path=API.Path.COMPLETE_EDIT.value,\ndata=json.dumps(dataclasses.asdict(request)),\n)\nkrequests.raise_for_status(complete_res)\ntest_case_data = from_dict(data_class=CoreAPI.EntityData, data=complete_res.json())\nself._populate_from_other(self._create_from_data(test_case_data))\nlog.success(f\"edited test case '{self.name}' (v{self.version})\")\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_suite.BaseTestSuite","title":"<code>BaseTestSuite(name, workflow, version=None, description=None, test_cases=None, reset=False)</code>","text":"<p>         Bases: <code>ABC</code>, <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A test suite groups together one or more test cases.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test suite to create or load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify the version of the test suite to load. When absent, the latest version is loaded. Ignored when creating new test suites.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Optionally specify a description for a newly created test suite. For existing test suites, this description can be edited via <code>TestSuite.edit</code>.</p> <code>None</code> <code>test_cases</code> <code>Optional[List[BaseTestCase]]</code> <p>optionally specify a list of test cases to populate a new test suite. For existing test suites, test cases can be edited via <code>TestSuite.edit</code>.</p> <code>None</code> Source code in <code>kolena/detection/_internal/test_suite.py</code> <pre><code>def __init__(\nself,\nname: str,\nworkflow: WorkflowType,\nversion: Optional[int] = None,\ndescription: Optional[str] = None,\ntest_cases: Optional[List[BaseTestCase]] = None,\nreset: bool = False,\n):\nself._validate_test_cases(test_cases)\ntry:\nself._populate_from_other(self.load(name, version))\nif description is not None and self.description != description and not reset:\nlog.warn(\"test suite already exists, not updating description when reset=False\")\nif test_cases is not None:\nif self.version &gt; 0 and not reset:\nlog.warn(\"test suite already exists, not updating test cases when reset=False\")\nelse:\nself._hydrate(test_cases, description)\nexcept NotFoundError:\nif version is not None:\nlog.warn(f\"creating new test suite '{name}', ignoring provided version\")\nself._populate_from_other(self._create(workflow, name, description, test_cases))\nself._freeze()\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_suite.BaseTestSuite.Editor","title":"<code>Editor(test_cases, description, reset)</code>","text":"<p>Interface to edit a test suite. Create with <code>TestSuite.edit</code>.</p> Source code in <code>kolena/detection/_internal/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, test_cases: List[BaseTestCase], description: str, reset: bool) -&gt; None:\nself._test_cases: Dict[str, int] = OrderedDict()  # map from name -&gt; id\nself._reset = reset\nself._description = description\nself._initial_test_case_ids = {tc._id for tc in test_cases}\nself._initial_description = description\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_suite.BaseTestSuite.Editor.description","title":"<code>description(description)</code>","text":"<p>Update the description of the test suite.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>The new description of the test suite.</p> required Source code in <code>kolena/detection/_internal/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef description(self, description: str) -&gt; None:\n\"\"\"\n    Update the description of the test suite.\n    :param description: The new description of the test suite.\n    \"\"\"\nself._description = description\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_suite.BaseTestSuite.Editor.add","title":"<code>add(test_case)</code>","text":"<p>Add the provided <code>TestCase</code> to the test suite. If a different version of the test case already exists in this test suite, it is replaced.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>BaseTestCase</code> <p>The test case to add to the test suite.</p> required Source code in <code>kolena/detection/_internal/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef add(self, test_case: BaseTestCase) -&gt; None:\n\"\"\"\n    Add the provided [`TestCase`][kolena.detection.TestCase] to the test suite.\n    If a different version of the test case already exists in this test suite, it is replaced.\n    :param test_case: The test case to add to the test suite.\n    \"\"\"\nself._assert_workflows_match(test_case)\nself._test_cases[test_case.name] = test_case._id\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_suite.BaseTestSuite.Editor.remove","title":"<code>remove(test_case)</code>","text":"<p>Remove the provided <code>TestCase</code> from the test suite. Any version of this test case in this test suite will be removed; the version does not need to match exactly.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>BaseTestCase</code> <p>The test case to be removed from the test suite.</p> required Source code in <code>kolena/detection/_internal/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef remove(self, test_case: BaseTestCase) -&gt; None:\n\"\"\"\n    Remove the provided [`TestCase`][kolena.detection.TestCase] from the test suite. Any version of this test\n    case in this test suite will be removed; the version does not need to match exactly.\n    :param test_case: The test case to be removed from the test suite.\n    \"\"\"\nname = test_case.name\nif name not in self._test_cases.keys():\nraise KeyError(f\"test case '{name}' not in test suite\")\nself._test_cases.pop(name)\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_suite.BaseTestSuite.Editor.merge","title":"<code>merge(test_case)</code>","text":"<p>Add the provided <code>TestCase</code> to the test suite, replacing any previous version of the test case that may be present in the suite.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>BaseTestCase</code> <p>The test case to be merged into the test suite.</p> required Source code in <code>kolena/detection/_internal/test_suite.py</code> <pre><code>@deprecated(details=\"use :meth:`add` instead\", deprecated_in=\"0.56.0\")\n@validate_arguments(config=ValidatorConfig)\ndef merge(self, test_case: BaseTestCase) -&gt; None:\n\"\"\"\n    Add the provided [`TestCase`][kolena.detection.TestCase] to the test suite, replacing any previous version\n    of the test case that may be present in the suite.\n    :param test_case: The test case to be merged into the test suite.\n    \"\"\"\nself.add(test_case)\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_suite.BaseTestSuite.create","title":"<code>create(name, description=None, test_cases=None)</code>  <code>classmethod</code>","text":"<p>Create a new test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new test suite to create.</p> required <code>description</code> <code>Optional[str]</code> <p>Optional free-form description of the test suite to create.</p> <code>None</code> <code>test_cases</code> <code>Optional[List[BaseTestCase]]</code> <p>Optionally specify a set of test cases to populate the test suite.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseTestSuite</code> <p>The newly created test suite.</p> Source code in <code>kolena/detection/_internal/test_suite.py</code> <pre><code>@classmethod\ndef create(\ncls,\nname: str,\ndescription: Optional[str] = None,\ntest_cases: Optional[List[BaseTestCase]] = None,\n) -&gt; \"BaseTestSuite\":\n\"\"\"\n    Create a new test suite with the provided name.\n    :param name: The name of the new test suite to create.\n    :param description: Optional free-form description of the test suite to create.\n    :param test_cases: Optionally specify a set of test cases to populate the test suite.\n    :return: The newly created test suite.\n    \"\"\"\nreturn cls._create(cls._workflow, name, description, test_cases)\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_suite.BaseTestSuite.load","title":"<code>load(name, version=None)</code>  <code>classmethod</code>","text":"<p>Load an existing test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test suite to load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify a particular version of the test suite to load. Defaults to the latest version when unset.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseTestSuite</code> <p>The loaded test suite.</p> Source code in <code>kolena/detection/_internal/test_suite.py</code> <pre><code>@classmethod\ndef load(cls, name: str, version: Optional[int] = None) -&gt; \"BaseTestSuite\":\n\"\"\"\n    Load an existing test suite with the provided name.\n    :param name: The name of the test suite to load.\n    :param version: Optionally specify a particular version of the test suite to load. Defaults to the latest\n        version when unset.\n    :return: The loaded test suite.\n    \"\"\"\ndata = cls._load_by_name(name, version)\nobj = cls._create_from_data(data)\nlog.info(f\"loaded test suite '{name}' (v{obj.version}) ({get_test_suite_url(obj._id)})\")\nreturn obj\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_suite.BaseTestSuite.edit","title":"<code>edit(reset=False)</code>","text":"<p>Edit this test suite in a context:</p> <pre><code>with test_suite.edit() as editor:\n# perform as many editing actions as desired\neditor.add(...)\neditor.remove(...)\n</code></pre> <p>Changes are committed to Kolena when the context is exited.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>Clear any and all test cases currently in the test suite.</p> <code>False</code> <p>Returns:</p> Type Description <code>Iterator[Editor]</code> <p>Context-managed <code>TestSuite.Editor</code> instance exposing editing functionality for this test suite.</p> Source code in <code>kolena/detection/_internal/test_suite.py</code> <pre><code>@contextmanager\ndef edit(self, reset: bool = False) -&gt; Iterator[Editor]:\n\"\"\"\n    Edit this test suite in a context:\n    ```python\n    with test_suite.edit() as editor:\n        # perform as many editing actions as desired\n        editor.add(...)\n        editor.remove(...)\n    ```\n    Changes are committed to Kolena when the context is exited.\n    :param reset: Clear any and all test cases currently in the test suite.\n    :return: Context-managed [`TestSuite.Editor`][kolena.detection._internal.test_suite.BaseTestSuite.Editor]\n        instance exposing editing functionality for this test suite.\n    \"\"\"\neditor = BaseTestSuite.Editor(self.test_cases, self.description, reset)\neditor._workflow = self._workflow  # set outside of init such that parameter does not leak into documentation\nif not reset:\neditor.description(self.description)\nfor test_case in self.test_cases:\neditor.add(test_case)\nyield editor\nif not editor._edited():\nlog.info(\"no op: nothing edited\")\nreturn\nlog.info(f\"editing test suite '{self.name}' (v{self.version})\")\nrequest = CoreAPI.TestSuite.EditRequest(\ntest_suite_id=self._id,\ncurrent_version=self.version,\nname=self.name,\ndescription=editor._description,\ntest_case_ids=list(editor._test_cases.values()),\n)\ndata = json.dumps(dataclasses.asdict(request))\nres = krequests.post(endpoint_path=API.Path.EDIT.value, data=data)\nkrequests.raise_for_status(res)\ntest_suite_data = from_dict(data_class=CoreAPI.TestSuite.EntityData, data=res.json())\nlog.success(f\"edited test suite '{self.name}' (v{self.version}) ({get_test_suite_url(test_suite_data.id)})\")\nwith self._unfrozen():\nself.version = test_suite_data.version\nself.description = test_suite_data.description\nself.test_cases = [self._test_case_from(tc) for tc in test_suite_data.test_cases]\nself._id = test_suite_data.id\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.model.BaseModel","title":"<code>BaseModel(name, workflow, metadata=None)</code>","text":"<p>         Bases: <code>ABC</code>, <code>Frozen</code>, <code>WithTelemetry</code></p> Source code in <code>kolena/detection/_internal/model.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, name: str, workflow: WorkflowType, metadata: Optional[Dict[str, Any]] = None):\ntry:\nloaded = self._load_by_name(name)\nif len(loaded.metadata) &gt; 0 and loaded.metadata != metadata:\nlog.warn(f\"mismatch in model metadata, using loaded metadata (loaded: {loaded.metadata})\")\nexcept NotFoundError:\nloaded = self._create(workflow, name, metadata or {})\nself.name = name\nself.metadata = loaded.metadata\nself._id = loaded.id\nself._workflow = WorkflowType(loaded.workflow)\nself._freeze()\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.model.BaseModel.load_inferences","title":"<code>load_inferences(test_object)</code>","text":"<p>Retrieve the uploaded inferences with identical ground truth labels for each image in a test case or test suite.</p> Source code in <code>kolena/detection/_internal/model.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef load_inferences(\nself,\ntest_object: Union[_TestCaseClass, _TestSuiteClass],\n) -&gt; List[Tuple[_TestImageClass, Optional[List[_InferenceClass]]]]:\n\"\"\"\n    Retrieve the uploaded inferences with identical ground truth labels for each image in a test case or test suite.\n    \"\"\"\nreturn list(self.iter_inferences(test_object))\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.model.BaseModel.iter_inferences","title":"<code>iter_inferences(test_object)</code>","text":"<p>Iterate the uploaded inferences with identical ground truth labels for each image in a test case or test suite.</p> Source code in <code>kolena/detection/_internal/model.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef iter_inferences(\nself,\ntest_object: Union[_TestCaseClass, _TestSuiteClass],\n) -&gt; Iterator[Tuple[_TestImageClass, Optional[List[_InferenceClass]]]]:\n\"\"\"\n    Iterate the uploaded inferences with identical ground truth labels for each image in a test case or test suite.\n    \"\"\"\nfor df_batch in self._iter_inference_batch_for_reference(test_object):\nyield from (self._inferences_from_record(record) for record in df_batch.itertuples())\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.model.BaseModel.load_inferences_by_test_case","title":"<code>load_inferences_by_test_case(test_suite)</code>","text":"<p>Retrieve the uploaded inferences of a test suite for each image, grouped by test case.</p> Source code in <code>kolena/detection/_internal/model.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef load_inferences_by_test_case(\nself,\ntest_suite: _TestSuiteClass,\n) -&gt; Dict[int, List[SampleInferences[_TestImageClass, _InferenceClass]]]:\n\"\"\"Retrieve the uploaded inferences of a test suite for each image, grouped by test case.\"\"\"\nbatches = list(self._iter_inference_batch_for_test_suite(test_suite))\ndf_all = pd.concat(batches)\ndf = pd.DataFrame(columns=[\"test_case_id\", \"test_sample\"])\ndf[\"test_case_id\"] = df_all[\"test_case_id\"]\ndf[\"test_sample\"] = df_all.apply(lambda record: self._inferences_from_record(record), axis=1)\ndf_by_testcase = df.groupby(\"test_case_id\")[\"test_sample\"].agg(list).to_dict()\nreturn df_by_testcase\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_run.BaseTestRun","title":"<code>BaseTestRun(model, test_suite, config=None, custom_metrics_callback=None, reset=False)</code>","text":"<p>         Bases: <code>ABC</code>, <code>Frozen</code>, <code>WithTelemetry</code></p> Source code in <code>kolena/detection/_internal/test_run.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nmodel: BaseModel,\ntest_suite: BaseTestSuite,\nconfig: Optional[Metrics.RunConfig] = None,\ncustom_metrics_callback: Optional[CustomMetricsCallback[_TestImageClass, _InferenceClass]] = None,\nreset: bool = False,\n):\nif model._workflow != test_suite._workflow:\nraise WorkflowMismatchError(\nf\"mismatching test suite workflow for model of type '{model._workflow}': '{test_suite._workflow}'\",\n)\nif reset:\nlog.warn(\"overwriting existing inferences from this model (reset=True)\")\nelse:\nlog.info(\"not overwriting any existing inferences from this model (reset=False)\")\nrequest = API.CreateOrRetrieveRequest(\nmodel_id=model._id,\ntest_suite_ids=[test_suite._id],\nconfig=config,\n)\nres = krequests.post(\nendpoint_path=API.Path.CREATE_OR_RETRIEVE.value,\ndata=json.dumps(dataclasses.asdict(request)),\n)\nkrequests.raise_for_status(res)\nresponse = from_dict(data_class=API.CreateOrRetrieveResponse, data=res.json())\nself._id = response.test_run_id\nself._model = model\nself._test_suite = test_suite\nself._locator_to_image_id: Dict[str, int] = {}\nself._inferences: Dict[int, List[Optional[Inference]]] = OrderedDict()\nself._ignored_image_ids: List[int] = []\nself._upload_uuid: Optional[str] = None\nself._n_inferences = 0\nself._custom_metrics_callback: CustomMetricsCallback = custom_metrics_callback\nself._active = False\nself._reset = reset\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_run.BaseTestRun.add_inferences","title":"<code>add_inferences(image, inferences)</code>","text":"<p>Add inferences for a test image to the test run results.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>_TestImageClass</code> <p>The image that inferences are evaluated on.</p> required <code>inferences</code> <code>Optional[List[_InferenceClass]]</code> <p>List of inferences corresponding to the image.</p> required Source code in <code>kolena/detection/_internal/test_run.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef add_inferences(self, image: _TestImageClass, inferences: Optional[List[_InferenceClass]]) -&gt; None:\n\"\"\"\n    Add inferences for a test image to the test run results.\n    :param image: The image that inferences are evaluated on.\n    :param inferences: List of inferences corresponding to the image.\n    \"\"\"\nself._assert_active()\nif image.locator not in self._locator_to_image_id.keys():\nraise InputValidationError(\nf\"Unrecognized locator '{image.locator}'. Images must be loaded and processed in the same context\",\n)\nimage_id = self._locator_to_image_id[image.locator]\nif inferences is None:\nself._ignored_image_ids.append(image_id)\nself._n_inferences += 1\nelse:\ncontext_image_inferences = self._inferences.get(image_id, [])\nif len(inferences) == 0:\ncontext_image_inferences.append(None)\nself._n_inferences += 1\nelse:\ncontext_image_inferences.extend(inferences)\nself._n_inferences += len(inferences)\nself._inferences[image_id] = context_image_inferences\nif self._n_inferences &gt;= BatchSize.UPLOAD_RESULTS.value:\nself._upload_chunk()\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_run.BaseTestRun.iter_images","title":"<code>iter_images()</code>","text":"<p>Returns an iterator of all remaining images that need inferences evaluated.</p> Source code in <code>kolena/detection/_internal/test_run.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef iter_images(self) -&gt; Iterator[_TestImageClass]:\n\"\"\"Returns an iterator of all remaining images that need inferences evaluated.\"\"\"\nself._assert_active()\nfor df_image_batch in self._iter_image_batch():\nfor record in df_image_batch.itertuples():\nyield self._image_from_load_image_record(record)\n</code></pre>"},{"location":"reference/built-in/base/#kolena.detection._internal.test_run.BaseTestRun.load_images","title":"<code>load_images(batch_size=BatchSize.LOAD_SAMPLES.value)</code>","text":"<p>Returns a list of images that still need inferences evaluated, bounded in count by <code>batch_size</code>. Note that image ground truths will be excluded from the returned batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>The maximum number of images to retrieve.</p> <code>BatchSize.LOAD_SAMPLES.value</code> Source code in <code>kolena/detection/_internal/test_run.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef load_images(self, batch_size: int = BatchSize.LOAD_SAMPLES.value) -&gt; List[_TestImageClass]:\n\"\"\"\n    Returns a list of images that still need inferences evaluated, bounded in count by `batch_size`. Note that image\n    ground truths will be excluded from the returned batch of images.\n    :param batch_size: The maximum number of images to retrieve.\n    \"\"\"\nself._assert_active()\nlog.info(\"loading batch of images for test run\")\ntry:\ndf_image_batch = next(self._iter_image_batch(batch_size=batch_size))\nexcept StopIteration:\n# no more images\nreturn []\nlog.info(\"loaded batch of images for test run\")\nreturn [self._image_from_load_image_record(record) for record in df_image_batch.itertuples()]\n</code></pre>"},{"location":"reference/built-in/classification/","title":"<code>kolena.classification</code>","text":"<p>Legacy Warning: Deprecated Module</p> <p>The built-in <code>kolena.classification</code> module has been deprecated. Consider using <code>kolena.workflow</code> instead.</p>"},{"location":"reference/built-in/classification/#quick-links","title":"Quick Links","text":"<ul> <li><code>kolena.classification.TestImage</code>: create images for testing</li> <li><code>kolena.classification.TestCase</code>: create and manage test cases</li> <li><code>kolena.classification.TestSuite</code>: create and manage test suites</li> <li><code>kolena.classification.TestRun</code>: test models on test suites</li> <li><code>kolena.classification.Model</code>: create models for testing</li> </ul>"},{"location":"reference/built-in/classification/#kolena.classification.TestConfig","title":"<code>TestConfig</code>","text":"<p>         Bases: <code>_TestConfig</code></p> <p>Base class for a testing configuration.</p> <p>See concrete implementations <code>FixedGlobalThreshold</code> and <code>AccuracyOptimal</code> for details.</p>"},{"location":"reference/built-in/classification/#kolena.classification.TestSuite","title":"<code>TestSuite(name, version=None, description=None, test_cases=None, reset=False)</code>","text":"<p>         Bases: <code>BaseTestSuite</code></p> <p>A test suite is a grouping of <code>TestCase</code> objects.</p> <p>Testing on test suites is performed via <code>test</code>. Metrics are computed across all samples in a test suite and also for each individual test case within the suite.</p> <p>For additional functionality, see the associated base class documentation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test suite. If a test suite by this name already exists, that test suite is loaded.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify the version of the test suite to load. Ignored when the a suite by the provided name does not already exist.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Optionally specify a description for the new test suite. Ignored when a test suite with the provided name already exists.</p> <code>None</code> <code>test_cases</code> <code>Optional[List[TestCase]]</code> <p>Optionally provide a list of <code>TestCase</code> tests used to seed a new test suite. Ignored when a test suite with the provided name already exists.</p> <code>None</code> Source code in <code>kolena/classification/test_suite.py</code> <pre><code>def __init__(\nself,\nname: str,\nversion: Optional[int] = None,\ndescription: Optional[str] = None,\ntest_cases: Optional[List[TestCase]] = None,\nreset: bool = False,\n):\nsuper().__init__(name, WorkflowType.CLASSIFICATION, version, description, test_cases, reset)\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.classification.test_suite.TestSuite.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Unique name of the test suite.</p>"},{"location":"reference/built-in/classification/#kolena.classification.test_suite.TestSuite.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of the test suite. Version is automatically incremented whenever the test suite is modified via <code>TestSuite.edit</code>.</p>"},{"location":"reference/built-in/classification/#kolena.classification.test_suite.TestSuite.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form description of this test suite. May be edited at any time via <code>TestSuite.edit</code>.</p>"},{"location":"reference/built-in/classification/#kolena.classification.test_suite.TestSuite.test_cases","title":"<code>test_cases: List[TestCase]</code>  <code>instance-attribute</code>","text":"<p>The <code>TestCase</code> objects in this test suite. May be edited at any time via <code>TestSuite.edit</code>.</p>"},{"location":"reference/built-in/classification/#kolena.classification.TestCase","title":"<code>TestCase(name, version=None, description=None, images=None, reset=False)</code>","text":"<p>         Bases: <code>BaseTestCase</code></p> <p>A test case is the base grouping of test data in the Kolena platform.</p> <p>Fundamentally, a test case can be thought of as a benchmark dataset. Metrics are computed for each test case.</p> <p>A test case may be as large or as small as necessary. A test case may have millions of images for high-level results across a large population. Alternatively, a test case may have only one or a handful of images for laser-focus on a specific scenario.</p> <p>For additional functionality, see the associated base class documentation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test case. If a test case by this name already exists, that test case is loaded.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify the version of the test case to load. Ignored when a test case by the provided name does not already exist.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Optionally specify a description for the new test case. Ignored when a test case with the provided name already exists.</p> <code>None</code> <code>images</code> <code>Optional[List[_TestImageClass]]</code> <p>Optionally provide a list of TestImage images used to seed a new test case. Ignored when a test case with the provided name already exists.</p> <code>None</code> Source code in <code>kolena/classification/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nname: str,\nversion: Optional[int] = None,\ndescription: Optional[str] = None,\nimages: Optional[List[_TestImageClass]] = None,\nreset: bool = False,\n):\nsuper().__init__(name, WorkflowType.CLASSIFICATION, version, description, images, reset)\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.classification.FixedGlobalThreshold","title":"<code>FixedGlobalThreshold(fixed_threshold)</code>","text":"<p>         Bases: <code>TestConfig</code></p> <p>Test configuration that sets the default display threshold in the Kolena UI to be a fixed global threshold for all label classes within the test run.</p> Source code in <code>kolena/classification/test_config.py</code> <pre><code>def __init__(self, fixed_threshold: float):\nif fixed_threshold &lt; 0 or fixed_threshold &gt; 1:\nraise InputValidationError(f\"threshold of {fixed_threshold} was not between 0 and 1\")\nself.fixed_threshold = fixed_threshold\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.classification.test_config.FixedGlobalThreshold.fixed_threshold","title":"<code>fixed_threshold: float = fixed_threshold</code>  <code>instance-attribute</code>","text":"<p>The threshold used as the default when visualizing results in Kolena. Must be between 0 and 1.</p>"},{"location":"reference/built-in/classification/#kolena.classification.TestImage","title":"<code>TestImage(locator, dataset=None, labels=None, metadata=None)</code>","text":"<p>         Bases: <code>BaseTestImage</code></p> <p>An image with associated ground truth labels for testing.</p> Source code in <code>kolena/classification/test_image.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nlocator: str,\ndataset: Optional[str] = None,\nlabels: Optional[List[str]] = None,\nmetadata: Optional[Dict[str, MetadataElement]] = None,\n):\nsuper().__init__(locator, dataset, metadata)\nself.labels = labels or []\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.classification.test_image.TestImage.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>Pointer to the bucket location of this image, e.g. <code>gs://my-bucket/my-dataset/example.png</code>.</p>"},{"location":"reference/built-in/classification/#kolena.classification.test_image.TestImage.dataset","title":"<code>dataset: str</code>  <code>instance-attribute</code>","text":"<p>The source dataset this image belongs to.</p>"},{"location":"reference/built-in/classification/#kolena.classification.test_image.TestImage.metadata","title":"<code>metadata: Dict[str, MetadataElement]</code>  <code>instance-attribute</code>","text":"<p>Arbitrary metadata associated with this image. This metadata is surfaced during testing and may be used as model inputs as necessary.</p> <p>Certain metadata values can be visualized in the web platform when viewing results:</p> <ul> <li><code>Annotation</code> objects are overlaid on the main image</li> <li><code>Asset</code> objects containing locators pointing to images, e.g.     <code>gs://my-bucket/my-dataset/example-1channel.png</code>, are displayed</li> </ul> <p>See the metadata documentation for more details.</p>"},{"location":"reference/built-in/classification/#kolena.classification.test_image.TestImage.labels","title":"<code>labels: List[str] = labels or []</code>  <code>instance-attribute</code>","text":"<p>Zero or more ground truth labels for this image. For binary classifiers, an arbitrary string such as <code>positive</code> may be used. Not surfaced during testing.</p>"},{"location":"reference/built-in/classification/#kolena.classification.test_image.TestImage.filter","title":"<code>filter(predicate)</code>","text":"<p>Return a copy of this test image with ground truth labels filtered to only those that match the provided predicate.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>Callable[[str], bool]</code> <p>Function accepting a string label and returning a boolean indicating whether or not to include the ground truth label.</p> required <p>Returns:</p> Type Description <code>TestImage</code> <p>A new test image with labels filtered by the predicate.</p> Source code in <code>kolena/classification/test_image.py</code> <pre><code>def filter(self, predicate: Callable[[str], bool]) -&gt; \"TestImage\":\n\"\"\"\n    Return a copy of this test image with ground truth labels filtered to only those that match the provided\n    predicate.\n    :param predicate: Function accepting a string label and returning a boolean indicating whether or not to include\n        the ground truth label.\n    :return: A new test image with labels filtered by the predicate.\n    \"\"\"\nreturn TestImage(**{**self._fields(), \"labels\": [label for label in self.labels if predicate(label)]})\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.classification.Model","title":"<code>Model(name, metadata=None)</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>The descriptor for a classification model in Kolena.</p> <p>For additional functionality, see the associated base class documentation.</p> Source code in <code>kolena/classification/model.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, name: str, metadata: Optional[Dict[str, Any]] = None):\nsuper().__init__(name, WorkflowType.CLASSIFICATION, metadata)\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.classification.model.Model.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Unique name of the model, potentially containing information about the architecture, training dataset, configuration, framework, commit hash, etc.</p>"},{"location":"reference/built-in/classification/#kolena.classification.model.Model.metadata","title":"<code>metadata: Dict[str, Any]</code>  <code>instance-attribute</code>","text":"<p>Unstructured metadata associated with the model.</p>"},{"location":"reference/built-in/classification/#kolena.classification.TestRun","title":"<code>TestRun(model, test_suite, test_config=None, custom_metrics_callback=None, reset=False)</code>","text":"<p>         Bases: <code>BaseTestRun</code></p> <p>Interface to run tests for a <code>Model</code> on a <code>TestSuite</code>. Any in-progress tests for this model on these suites are resumed.</p> <p>For a streamlined interface, see <code>test</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model being tested.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>test_config</code> <code>Optional[TestConfig]</code> <p>Optionally specify a configuration, e.g. <code>FixedGlobalThreshold</code>, to customize the metrics evaluation logic for this test run. Defaults to <code>AccuracyOptimal</code> if unspecified.</p> <code>None</code> <code>custom_metrics_callback</code> <code>Optional[CustomMetricsCallback[_TestImageClass, _InferenceClass]]</code> <p>Optionally specify a callback function to compute custom metrics for each test-case. The callback would be passed inferences of images in each testcase and should return a dictionary with metric name as key and metric value as value.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>Overwrites existing inferences if set.</p> <code>False</code> Source code in <code>kolena/classification/test_run.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nmodel: Model,\ntest_suite: TestSuite,\ntest_config: Optional[TestConfig] = None,\ncustom_metrics_callback: Optional[CustomMetricsCallback[_TestImageClass, _InferenceClass]] = None,\nreset: bool = False,\n):\nconfig = AccuracyOptimal() if test_config is None else test_config\nsuper().__init__(\nmodel,\ntest_suite,\nconfig=config._to_run_config(),\ncustom_metrics_callback=custom_metrics_callback,\nreset=reset,\n)\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.classification.AccuracyOptimal","title":"<code>AccuracyOptimal()</code>","text":"<p>         Bases: <code>TestConfig</code></p> <p>Test configuration that sets the default display threshold in the Kolena UI to be dynamically set to the threshold that corresponds to the highest accuracy score for the test suite within the test run.</p> <p>This threshold is evaluated and set per label for test suites with multiple label classes.</p> Source code in <code>kolena/classification/test_config.py</code> <pre><code>def __init__(self) -&gt; None:\n...\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.classification.InferenceModel","title":"<code>InferenceModel(name, infer, metadata=None)</code>","text":"<p>         Bases: <code>Model</code></p> <p>A <code>Model</code> with a special <code>infer][kolena.classification.InferenceModel.infer] member performing inference on a provided [</code>TestImage`.</p> Source code in <code>kolena/classification/model.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nname: str,\ninfer: Callable[[TestImage], Optional[List[Tuple[str, float]]]],\nmetadata: Optional[Dict[str, Any]] = None,\n):\nsetattr(self, \"infer\", infer)  # bypass mypy method assignment bug\nsuper().__init__(name, metadata)\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.classification.model.InferenceModel.infer","title":"<code>infer: Callable[[TestImage], Optional[List[Tuple[str, float]]]]</code>  <code>instance-attribute</code>","text":"<p>A function transforming an input <code>TestImage</code> to zero or more <code>(label, confidence)</code> tuples representing model predictions.</p>"},{"location":"reference/built-in/classification/#kolena.classification.test","title":"<code>test(model, test_suite, test_config=None, custom_metrics_callback=None, reset=False)</code>","text":"<p>Test the provided <code>InferenceModel</code> on a <code>TestSuite</code>. Any tests already in progress for this model on these suites are resumed.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>InferenceModel</code> <p>The model being tested, complete with an <code>InferenceModel.infer</code> function to perform inference.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>test_config</code> <code>Optional[TestConfig]</code> <p>Optionally specify a configuration, e.g. <code>FixedGlobalThreshold</code>, to customize the metrics evaluation logic for this test run. Defaults to <code>AccuracyOptimal</code> if unspecified.</p> <code>None</code> <code>custom_metrics_callback</code> <code>Optional[CustomMetricsCallback[TestImage, Tuple[str, float]]]</code> <p>Optionally specify a callback function to compute custom metrics for each test case. The callback would be passed inferences of images in each testcase and should return a dictionary with metric name as key and metric value as value.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>Overwrites existing inferences if set.</p> <code>False</code> Source code in <code>kolena/classification/test_run.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef test(\nmodel: InferenceModel,\ntest_suite: TestSuite,\ntest_config: Optional[TestConfig] = None,\ncustom_metrics_callback: Optional[CustomMetricsCallback[TestImage, Tuple[str, float]]] = None,\nreset: bool = False,\n) -&gt; None:\n\"\"\"\n    Test the provided [`InferenceModel`][kolena.classification.InferenceModel] on a\n    [`TestSuite`][kolena.classification.TestSuite]. Any tests already in progress for this model on these suites are\n    resumed.\n    :param model: The model being tested, complete with an\n        [`InferenceModel.infer`][kolena.classification.InferenceModel.infer] function to perform inference.\n    :param test_suite: The test suite on which to test the model.\n    :param test_config: Optionally specify a configuration, e.g.\n        [`FixedGlobalThreshold`][kolena.classification.FixedGlobalThreshold], to customize the metrics evaluation logic\n        for this test run. Defaults to [`AccuracyOptimal`][kolena.classification.test_config.AccuracyOptimal] if\n        unspecified.\n    :param custom_metrics_callback: Optionally specify a callback function to compute custom metrics for each test case.\n        The callback would be passed inferences of images in each testcase and should return a dictionary with metric\n        name as key and metric value as value.\n    :param reset: Overwrites existing inferences if set.\n    \"\"\"\nwith TestRun(\nmodel,\ntest_suite,\ntest_config=test_config,\ncustom_metrics_callback=custom_metrics_callback,\nreset=reset,\n) as test_run:\nlog.info(\"performing inference\")\nfor image in log.progress_bar(test_run.iter_images()):\ntest_run.add_inferences(image, model.infer(image))\nlog.success(\"performed inference\")\n</code></pre>"},{"location":"reference/built-in/classification/#metadata","title":"Metadata","text":"<p>Metadata associated with a <code>TestImage</code>.</p> <pre><code>from kolena.classification import TestImage\nfrom kolena.classification.metadata import Landmarks, BoundingBox, Asset\ntest_image = TestImage(\"s3://bucket/path/to/image.png\", metadata=dict(\ninput_landmarks=Landmarks([(0,0), (10, 10), (20, 20), (30, 30), (40, 40)]),\ninput_bounding_box=BoundingBox((0, 0), (100, 100)),\nimage_grayscale=Asset(\"s3://bucket/path/to/image_grayscale.png\"),\n))\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.classification.metadata.Annotation","title":"<code>Annotation()</code>","text":"<p>         Bases: <code>Frozen</code>, <code>Serializable</code></p> <p>An annotation associated with an image.</p> <p>Annotations are surfaced during testing along with the image locator and any other metadata associated with an image. In the web platform, annotations are overlaid on top of images when visualizing results.</p> Source code in <code>kolena/detection/_internal/metadata.py</code> <pre><code>@abstractmethod\ndef __init__(self) -&gt; None:\n...\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.classification.metadata.Asset","title":"<code>Asset(locator)</code>","text":"<p>         Bases: <code>Frozen</code>, <code>Serializable</code></p> <p>An asset living in your shared bucket. Assets are surfaced during testing along with any other metadata associated with a given test image.</p> <p>In the web platform, certain assets such as PNG and JPG images are viewable when visualizing results in the gallery.</p> Source code in <code>kolena/detection/_internal/metadata.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, locator: str):\nself.locator = locator  # TODO: validate locator\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.detection._internal.metadata.Asset.locator","title":"<code>locator: str = locator</code>  <code>instance-attribute</code>","text":"<p>Location of this asset in shared bucket, e.g. <code>s3://my-bucket/path/to/image.png</code>.</p>"},{"location":"reference/built-in/classification/#kolena.classification.metadata.BoundingBox","title":"<code>BoundingBox(top_left, bottom_right)</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>An annotation comprising a bounding box around an object in an image.</p> Source code in <code>kolena/detection/_internal/metadata.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, top_left: Tuple[float, float], bottom_right: Tuple[float, float]):\nself.top_left = top_left\nself.bottom_right = bottom_right\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.detection._internal.metadata.BoundingBox.top_left","title":"<code>top_left: Tuple[float, float] = top_left</code>  <code>instance-attribute</code>","text":"<p>Point in <code>(x, y)</code> pixel coordinates representing the top left corner of the bounding box.</p>"},{"location":"reference/built-in/classification/#kolena.detection._internal.metadata.BoundingBox.bottom_right","title":"<code>bottom_right: Tuple[float, float] = bottom_right</code>  <code>instance-attribute</code>","text":"<p>Point in <code>(x, y)</code> pixel coordinates representing the bottom right corner of the bounding box.</p>"},{"location":"reference/built-in/classification/#kolena.classification.metadata.Landmarks","title":"<code>Landmarks(points)</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>An annotation comprising an arbitrary-length set of landmarks corresponding to some object in an image, e.g. face landmarks used for pose estimation.</p> Source code in <code>kolena/detection/_internal/metadata.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, points: List[Tuple[float, float]]):\nif len(points) == 0:\nraise ValueError(\"At least one point required for landmarks annotation.\")\nself.points = points\n</code></pre>"},{"location":"reference/built-in/classification/#kolena.detection._internal.metadata.Landmarks.points","title":"<code>points: List[Tuple[float, float]] = points</code>  <code>instance-attribute</code>","text":"<p>Any number of <code>(x, y)</code> points in pixel coordinates representing a set of landmarks.</p>"},{"location":"reference/built-in/detection/","title":"<code>kolena.detection</code>","text":"<p>Legacy Warning: Deprecated Module</p> <p>The built-in <code>kolena.detection</code> module has been deprecated. Consider using <code>kolena.workflow</code> instead.</p>"},{"location":"reference/built-in/detection/#quick-links","title":"Quick Links","text":"<ul> <li><code>kolena.detection.TestImage</code>: create images for testing</li> <li><code>kolena.detection.TestCase</code>: create and manage test cases</li> <li><code>kolena.detection.TestSuite</code>: create and manage test suites</li> <li><code>kolena.detection.TestRun</code>: test models on test suites</li> <li><code>kolena.detection.Model</code>: create models for testing</li> </ul>"},{"location":"reference/built-in/detection/#kolena.detection.TestConfig","title":"<code>TestConfig</code>","text":"<p>         Bases: <code>_TestConfig</code></p> <p>Base class for a testing configuration.</p> <p>See concrete implementations <code>FixedGlobalThreshold</code> and <code>F1Optimal</code> for details.</p>"},{"location":"reference/built-in/detection/#kolena.detection.TestSuite","title":"<code>TestSuite(name, version=None, description=None, test_cases=None, reset=False)</code>","text":"<p>         Bases: <code>BaseTestSuite</code></p> <p>A test suite groups together one or more test cases.</p> <p>For additional functionality, see the associated base class documentation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the test suite to create or load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify the version of the test suite to load. When absent, the latest version is loaded. Ignored when creating new test suites.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Optionally specify a description for a newly created test suite. For existing test suites, this description can be edited via <code>TestSuite.edit</code>.</p> <code>None</code> <code>test_cases</code> <code>Optional[List[TestCase]]</code> <p>Optionally specify a list of test cases to populate a new test suite. For existing test suites, test cases can be edited via <code>TestSuite.edit</code>.</p> <code>None</code> Source code in <code>kolena/detection/test_suite.py</code> <pre><code>def __init__(\nself,\nname: str,\nversion: Optional[int] = None,\ndescription: Optional[str] = None,\ntest_cases: Optional[List[TestCase]] = None,\nreset: bool = False,\n):\nsuper().__init__(name, WorkflowType.DETECTION, version, description, test_cases, reset)\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.test_suite.TestSuite.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Unique name for this test suite.</p>"},{"location":"reference/built-in/detection/#kolena.detection.test_suite.TestSuite.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of the test suite. This version is automatically incremented each time the suite is edited with <code>TestSuite.edit</code>.</p>"},{"location":"reference/built-in/detection/#kolena.detection.test_suite.TestSuite.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form, human-readable description of the test suite. Can be edited at any time via <code>TestSuite.edit</code>.</p>"},{"location":"reference/built-in/detection/#kolena.detection.test_suite.TestSuite.test_cases","title":"<code>test_cases: List[TestCase]</code>  <code>instance-attribute</code>","text":"<p>The test cases within this test suite.</p>"},{"location":"reference/built-in/detection/#kolena.detection.TestCase","title":"<code>TestCase(name, version=None, description=None, images=None, reset=False)</code>","text":"<p>         Bases: <code>BaseTestCase</code></p> <p>A test case holds a set of images to compute performance metrics against.</p> <p>For additional functionality, see the associated base class documentation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the test case to create or load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify the version of the test case to load. When absent, the latest version is loaded. Ignored when creating new test cases.</p> <code>None</code> <code>description</code> <code>Optional[str]</code> <p>Optionally specify a description for a newly created test case. For existing test cases, this description can be edited via <code>TestCase.edit</code>.</p> <code>None</code> <code>images</code> <code>Optional[List[_TestImageClass]]</code> <p>Optionally provide a list of images and associated ground truths to populate a new test case. For existing test cases, images can be edited via <code>TestCase.edit</code>.</p> <code>None</code> Source code in <code>kolena/detection/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nname: str,\nversion: Optional[int] = None,\ndescription: Optional[str] = None,\nimages: Optional[List[_TestImageClass]] = None,\nreset: bool = False,\n):\nsuper().__init__(name, WorkflowType.DETECTION, version, description, images, reset)\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.FixedGlobalThreshold","title":"<code>FixedGlobalThreshold(fixed_threshold, iou_threshold=0.5)</code>","text":"<p>         Bases: <code>TestConfig</code></p> <p>Test configuration that sets the default display threshold in Kolena to be a fixed global threshold for all label classes within the test run.</p> Source code in <code>kolena/detection/test_config.py</code> <pre><code>def __init__(self, fixed_threshold: float, iou_threshold: float = 0.5):\nif iou_threshold &lt; 0 or iou_threshold &gt; 1:\nraise InputValidationError(f\"iou_threshold of {iou_threshold} was not between 0 and 1\")\nif fixed_threshold &lt; 0 or fixed_threshold &gt; 1:\nraise InputValidationError(f\"threshold of {fixed_threshold} was not between 0 and 1\")\nself.iou_threshold = iou_threshold\nself.threshold = fixed_threshold\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.test_config.FixedGlobalThreshold.fixed_threshold","title":"<code>fixed_threshold: float</code>  <code>instance-attribute</code>","text":"<p>The threshold used as the default for all label classes when visualizing results in Kolena. Must be between 0 and 1.</p>"},{"location":"reference/built-in/detection/#kolena.detection.test_config.FixedGlobalThreshold.iou_threshold","title":"<code>iou_threshold: float = iou_threshold</code>  <code>instance-attribute</code>","text":"<p>The minimum intersection over union score between an inference and a ground truth for it to qualify as a potential match. Must be between 0 and 1.</p>"},{"location":"reference/built-in/detection/#kolena.detection.Inference","title":"<code>Inference</code>","text":"<p>         Bases: <code>_Inference</code></p> <p>Base class for model inferences associated with an image.</p> <p>See concrete implementations <code>BoundingBox</code>, and <code>SegmentationMask</code> for details.</p>"},{"location":"reference/built-in/detection/#kolena.detection.GroundTruth","title":"<code>GroundTruth()</code>","text":"<p>         Bases: <code>Serializable</code>, <code>Frozen</code></p> <p>Base class for ground truths associated with an image.</p> <p>See concrete implementations <code>BoundingBox</code> and <code>SegmentationMask</code> for details.</p> Source code in <code>kolena/detection/ground_truth.py</code> <pre><code>@abstractmethod\ndef __init__(self) -&gt; None:\n...\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.Model","title":"<code>Model(name, metadata=None)</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>The descriptor for an object detection model within Kolena.</p> <p>For additional functionality, see the associated base class documentation.</p> Source code in <code>kolena/detection/model.py</code> <pre><code>def __init__(self, name: str, metadata: Optional[Dict[str, Any]] = None):\nsuper().__init__(name, WorkflowType.DETECTION, metadata)\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.model.Model.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Unique name of the model within the platform. If the provided model name has already been registered, that model and its metadata are loaded upon instantiation.</p>"},{"location":"reference/built-in/detection/#kolena.detection.model.Model.metadata","title":"<code>metadata: Dict[str, Any]</code>  <code>instance-attribute</code>","text":"<p>Unstructured metadata associated with the model.</p>"},{"location":"reference/built-in/detection/#kolena.detection.TestRun","title":"<code>TestRun(model, test_suite, test_config=None, custom_metrics_callback=None, reset=False)</code>","text":"<p>         Bases: <code>BaseTestRun</code></p> <p>Interface to run tests for a <code>Model</code> on a <code>TestSuite</code>.</p> <p>For a streamlined interface, see <code>test</code>.</p> <p>Changes are committed to Kolena during execution and when the context is exited.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model being tested.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>test_config</code> <code>Optional[TestConfig]</code> <p>Optionally specify a configuration, e.g. <code>FixedGlobalThreshold</code> to customize the metrics evaluation logic for this test run. Defaults to <code>F1Optimal</code> with an <code>iou_threshold</code> of 0.5 if unspecified.</p> <code>None</code> <code>custom_metrics_callback</code> <code>Optional[CustomMetricsCallback[_TestImageClass, _InferenceClass]]</code> <p>Optionally specify a callback function to compute custom metrics for each test case. The callback would be passed inferences of images in each testcase and should return a dictionary with metric name as key and metric value as value.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>Overwrites existing inferences if set.</p> <code>False</code> Source code in <code>kolena/detection/test_run.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nmodel: Model,\ntest_suite: TestSuite,\ntest_config: Optional[TestConfig] = None,\ncustom_metrics_callback: Optional[CustomMetricsCallback[_TestImageClass, _InferenceClass]] = None,\nreset: bool = False,\n):\nconfig = F1Optimal(iou_threshold=0.5) if test_config is None else test_config\nsuper().__init__(\nmodel,\ntest_suite,\nconfig=config._to_run_config(),\ncustom_metrics_callback=custom_metrics_callback,\nreset=reset,\n)\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.TestImage","title":"<code>TestImage(locator, dataset=None, ground_truths=None, metadata=None)</code>","text":"<p>         Bases: <code>BaseTestImage</code></p> <p>Test image comprising a single image locator.</p> Source code in <code>kolena/detection/test_image.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nlocator: str,\ndataset: Optional[str] = None,\nground_truths: Optional[List[GroundTruth]] = None,\nmetadata: Dict[str, MetadataElement] = None,\n):\nsuper().__init__(locator, dataset, metadata)\nself.ground_truths = ground_truths or []\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.test_image.TestImage.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>Bucket locator for the provided test sample, e.g. <code>gs://my-bucket/path/to/image.png</code>.</p>"},{"location":"reference/built-in/detection/#kolena.detection.test_image.TestImage.dataset","title":"<code>dataset: str</code>  <code>instance-attribute</code>","text":"<p>Dataset this test image belongs to. Empty when unspecified.</p>"},{"location":"reference/built-in/detection/#kolena.detection.test_image.TestImage.metadata","title":"<code>metadata: Dict[str, MetadataElement]</code>  <code>instance-attribute</code>","text":"<p>Metadata associated with this test image. Surfaced during test runs.</p>"},{"location":"reference/built-in/detection/#kolena.detection.test_image.TestImage.ground_truths","title":"<code>ground_truths: List[GroundTruth] = ground_truths or []</code>  <code>instance-attribute</code>","text":"<p>List of <code>GroundTruth</code> annotations associated with this image.</p>"},{"location":"reference/built-in/detection/#kolena.detection.test_image.TestImage.filter","title":"<code>filter(predicate)</code>","text":"<p>Return a copy of this test image with ground truths filtered to only those that match the provided predicate.</p> <p>Parameters:</p> Name Type Description Default <code>predicate</code> <code>Callable[[GroundTruth], bool]</code> <p>Function accepting a <code>GroundTruth</code> and returning a boolean indicating whether or not to include the ground truth.</p> required <p>Returns:</p> Type Description <code>TestImage</code> <p>A new test image with ground truths filtered by the predicate.</p> Source code in <code>kolena/detection/test_image.py</code> <pre><code>def filter(self, predicate: Callable[[GroundTruth], bool]) -&gt; \"TestImage\":\n\"\"\"\n    Return a copy of this test image with ground truths filtered to only those that match the provided predicate.\n    :param predicate: Function accepting a [`GroundTruth`][kolena.detection.ground_truth.GroundTruth] and returning\n        a boolean indicating whether or not to include the ground truth.\n    :return: A new test image with ground truths filtered by the predicate.\n    \"\"\"\nreturn TestImage(**{**self._fields(), \"ground_truths\": [gt for gt in self.ground_truths if predicate(gt)]})\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.F1Optimal","title":"<code>F1Optimal(iou_threshold=0.5)</code>","text":"<p>         Bases: <code>TestConfig</code></p> <p>Test configuration that sets the default display threshold in Kolena to be dynamically set to the threshold that corresponds to the highest F1 score for the test suite within the test run.</p> <p>This threshold is evaluated and set per label for test suites with multiple label classes.</p> Source code in <code>kolena/detection/test_config.py</code> <pre><code>def __init__(self, iou_threshold: float = 0.5):\nif iou_threshold &lt; 0 or iou_threshold &gt; 1:\nraise InputValidationError(f\"iou_threshold of {iou_threshold} was not between 0 and 1\")\nself.iou_threshold = iou_threshold\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.test_config.F1Optimal.iou_threshold","title":"<code>iou_threshold: float = iou_threshold</code>  <code>instance-attribute</code>","text":"<p>The minimum intersection over union score between an inference and a ground truth for it to qualify as a potential match. Must be between 0 and 1.</p>"},{"location":"reference/built-in/detection/#kolena.detection.InferenceModel","title":"<code>InferenceModel(name, infer, metadata=None)</code>","text":"<p>         Bases: <code>Model</code></p> <p>Extension of <code>Model</code> with an <code>infer</code> method to perform inference.</p> <p>See documentation parent <code>Model</code> for details.</p> Source code in <code>kolena/detection/model.py</code> <pre><code>def __init__(\nself,\nname: str,\ninfer: Callable[[TestImage], Optional[List[Inference]]],\nmetadata: Optional[Dict[str, Any]] = None,\n):\nsetattr(self, \"infer\", infer)  # bypass mypy method assignment bug\nsuper().__init__(name, metadata)\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.model.InferenceModel.infer","title":"<code>infer: Callable[[TestImage], Optional[List[Inference]]]</code>  <code>instance-attribute</code>","text":"<p>Function transforming a <code>TestImage</code> into a list of zero or more <code>Inference</code> objects.</p>"},{"location":"reference/built-in/detection/#kolena.detection.test","title":"<code>test(model, test_suite, test_config=None, custom_metrics_callback=None, reset=False)</code>","text":"<p>Test the provided <code>InferenceModel</code> on the provided <code>TestSuite</code>. Any tests already in progress for this model on these suites are resumed.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>InferenceModel</code> <p>The model to test.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>test_config</code> <code>Optional[TestConfig]</code> <p>Optionally specify a configuration, e.g. <code>FixedGlobalThreshold</code>, to customize metrics evaluation logic for this test run. Defaults to <code>F1Optimal</code> with an <code>iou_threshold</code> of 0.5 if unspecified.</p> <code>None</code> <code>custom_metrics_callback</code> <code>Optional[CustomMetricsCallback[TestImage, Inference]]</code> <p>Optionally specify a callback function to compute custom metrics for each test case. The callback would be passed inferences of images in each testcase and should return a dictionary with metric name as key and metric value as value.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>Overwrites existing inferences if set.</p> <code>False</code> Source code in <code>kolena/detection/test_run.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef test(\nmodel: InferenceModel,\ntest_suite: TestSuite,\ntest_config: Optional[TestConfig] = None,\ncustom_metrics_callback: Optional[CustomMetricsCallback[TestImage, Inference]] = None,\nreset: bool = False,\n) -&gt; None:\n\"\"\"\n    Test the provided [`InferenceModel`][kolena.detection.InferenceModel] on the provided\n    [`TestSuite`][kolena.detection.TestSuite]. Any tests already in progress for this model on these suites are resumed.\n    :param model: The model to test.\n    :param test_suite: The test suite on which to test the model.\n    :param test_config: Optionally specify a configuration, e.g.\n        [`FixedGlobalThreshold`][kolena.detection.FixedGlobalThreshold], to customize metrics evaluation logic for this\n        test run. Defaults to [`F1Optimal`][kolena.detection.F1Optimal] with an `iou_threshold` of 0.5 if unspecified.\n    :param custom_metrics_callback: Optionally specify a callback function to compute custom metrics for each test case.\n        The callback would be passed inferences of images in each testcase and should return a dictionary with metric\n        name as key and metric value as value.\n    :param reset: Overwrites existing inferences if set.\n    \"\"\"\nwith TestRun(\nmodel,\ntest_suite,\ntest_config=test_config,\ncustom_metrics_callback=custom_metrics_callback,\nreset=reset,\n) as test_run:\nlog.info(\"performing inference\")\nfor image in log.progress_bar(test_run.iter_images()):\ntest_run.add_inferences(image, model.infer(image))\nlog.success(\"performed inference\")\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.load_images","title":"<code>load_images(dataset=None)</code>","text":"<p>Deprecated: since <code>0.26.0</code></p> <p>Please use <code>TestCase.load_images</code> instead.</p> <p>Load a list of <code>TestImage</code> samples registered in Kolena.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Optional[str]</code> <p>Optionally specify the single dataset to be retrieved. By default, images from all datasets are returned.</p> <code>None</code> Source code in <code>kolena/detection/test_image.py</code> <pre><code>@deprecated(details=\"use :class:`kolena.detection.TestCase.load_images`\", deprecated_in=\"0.26.0\")\n@validate_arguments(config=ValidatorConfig)\ndef load_images(dataset: Optional[str] = None) -&gt; List[TestImage]:\n\"\"\"\n    !!! warning \"Deprecated: since `0.26.0`\"\n        Please use [`TestCase.load_images`][kolena.detection._internal.test_case.BaseTestCase.load_images] instead.\n    Load a list of [`TestImage`][kolena.detection.TestImage] samples registered in Kolena.\n    :param dataset: Optionally specify the single dataset to be retrieved. By default, images from all\n        datasets are returned.\n    \"\"\"\nreturn list(iter_images(dataset))\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.iter_images","title":"<code>iter_images(dataset=None)</code>","text":"<p>Deprecated: since <code>0.26.0</code></p> <p>Please use <code>TestCase.iter_images</code> instead.</p> <p>Return iterator over <code>TestImage</code> samples registered in Kolena. Images are lazily loaded in chunks to facilitate working with large datasets that are cumbersome to hold in memory.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Optional[str]</code> <p>Optionally specify the single dataset to be retrieved. By default, images from all datasets are returned.</p> <code>None</code> Source code in <code>kolena/detection/test_image.py</code> <pre><code>@deprecated(details=\"use :class:`kolena.detection.TestCase.iter_images`\", deprecated_in=\"0.26.0\")\n@validate_arguments(config=ValidatorConfig)\ndef iter_images(dataset: Optional[str] = None) -&gt; Iterator[TestImage]:\n\"\"\"\n    !!! warning \"Deprecated: since `0.26.0`\"\n        Please use [`TestCase.iter_images`][kolena.detection._internal.test_case.BaseTestCase.iter_images] instead.\n    Return iterator over [`TestImage`][kolena.detection.TestImage] samples registered in Kolena. Images are lazily\n    loaded in chunks to facilitate working with large datasets that are cumbersome to hold in memory.\n    :param dataset: Optionally specify the single dataset to be retrieved. By default, images from all\n        datasets are returned.\n    \"\"\"\ninit_request = API.InitLoadImagesRequest(dataset=dataset, batch_size=BatchSize.LOAD_RECORDS.value)\nfor df in _BatchedLoader.iter_data(\ninit_request=init_request,\nendpoint_path=API.Path.INIT_LOAD_IMAGES.value,\ndf_class=TestImageDataFrame,\n):\nfor record in df.itertuples():\nyield TestImage._from_record(record)\n</code></pre>"},{"location":"reference/built-in/detection/#ground-truth","title":"Ground Truth","text":""},{"location":"reference/built-in/detection/#kolena.detection.ground_truth.GroundTruth","title":"<code>GroundTruth()</code>","text":"<p>         Bases: <code>Serializable</code>, <code>Frozen</code></p> <p>Base class for ground truths associated with an image.</p> <p>See concrete implementations <code>BoundingBox</code> and <code>SegmentationMask</code> for details.</p> Source code in <code>kolena/detection/ground_truth.py</code> <pre><code>@abstractmethod\ndef __init__(self) -&gt; None:\n...\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.ground_truth.BoundingBox","title":"<code>BoundingBox(label, top_left, bottom_right, difficult=False)</code>","text":"<p>         Bases: <code>GroundTruth</code></p> <p>Ground truth data object representing a bounding box.</p> <p>Point coordinates should be in <code>(x, y)</code> format, as absolute pixel values.</p> Source code in <code>kolena/detection/ground_truth.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nlabel: str,\ntop_left: Tuple[float, float],\nbottom_right: Tuple[float, float],\ndifficult: bool = False,\n):\nsuper().__init__()\nvalidate_label(label)\nself.label = label\nself.top_left = top_left\nself.bottom_right = bottom_right\nself.difficult = difficult\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.ground_truth.BoundingBox.label","title":"<code>label: str = label</code>  <code>instance-attribute</code>","text":"<p>Label to associate with this bounding box.</p>"},{"location":"reference/built-in/detection/#kolena.detection.ground_truth.BoundingBox.top_left","title":"<code>top_left: Tuple[float, float] = top_left</code>  <code>instance-attribute</code>","text":"<p>Point in <code>(x, y)</code> pixel coordinates representing the top left corner of the bounding box.</p>"},{"location":"reference/built-in/detection/#kolena.detection.ground_truth.BoundingBox.bottom_right","title":"<code>bottom_right: Tuple[float, float] = bottom_right</code>  <code>instance-attribute</code>","text":"<p>Point in <code>(x, y)</code> pixel coordinates representing the bottom right corner of the bounding box.</p>"},{"location":"reference/built-in/detection/#kolena.detection.ground_truth.BoundingBox.difficult","title":"<code>difficult: bool = difficult</code>  <code>instance-attribute</code>","text":"<p>A bounding box marked as <code>difficult</code> indicates that the object is considered difficult to recognize and should be ignored when evaluating metrics.</p>"},{"location":"reference/built-in/detection/#kolena.detection.ground_truth.SegmentationMask","title":"<code>SegmentationMask(label, points, difficult=False)</code>","text":"<p>         Bases: <code>GroundTruth</code></p> <p>Ground truth data object representing a detection mask.</p> <p>Point coordinates should be in <code>(x, y)</code> format, as absolute pixel values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When fewer than three points are provided.</p> Source code in <code>kolena/detection/ground_truth.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, label: str, points: List[Tuple[float, float]], difficult: bool = False):\nsuper().__init__()\nvalidate_label(label)\nvalidate_polygon(points)\nself.label = label\nself.points = points\nself.difficult = difficult\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.ground_truth.SegmentationMask.label","title":"<code>label: str = label</code>  <code>instance-attribute</code>","text":"<p>Label to associate with this segmentation mask.</p>"},{"location":"reference/built-in/detection/#kolena.detection.ground_truth.SegmentationMask.points","title":"<code>points: List[Tuple[float, float]] = points</code>  <code>instance-attribute</code>","text":"<p>Polygon corresponding to the vertices of the segmentation mask. Must have at least three distinct elements, and may not cross itself or touch itself at any point.</p>"},{"location":"reference/built-in/detection/#kolena.detection.ground_truth.SegmentationMask.difficult","title":"<code>difficult: bool = difficult</code>  <code>instance-attribute</code>","text":"<p>A segmentation mask marked as <code>difficult</code> indicates that the object is considered difficult to recognize and should be ignored when evaluating metrics.</p>"},{"location":"reference/built-in/detection/#inference","title":"Inference","text":""},{"location":"reference/built-in/detection/#kolena.detection.inference.Inference","title":"<code>Inference</code>","text":"<p>         Bases: <code>_Inference</code></p> <p>Base class for model inferences associated with an image.</p> <p>See concrete implementations <code>BoundingBox</code>, and <code>SegmentationMask</code> for details.</p>"},{"location":"reference/built-in/detection/#kolena.detection.inference.BoundingBox","title":"<code>BoundingBox(label, confidence, top_left, bottom_right)</code>","text":"<p>         Bases: <code>Inference</code></p> <p>Inference representing a bounding box.</p> <p>Point coordinates should be in <code>(x, y)</code> format, as absolute pixel values.</p> Source code in <code>kolena/detection/inference.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, label: str, confidence: float, top_left: Tuple[float, float], bottom_right: Tuple[float, float]):\nvalidate_label(label)\nvalidate_confidence(confidence)\nself.label = label\nself.confidence = confidence\nself.top_left = top_left\nself.bottom_right = bottom_right\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.inference.BoundingBox.label","title":"<code>label: str = label</code>  <code>instance-attribute</code>","text":"<p>Label to associate with this bounding box.</p>"},{"location":"reference/built-in/detection/#kolena.detection.inference.BoundingBox.confidence","title":"<code>confidence: float = confidence</code>  <code>instance-attribute</code>","text":"<p>Confidence score associated with this inference.</p>"},{"location":"reference/built-in/detection/#kolena.detection.inference.BoundingBox.top_left","title":"<code>top_left: Tuple[float, float] = top_left</code>  <code>instance-attribute</code>","text":"<p>Point in <code>(x, y)</code> pixel coordinates representing the top left corner of the bounding box.</p>"},{"location":"reference/built-in/detection/#kolena.detection.inference.BoundingBox.bottom_right","title":"<code>bottom_right: Tuple[float, float] = bottom_right</code>  <code>instance-attribute</code>","text":"<p>Point in <code>(x, y)</code> pixel coordinates representing the bottom right corner of the bounding box.</p>"},{"location":"reference/built-in/detection/#kolena.detection.inference.SegmentationMask","title":"<code>SegmentationMask(label, confidence, points)</code>","text":"<p>         Bases: <code>Inference</code></p> <p>Inference data object representing a segmentation mask.</p> <p>Point coordinates should be in <code>(x, y)</code> format, as absolute pixel values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When fewer than three points are provided.</p> Source code in <code>kolena/detection/inference.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, label: str, confidence: float, points: List[Tuple[float, float]]):\nvalidate_label(label)\nvalidate_confidence(confidence)\nvalidate_polygon(points)\nself.label = label\nself.confidence = confidence\nself.points = points\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.inference.SegmentationMask.label","title":"<code>label: str = label</code>  <code>instance-attribute</code>","text":"<p>Label to associate with this segmentation mask.</p>"},{"location":"reference/built-in/detection/#kolena.detection.inference.SegmentationMask.confidence","title":"<code>confidence: float = confidence</code>  <code>instance-attribute</code>","text":"<p>Confidence score associated with this inference.</p>"},{"location":"reference/built-in/detection/#kolena.detection.inference.SegmentationMask.points","title":"<code>points: List[Tuple[float, float]] = points</code>  <code>instance-attribute</code>","text":"<p>Polygon corresponding to the vertices of the segmentation mask. Must have at least three distinct elements, and may not cross itself or touch itself at any point.</p>"},{"location":"reference/built-in/detection/#metadata","title":"Metadata","text":"<p>Metadata associated with a <code>TestImage</code>.</p> <pre><code>from kolena.detection import TestImage\nfrom kolena.detection.metadata import Landmarks, BoundingBox, Asset\ntest_image = TestImage(\"s3://bucket/path/to/image.png\", metadata=dict(\ninput_landmarks=Landmarks([(0,0), (10, 10), (20, 20), (30, 30), (40, 40)]),\ninput_bounding_box=BoundingBox((0, 0), (100, 100)),\nimage_grayscale=Asset(\"s3://bucket/path/to/image_grayscale.png\"),\n))\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.metadata.Annotation","title":"<code>Annotation()</code>","text":"<p>         Bases: <code>Frozen</code>, <code>Serializable</code></p> <p>An annotation associated with an image.</p> <p>Annotations are surfaced during testing along with the image locator and any other metadata associated with an image. In the web platform, annotations are overlaid on top of images when visualizing results.</p> Source code in <code>kolena/detection/_internal/metadata.py</code> <pre><code>@abstractmethod\ndef __init__(self) -&gt; None:\n...\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection.metadata.Asset","title":"<code>Asset(locator)</code>","text":"<p>         Bases: <code>Frozen</code>, <code>Serializable</code></p> <p>An asset living in your shared bucket. Assets are surfaced during testing along with any other metadata associated with a given test image.</p> <p>In the web platform, certain assets such as PNG and JPG images are viewable when visualizing results in the gallery.</p> Source code in <code>kolena/detection/_internal/metadata.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, locator: str):\nself.locator = locator  # TODO: validate locator\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection._internal.metadata.Asset.locator","title":"<code>locator: str = locator</code>  <code>instance-attribute</code>","text":"<p>Location of this asset in shared bucket, e.g. <code>s3://my-bucket/path/to/image.png</code>.</p>"},{"location":"reference/built-in/detection/#kolena.detection.metadata.BoundingBox","title":"<code>BoundingBox(top_left, bottom_right)</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>An annotation comprising a bounding box around an object in an image.</p> Source code in <code>kolena/detection/_internal/metadata.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, top_left: Tuple[float, float], bottom_right: Tuple[float, float]):\nself.top_left = top_left\nself.bottom_right = bottom_right\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection._internal.metadata.BoundingBox.top_left","title":"<code>top_left: Tuple[float, float] = top_left</code>  <code>instance-attribute</code>","text":"<p>Point in <code>(x, y)</code> pixel coordinates representing the top left corner of the bounding box.</p>"},{"location":"reference/built-in/detection/#kolena.detection._internal.metadata.BoundingBox.bottom_right","title":"<code>bottom_right: Tuple[float, float] = bottom_right</code>  <code>instance-attribute</code>","text":"<p>Point in <code>(x, y)</code> pixel coordinates representing the bottom right corner of the bounding box.</p>"},{"location":"reference/built-in/detection/#kolena.detection.metadata.Landmarks","title":"<code>Landmarks(points)</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>An annotation comprising an arbitrary-length set of landmarks corresponding to some object in an image, e.g. face landmarks used for pose estimation.</p> Source code in <code>kolena/detection/_internal/metadata.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, points: List[Tuple[float, float]]):\nif len(points) == 0:\nraise ValueError(\"At least one point required for landmarks annotation.\")\nself.points = points\n</code></pre>"},{"location":"reference/built-in/detection/#kolena.detection._internal.metadata.Landmarks.points","title":"<code>points: List[Tuple[float, float]] = points</code>  <code>instance-attribute</code>","text":"<p>Any number of <code>(x, y)</code> points in pixel coordinates representing a set of landmarks.</p>"},{"location":"reference/built-in/fr/","title":"<code>kolena.fr</code>","text":"<p>Legacy Warning: Deprecated Module</p> <p>The built-in <code>kolena.fr</code> module has been deprecated. Consider using <code>kolena.workflow</code> instead.</p>"},{"location":"reference/built-in/fr/#quick-links","title":"Quick Links","text":"<ul> <li><code>kolena.fr.TestImages</code>: register new images for testing</li> <li><code>kolena.fr.TestCase</code>: create and manage test cases</li> <li><code>kolena.fr.TestSuite</code>: create and manage test suites</li> <li><code>kolena.fr.TestRun</code>: test models on test suites</li> <li><code>kolena.fr.Model</code>: create models for testing</li> </ul>"},{"location":"reference/built-in/fr/#kolena.fr.test_images.TestImages","title":"<code>TestImages</code>","text":"<p>         Bases: <code>Uninstantiable[None]</code></p>"},{"location":"reference/built-in/fr/#kolena.fr.test_images.TestImages.Registrar","title":"<code>Registrar</code>","text":"<p>         Bases: <code>Uninstantiable[_Registrar]</code></p>"},{"location":"reference/built-in/fr/#kolena.fr.test_images.TestImages.Registrar.add","title":"<code>add(locator, data_source, width, height, bounding_box=None, landmarks=None, tags=None)</code>","text":"<p>Add a new image to Kolena. If the provided locator is already registered with the platform, its metadata will be updated.</p> <p>Parameters:</p> Name Type Description Default <code>locator</code> <code>str</code> <p>Bucket locator for the provided image, e.g. <code>s3://bucket-name/path/to/image.jpg</code>.</p> required <code>data_source</code> <code>str</code> <p>Name of the source for the image being registered.</p> required <code>width</code> <code>int</code> <p>Width in pixels of the image being registered.</p> required <code>height</code> <code>int</code> <p>Height in pixels of the image being registered.</p> required <code>bounding_box</code> <code>Optional[np.ndarray]</code> <p>Optional 4-element array specifying the ground truth bounding box for this image, of the form <code>[top_left_x, top_left_y, bottom_right_x, bottom_right_y]</code>.</p> <code>None</code> <code>landmarks</code> <code>Optional[np.ndarray]</code> <p>Optional 10-element array specifying (x, y) coordinates for five facial landmarks of the form <code>[left_eye_{x,y}, right_eye_{x,y}, nose_{x,y}, left_mouth_{x,y}, right_mouth_{x,y}]</code>.</p> <code>None</code> <code>tags</code> <code>Optional[Dict[str, str]]</code> <p>Tags to associate with the image, of the form <code>{category: value}</code>.</p> <code>None</code> Source code in <code>kolena/fr/test_images.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef add(\nself,\nlocator: str,\ndata_source: str,\nwidth: int,\nheight: int,\nbounding_box: Optional[np.ndarray] = None,\nlandmarks: Optional[np.ndarray] = None,\ntags: Optional[Dict[str, str]] = None,\n) -&gt; None:\n\"\"\"\n    Add a new image to Kolena. If the provided locator is already registered with the platform, its metadata\n    will be updated.\n    :param locator: Bucket locator for the provided image, e.g. `s3://bucket-name/path/to/image.jpg`.\n    :param data_source: Name of the source for the image being registered.\n    :param width: Width in pixels of the image being registered.\n    :param height: Height in pixels of the image being registered.\n    :param bounding_box: Optional 4-element array specifying the ground truth bounding box for this image, of\n        the form `[top_left_x, top_left_y, bottom_right_x, bottom_right_y]`.\n    :param landmarks: Optional 10-element array specifying (x, y) coordinates for five facial landmarks of the\n        form `[left_eye_{x,y}, right_eye_{x,y}, nose_{x,y}, left_mouth_{x,y}, right_mouth_{x,y}]`.\n    :param tags: Tags to associate with the image, of the form `{category: value}`.\n    \"\"\"\nif locator in self.data.locators:\nraise ValueError(f\"duplicate locator: {locator}\")\nself.data.locators.add(locator)\nself.data.records.append(\n(\nlocator,\ndata_source,\nwidth,\nheight,\nNone,\nNone,\nbounding_box,\nlandmarks,\ntags or {},\n),\n)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_images.TestImages.Registrar.add_augmented","title":"<code>add_augmented(original_locator, augmented_locator, augmentation_spec, width=None, height=None, bounding_box=None, landmarks=None, tags=None)</code>","text":"<p>Add an augmented version of an existing image to Kolena.</p> <p>Note that the original image must already be registered in a previous pass. Tags on the original image are not propagated forward to the augmented image.</p> <p>Parameters:</p> Name Type Description Default <code>original_locator</code> <code>str</code> <p>The bucket locator for the original version of this image within the platform.</p> required <code>augmented_locator</code> <code>str</code> <p>The bucket locator for the augmented image being registered.</p> required <code>augmentation_spec</code> <code>Dict[str, Any]</code> <p>Free-form JSON specification for the augmentation applied to this image.</p> required <code>width</code> <code>Optional[int]</code> <p>Optionally specify the width of the augmented image. When absent, the width of the corresponding original image is used.</p> <code>None</code> <code>height</code> <code>Optional[int]</code> <p>Optionally specify the height of the augmented image. When absent, the height of the corresponding original image is used.</p> <code>None</code> <code>bounding_box</code> <code>Optional[np.ndarray]</code> <p>Optionally specify a new bounding box for the augmented image. When absent, any bounding box corresponding to the original image is used.</p> <code>None</code> <code>landmarks</code> <code>Optional[np.ndarray]</code> <p>Optionally specify a new set of landmarks for the augmented image. When absent, any set of landmarks corresponding to the original image is used.</p> <code>None</code> <code>tags</code> <code>Optional[Dict[str, str]]</code> <p>Optionally specify a set of tags to associate with the augmented image.</p> <code>None</code> Source code in <code>kolena/fr/test_images.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef add_augmented(\nself,\noriginal_locator: str,\naugmented_locator: str,\naugmentation_spec: Dict[str, Any],\nwidth: Optional[int] = None,  # if absent, original width is used\nheight: Optional[int] = None,  # if absent, original height is used\nbounding_box: Optional[np.ndarray] = None,  # if absent, original bbox is used if defined\nlandmarks: Optional[np.ndarray] = None,  # if absent, original lmks are used if defined\ntags: Optional[Dict[str, str]] = None,  # note that tags are not propagated forward from the original\n) -&gt; None:\n\"\"\"\n    Add an augmented version of an existing image to Kolena.\n    Note that the original image must already be registered in a previous pass. Tags on the original image are\n    not propagated forward to the augmented image.\n    :param original_locator: The bucket locator for the original version of this image within the platform.\n    :param augmented_locator: The bucket locator for the augmented image being registered.\n    :param augmentation_spec: Free-form JSON specification for the augmentation applied to this image.\n    :param width: Optionally specify the width of the augmented image. When absent, the width of the\n        corresponding original image is used.\n    :param height: Optionally specify the height of the augmented image. When absent, the height of the\n        corresponding original image is used.\n    :param bounding_box: Optionally specify a new bounding box for the augmented image. When absent, any\n        bounding box corresponding to the original image is used.\n    :param landmarks: Optionally specify a new set of landmarks for the augmented image. When absent, any set of\n        landmarks corresponding to the original image is used.\n    :param tags: Optionally specify a set of tags to associate with the augmented image.\n    \"\"\"\nif augmented_locator in self.data.locators:\nraise ValueError(f\"duplicate locator: {augmented_locator}\")\nself.data.locators.add(augmented_locator)\nself.data.records.append(\n(\naugmented_locator,\nNone,\nwidth or -1,\nheight or -1,\noriginal_locator,\naugmentation_spec,\nbounding_box,\nlandmarks,\ntags or {},\n),\n)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_images.TestImages.load","title":"<code>load(data_source=None, include_augmented=False)</code>  <code>classmethod</code>","text":"<p>Load a DataFrame describing images registered in the Kolena platform.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>Optional[Union[str, TestSuite, TestSuite.Data, TestCase, TestCase.Data]]</code> <p>Optionally specify the single data source to be retrieved, e.g. <code>\"my-data-source\"</code>. Alternatively, provide a <code>TestSuite</code> or <code>TestCase</code> as source. If no argument is provided, all images registered using `TestImages.register are returned.</p> <code>None</code> <code>include_augmented</code> <code>bool</code> <p>Optionally specify that augmented images should be returned. By default, only original images are returned. Ignored when test case or test suite is provided as <code>data_source</code>.</p> <code>False</code> Source code in <code>kolena/fr/test_images.py</code> <pre><code>@classmethod\n@validate_arguments(config=ValidatorConfig)\ndef load(\ncls,\ndata_source: Optional[Union[str, TestSuite, TestSuite.Data, TestCase, TestCase.Data]] = None,\ninclude_augmented: bool = False,\n) -&gt; TestImageDataFrame:\n\"\"\"\n    Load a DataFrame describing images registered in the Kolena platform.\n    :param data_source: Optionally specify the single data source to be retrieved, e.g. `\"my-data-source\"`.\n        Alternatively, provide a [`TestSuite`][kolena.fr.TestSuite] or [`TestCase`][kolena.fr.TestCase] as source.\n        If no argument is provided, all images registered using\n        [`TestImages.register][kolena.fr.TestImages.register] are returned.\n    :param include_augmented: Optionally specify that augmented images should be returned. By default, only\n        original images are returned. Ignored when test case or test suite is provided as `data_source`.\n    \"\"\"\nlog.info(\"loading test images\")\nreturn _BatchedLoader.concat(\ncls.iter(data_source=data_source, include_augmented=include_augmented),\nTestImageDataFrame,\n)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_images.TestImages.register","title":"<code>register()</code>  <code>classmethod</code>","text":"<p>Context-managed interface to register new images with Kolena. Images with locators that already exist in the platform will have their metadata updated. All changes are committed when the context is exited.</p> <p>Raises:</p> Type Description <code>RemoteError</code> <p>The registered images were unable to be successfully committed for any reason.</p> Source code in <code>kolena/fr/test_images.py</code> <pre><code>@classmethod\n@contextmanager\ndef register(cls) -&gt; Iterator[Registrar]:\n\"\"\"\n    Context-managed interface to register new images with Kolena. Images with locators that already exist in the\n    platform will have their metadata updated. All changes are committed when the context is exited.\n    :raises RemoteError: The registered images were unable to be successfully committed for any reason.\n    \"\"\"\nlog.info(\"registering test images\")\nregistrar = TestImages.Registrar.__factory__(TestImages._Registrar(records=[], locators=set()))\nyield registrar\ninit_response = init_upload()\ndf = pd.DataFrame(registrar.data.records, columns=TEST_IMAGE_COLUMNS)\ndf[\"image_id\"] = -1\ndf_validated = TestImageDataFrame(validate_df_schema(df, TestImageDataFrameSchema))\ndf_serializable = df_validated.as_serializable()\nupload_data_frame(df=df_serializable, batch_size=BatchSize.UPLOAD_RECORDS.value, load_uuid=init_response.uuid)\nrequest = LoadAPI.WithLoadUUID(uuid=init_response.uuid)\nfinalize_res = krequests.put(\nendpoint_path=API.Path.COMPLETE_REGISTER.value,\ndata=json.dumps(dataclasses.asdict(request)),\n)\nkrequests.raise_for_status(finalize_res)\nlog.success(\"registered test images\")\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_images.TestImages.iter","title":"<code>iter(data_source=None, include_augmented=False, batch_size=10000000)</code>  <code>classmethod</code>","text":"<p>Iterator of DataFrames describing images registered in the Kolena platform.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>Optional[Union[str, TestSuite, TestSuite.Data, TestCase, TestCase.Data]]</code> <p>Optionally specify the single data source to be retrieved, e.g. <code>\"my-data-source\"</code>. Alternatively, provide a <code>TestSuite</code> or <code>TestCase</code> as source. If no argument is provided, all images registered using :meth:<code>TestImages.register</code> are returned.</p> <code>None</code> <code>include_augmented</code> <code>bool</code> <p>Optionally specify that augmented images should be returned. By default, only original images are returned. Ignored when test case or test suite is provided as <code>data_source</code>.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>Optionally specify maximum number of rows to be returned in a single DataFrame.</p> <code>10000000</code> Source code in <code>kolena/fr/test_images.py</code> <pre><code>@classmethod\n@validate_arguments(config=ValidatorConfig)\ndef iter(\ncls,\ndata_source: Optional[Union[str, TestSuite, TestSuite.Data, TestCase, TestCase.Data]] = None,\ninclude_augmented: bool = False,\nbatch_size: int = 10_000_000,\n) -&gt; Iterator[TestImageDataFrame]:\n\"\"\"\n    Iterator of DataFrames describing images registered in the Kolena platform.\n    :param data_source: Optionally specify the single data source to be retrieved, e.g. `\"my-data-source\"`.\n        Alternatively, provide a [`TestSuite`][kolena.fr.TestSuite] or [`TestCase`][kolena.fr.TestCase] as source.\n        If no argument is provided, all images registered using :meth:`TestImages.register` are returned.\n    :param include_augmented: Optionally specify that augmented images should be returned. By default, only\n        original images are returned. Ignored when test case or test suite is provided as `data_source`.\n    :param batch_size: Optionally specify maximum number of rows to be returned in a single DataFrame.\n    \"\"\"\ntest_suite_data = data_source.data if isinstance(data_source, TestSuite) else data_source\ntest_suite_id = test_suite_data.id if isinstance(test_suite_data, TestSuite.Data) else None\ntest_case_data = data_source.data if isinstance(data_source, TestCase) else data_source\ntest_case_id = test_case_data.id if isinstance(test_case_data, TestCase.Data) else None\ndata_source_display_name = cls._data_source_display_name(data_source, include_augmented)\nfrom_extra = f\" from '{data_source_display_name}'\" if data_source_display_name is not None else \"\"\nlog.info(f\"loading test images{from_extra}\")\ninit_request = API.InitLoadRequest(\ninclude_augmented=include_augmented,\ndata_source=data_source if isinstance(data_source, str) else None,\ntest_suite_id=test_suite_id,\ntest_case_id=test_case_id,\nbatch_size=batch_size,\n)\nyield from _BatchedLoader.iter_data(\ninit_request=init_request,\nendpoint_path=API.Path.INIT_LOAD_REQUEST.value,\ndf_class=TestImageDataFrame,\n)\nlog.info(f\"loaded test images{from_extra}\")\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase","title":"<code>TestCase(name, version=None, description=None, test_samples=None, reset=False)</code>","text":"<p>         Bases: <code>ABC</code>, <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A group of test samples that can be added to a :class:<code>kolena.fr.TestSuite</code>.</p> <p>The test case is the base unit of results computation in the Kolena platform. Metrics are computed by test case.</p> Source code in <code>kolena/fr/test_case.py</code> <pre><code>def __init__(\nself,\nname: str,\nversion: Optional[int] = None,\ndescription: Optional[str] = None,\ntest_samples: Optional[List[TestCaseRecord]] = None,\nreset: bool = False,\n):\ntry:\nself._populate_from_other(self.load(name, version))\nif description is not None and self.description != description and not reset:\nlog.warn(\"test case already exists, not updating description when reset=False\")\nif test_samples is not None:\nif self.version &gt; 0 and not reset:\nlog.warn(\"not updating test samples for test case that has already been edited when reset=False\")\nelse:\nself._hydrate(test_samples, description)\nexcept NotFoundError:\nif version is not None:\nlog.warn(f\"creating new test case '{name}', ignoring provided version\")\nself._populate_from_other(self.create(name, description, test_samples))\nself._freeze()\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>The unique name of this test case. Cannot be changed after creation.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of this test case. A test case's version is automatically incremented whenever it is edited via <code>TestCase.edit</code>.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form, human-readable description of this test case. Can be edited at any time via <code>TestCase.edit</code>.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.image_count","title":"<code>image_count: int</code>  <code>instance-attribute</code>","text":"<p>The number of images included in this test case.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.pair_count_genuine","title":"<code>pair_count_genuine: int</code>  <code>instance-attribute</code>","text":"<p>The number of genuine image pairs included in this test case.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.pair_count_imposter","title":"<code>pair_count_imposter: int</code>  <code>instance-attribute</code>","text":"<p>The number of imposter image pairs included in this test case.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.data","title":"<code>data: API.EntityData</code>  <code>property</code> <code>writable</code>","text":"<p>Deprecated: since <code>0.57.0</code></p> <p>Access this data via instance attributes, e.g. <code>self.image_count</code>, directly.</p> <p>The data associated with this test case.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.Editor","title":"<code>Editor(description, reset=False)</code>","text":"Source code in <code>kolena/fr/test_case.py</code> <pre><code>def __init__(self, description: str, reset: bool = False) -&gt; None:\nself._reset = reset\nself._description = description\nself._initial_description = description\nself._samples: Dict[str, TestCaseRecord] = OrderedDict()\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.Editor.description","title":"<code>description(description)</code>","text":"<p>Update the description of this test case.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>The new test case description.</p> required Source code in <code>kolena/fr/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef description(self, description: str) -&gt; None:\n\"\"\"\n    Update the description of this test case.\n    :param description: The new test case description.\n    \"\"\"\nself._description = description\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.Editor.add","title":"<code>add(locator_a, locator_b, is_same)</code>","text":"<p>Add the provided image pair to the test case.</p> <p>Note that if the image pair with <code>locator_a</code> and <code>locator_b</code> is already defined within the platform, the value for <code>is_same</code> must match the value already defined.</p> <p>Parameters:</p> Name Type Description Default <code>locator_a</code> <code>str</code> <p>The left locator for the image pair.</p> required <code>locator_b</code> <code>str</code> <p>The right locator for the image pair.</p> required <code>is_same</code> <code>bool</code> <p>Whether to treat this image pair as a a genuine pair (<code>True</code>) or an imposter pair (<code>False</code>).</p> required Source code in <code>kolena/fr/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef add(self, locator_a: str, locator_b: str, is_same: bool) -&gt; None:\n\"\"\"\n    Add the provided image pair to the test case.\n    Note that if the image pair with `locator_a` and `locator_b` is already defined within the platform,\n    the value for `is_same` must match the value already defined.\n    :param locator_a: The left locator for the image pair.\n    :param locator_b: The right locator for the image pair.\n    :param is_same: Whether to treat this image pair as a a genuine pair (`True`) or an imposter pair (`False`).\n    \"\"\"\nkey = self._key(locator_a, locator_b)\nval = (locator_a, locator_b, is_same)\nif val == self._samples.get(key, None):\nlog.info(f\"no op: {val} already in test case\")\nreturn\nself._samples[key] = val\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.Editor.remove","title":"<code>remove(locator_a, locator_b)</code>","text":"<p>Remove the provided pair from the test case.</p> <p>Parameters:</p> Name Type Description Default <code>locator_a</code> <code>str</code> <p>The left locator for the image pair.</p> required <code>locator_b</code> <code>str</code> <p>The right locator for the image pair.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If the provided locator pair is not in the test case.</p> Source code in <code>kolena/fr/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef remove(self, locator_a: str, locator_b: str) -&gt; None:\n\"\"\"\n    Remove the provided pair from the test case.\n    :param locator_a: The left locator for the image pair.\n    :param locator_b: The right locator for the image pair.\n    :raises KeyError: If the provided locator pair is not in the test case.\n    \"\"\"\nkey = self._key(locator_a, locator_b)\nif key not in self._samples.keys():\nraise KeyError(f\"pair not in test case: {locator_a}, {locator_b}\")\nself._samples.pop(key)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.create","title":"<code>create(name, description=None, test_samples=None)</code>  <code>classmethod</code>","text":"<p>Create a new test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new test case to create.</p> required <code>description</code> <code>Optional[str]</code> <p>Optional free-form description of the test case to create.</p> <code>None</code> <code>test_samples</code> <code>Optional[List[TestCaseRecord]]</code> <p>Optionally specify a set of test samples to populate the test case.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestCase</code> <p>The newly created test case.</p> Source code in <code>kolena/fr/test_case.py</code> <pre><code>@classmethod\ndef create(\ncls,\nname: str,\ndescription: Optional[str] = None,\ntest_samples: Optional[List[TestCaseRecord]] = None,\n) -&gt; \"TestCase\":\n\"\"\"\n    Create a new test case with the provided name.\n    :param name: The name of the new test case to create.\n    :param description: Optional free-form description of the test case to create.\n    :param test_samples: Optionally specify a set of test samples to populate the test case.\n    :return: The newly created test case.\n    \"\"\"\nrequest = API.CreateRequest(name=name, description=description or \"\")\nres = krequests.post(endpoint_path=API.Path.CREATE.value, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\ndata = from_dict(data_class=API.EntityData, data=res.json())\nobj = cls._create_from_data(data)\nlog.info(f\"created test case '{name}' (v{obj.version})\")\nif test_samples is not None:\nobj._hydrate(test_samples)\nreturn obj\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.load","title":"<code>load(name, version=None)</code>  <code>classmethod</code>","text":"<p>Load an existing test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test case to load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify a particular version of the test case to load. Defaults to the latest version when unset.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestCase</code> <p>The loaded test case.</p> Source code in <code>kolena/fr/test_case.py</code> <pre><code>@classmethod\ndef load(cls, name: str, version: Optional[int] = None) -&gt; \"TestCase\":\n\"\"\"\n    Load an existing test case with the provided name.\n    :param name: The name of the test case to load.\n    :param version: Optionally specify a particular version of the test case to load. Defaults to the latest version\n        when unset.\n    :return: The loaded test case.\n    \"\"\"\nreturn cls._load_by_name(name, version)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.load_by_name","title":"<code>load_by_name(name, version=None)</code>  <code>classmethod</code>","text":"<p>Deprecated: since <code>0.57.0</code></p> <p>Use <code>TestCase.load</code> instead.</p> <p>Load an existing test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test case to load.</p> required <code>version</code> <code>Optional[int]</code> <p>optionally specify the target version of the test case to load. When absent, the highest version of the test case with the provided name is returned</p> <code>None</code> <p>Returns:</p> Type Description <code>TestCase</code> <p>the loaded test case</p> Source code in <code>kolena/fr/test_case.py</code> <pre><code>@classmethod\n@deprecated(details=\"use :meth:`load` instead\", deprecated_in=\"0.57.0\")\ndef load_by_name(cls, name: str, version: Optional[int] = None) -&gt; \"TestCase\":\n\"\"\"\n    !!! warning \"Deprecated: since `0.57.0`\"\n        Use [`TestCase.load`][kolena.fr.TestCase.load] instead.\n    Load an existing test case with the provided name.\n    :param name: The name of the test case to load.\n    :param version: optionally specify the target version of the test case to load. When absent, the highest version\n        of the test case with the provided name is returned\n    :return: the loaded test case\n    \"\"\"\nreturn cls.load(name, version)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.load_data","title":"<code>load_data()</code>","text":"<p>Load all image pairs for a test case.</p> <p>Returns:</p> Type Description <code>TestCaseDataFrame</code> <p>DataFrame containing all pairs defined in this test case.</p> Source code in <code>kolena/fr/test_case.py</code> <pre><code>def load_data(self) -&gt; TestCaseDataFrame:\n\"\"\"\n    Load all image pairs for a test case.\n    :return: DataFrame containing all pairs defined in this test case.\n    \"\"\"\nreturn _BatchedLoader.concat(self.iter_data(), TestCaseDataFrame)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.edit","title":"<code>edit(reset=False)</code>","text":"<p>Edit this test case in a context:</p> <pre><code>with test_case.edit() as editor:\n# perform as many editing actions as desired\neditor.add(...)\neditor.remove(...)\n</code></pre> <p>Changes are committed to the Kolena platform when the context is exited.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>Clear any and all test samples currently in the test case.</p> <code>False</code> Source code in <code>kolena/fr/test_case.py</code> <pre><code>@contextmanager\ndef edit(self, reset: bool = False) -&gt; Iterator[Editor]:\n\"\"\"\n    Edit this test case in a context:\n    ```python\n    with test_case.edit() as editor:\n        # perform as many editing actions as desired\n        editor.add(...)\n        editor.remove(...)\n    ```\n    Changes are committed to the Kolena platform when the context is exited.\n    :param reset: Clear any and all test samples currently in the test case.\n    \"\"\"\neditor = self.Editor(self.description, reset)\nif not reset:\ndf_existing = self.load_data()\n# avoid calling the expensive self.load_data() multiple times\n_initial_samples: Dict[str, TestCaseRecord] = OrderedDict()\nfor record in df_existing.itertuples():\nlocator_a, locator_b, is_same = record.locator_a, record.locator_b, record.is_same\ntest_case_record = (locator_a, locator_b, is_same)\neditor.add(*test_case_record)\n_initial_samples[editor._key(locator_a, locator_b)] = test_case_record\neditor._initial_samples = _initial_samples\nyield editor\n# no-op contexts have no effect, do not bump version\nif not editor._edited():\nreturn\nlog.info(f\"editing test case '{self.name}' (v{self.version})\")\ninit_response = init_upload()\ndf = pd.DataFrame(editor._samples.values(), columns=TEST_CASE_COLUMNS)\ndf_validated = validate_df_schema(df, TestCaseDataFrameSchema)\nupload_data_frame(df=df_validated, batch_size=BatchSize.UPLOAD_RECORDS.value, load_uuid=init_response.uuid)\nrequest = API.CompleteEditRequest(\ntest_case_id=self._id,\ncurrent_version=self.version,\nname=self.name,\ndescription=editor._description,\nuuid=init_response.uuid,\n)\ncomplete_res = krequests.post(\nendpoint_path=API.Path.COMPLETE_EDIT.value,\ndata=json.dumps(dataclasses.asdict(request)),\n)\nkrequests.raise_for_status(complete_res)\ntest_case_data = from_dict(data_class=API.EntityData, data=complete_res.json())\nself._populate_from_other(self._create_from_data(test_case_data))\nlog.success(f\"edited test case '{self.name}' (v{self.version})\")\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_case.TestCase.iter_data","title":"<code>iter_data(batch_size=10000000)</code>","text":"<p>Iterator of DataFrames describing all pairs data for a test case.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Optionally specify maximum number of rows to be returned in a single DataFrame.</p> <code>10000000</code> Source code in <code>kolena/fr/test_case.py</code> <pre><code>@validate_arguments\ndef iter_data(self, batch_size: int = 10_000_000) -&gt; Iterator[TestCaseDataFrame]:\n\"\"\"\n    Iterator of DataFrames describing all pairs data for a test case.\n    :param batch_size: Optionally specify maximum number of rows to be returned in a single DataFrame.\n    \"\"\"\nlog.info(f\"loading image pairs in test case '{self.name}' (v{self.version})\")\ninit_request = API.InitLoadDataRequest(batch_size=batch_size, test_case_id=self._id)\nyield from _BatchedLoader.iter_data(\ninit_request=init_request,\nendpoint_path=API.Path.INIT_LOAD_DATA.value,\ndf_class=TestCaseDataFrame,\n)\nlog.info(f\"loaded image pairs in test case '{self.name}' (v{self.version})\")\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite","title":"<code>TestSuite(name, version=None, description=None, baseline_test_cases=None, non_baseline_test_cases=None, reset=False)</code>","text":"<p>         Bases: <code>ABC</code>, <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A test suite groups together one or more test cases.</p> Source code in <code>kolena/fr/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nname: str,\nversion: Optional[int] = None,\ndescription: Optional[str] = None,\nbaseline_test_cases: Optional[List[TestCase]] = None,\nnon_baseline_test_cases: Optional[List[TestCase]] = None,\nreset: bool = False,\n):\ntry:\nself._populate_from_other(self.load(name, version))\nif description is not None and self.description != description and not reset:\nlog.warn(\"test suite already exists, not updating description when reset=False\")\nif baseline_test_cases is not None or non_baseline_test_cases is not None:\nif self.version &gt; 0 and not reset:\nlog.warn(\"test suite already exists, not updating test cases when reset=False\")\nelse:\nself._hydrate(baseline_test_cases, non_baseline_test_cases, description)\nexcept NotFoundError:\nif version is not None:\nlog.warn(f\"creating new test suite '{name}', ignoring provided version\")\nself._populate_from_other(self.create(name, description, baseline_test_cases, non_baseline_test_cases))\nself._freeze()\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>The unique name of this test suite.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of this test suite. A test suite's version is automatically incremented whenever it is edited via <code>TestSuite.edit</code>.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form, human-readable description of this test suite. Can be edited at any time via <code>TestSuite.edit</code>.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.baseline_test_cases","title":"<code>baseline_test_cases: List[TestCase]</code>  <code>instance-attribute</code>","text":"<p>The baseline <code>TestCase</code> object(s) for this test suite.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.non_baseline_test_cases","title":"<code>non_baseline_test_cases: List[TestCase]</code>  <code>instance-attribute</code>","text":"<p>The non-baseline <code>TestCase</code> object(s) for this test suite.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.baseline_image_count","title":"<code>baseline_image_count: int</code>  <code>instance-attribute</code>","text":"<p>The number of images attached to the baseline test case(s).</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.baseline_pair_count_genuine","title":"<code>baseline_pair_count_genuine: int</code>  <code>instance-attribute</code>","text":"<p>The number of genuine pairs attached to the baseline test case(s).</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.baseline_pair_count_imposter","title":"<code>baseline_pair_count_imposter: int</code>  <code>instance-attribute</code>","text":"<p>The count of imposter pairs attached to the baseline test case(s).</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.data","title":"<code>data: API.EntityData</code>  <code>property</code> <code>writable</code>","text":"<p>Deprecated: since <code>0.57.0</code></p> <p>Access this data via instance attributes, e.g. <code>self.baseline_test_cases</code>, directly.</p> <p>The data associated with this test suite.</p>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.Editor","title":"<code>Editor(description, reset)</code>","text":"<p>Interface to edit a test suite. Create with :meth:<code>TestSuite.edit</code>.</p> Source code in <code>kolena/fr/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, description: str, reset: bool) -&gt; None:\nself._baseline_test_cases: Dict[str, int] = OrderedDict()  # map from name -&gt; id\nself._non_baseline_test_cases: Dict[str, int] = OrderedDict()  # map from name -&gt; id\nself._reset = reset\nself._description = description\nself._initial_description = description\nself._edited = False\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.Editor.description","title":"<code>description(description)</code>","text":"<p>Update the description of the test suite.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>The new description of the test suite.</p> required Source code in <code>kolena/fr/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef description(self, description: str) -&gt; None:\n\"\"\"\n    Update the description of the test suite.\n    :param description: The new description of the test suite.\n    \"\"\"\nself._description = description\nself._edited = True\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.Editor.add","title":"<code>add(test_case, is_baseline=None)</code>","text":"<p>Add a test case to this test suite. If a different version of the test case already exists in this test suite, it is replaced and its baseline status will be propagated when <code>is_baseline</code> is unset.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case to add to the test suite.</p> required <code>is_baseline</code> <code>Optional[bool]</code> <p>Specify that this test case is a part of the \"baseline,\" i.e. if the samples in this test case should contribute to the computation of thresholds within this test suite.</p> <code>None</code> Source code in <code>kolena/fr/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef add(self, test_case: TestCase, is_baseline: Optional[bool] = None) -&gt; None:\n\"\"\"\n    Add a test case to this test suite. If a different version of the test case already exists in this test\n    suite, it is replaced and its baseline status will be propagated when `is_baseline` is unset.\n    :param test_case: The test case to add to the test suite.\n    :param is_baseline: Specify that this test case is a part of the \"baseline,\" i.e. if the samples in this\n        test case should contribute to the computation of thresholds within this test suite.\n    \"\"\"\nname = test_case.name\n# clean up any previous versions and propagates its baseline status\nset_is_baseline: Optional[bool] = None\nif name in self._baseline_test_cases.keys():\nself._baseline_test_cases.pop(name)\nset_is_baseline = is_baseline if is_baseline is not None else True\nif name in self._non_baseline_test_cases.keys():\nself._non_baseline_test_cases.pop(name)\nset_is_baseline = is_baseline if is_baseline is not None else False\nis_baseline_ret = set_is_baseline or is_baseline or False\nself._add(test_case, is_baseline=is_baseline_ret)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.Editor.remove","title":"<code>remove(test_case)</code>","text":"<p>Remove the provided <code>TestCase</code> from the test suite. Any version of this test case in the suite will be removed; the version does not need to match exactly.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case to remove.</p> required Source code in <code>kolena/fr/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef remove(self, test_case: TestCase) -&gt; None:\n\"\"\"\n    Remove the provided [`TestCase`][kolena.fr.TestCase] from the test suite. Any version of this test case in\n    the suite will be removed; the version does not need to match exactly.\n    :param test_case: The test case to remove.\n    \"\"\"\nname = test_case.name\nif name in self._baseline_test_cases.keys():\nself._baseline_test_cases.pop(name)\nelif name in self._non_baseline_test_cases.keys():\nself._non_baseline_test_cases.pop(name)\nelse:\nraise KeyError(f\"test case '{name}' not in test suite\")\nself._edited = True\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.Editor.merge","title":"<code>merge(test_case, is_baseline=None)</code>","text":"<p>Add the <code>TestCase</code> to the suite. If a test case by this name already exists in the suite, replace the previous version of that test case with the newly provided version.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case to be merged into the test suite.</p> required <code>is_baseline</code> <code>Optional[bool]</code> <p>Optionally specify whether or not this test case should be considered as a part of the baseline for this test suite. When not specified, the previous value for <code>is_baseline</code> for this test case in this test suite is propagated forward. Defaults to false if the test case does not already exist in this suite.</p> <code>None</code> Source code in <code>kolena/fr/test_suite.py</code> <pre><code>@deprecated(details=\"use :meth:`add` instead\", deprecated_in=\"0.57.0\")\n@validate_arguments(config=ValidatorConfig)\ndef merge(self, test_case: TestCase, is_baseline: Optional[bool] = None) -&gt; None:\n\"\"\"\n    Add the [`TestCase`][kolena.fr.TestCase] to the suite. If a test case by this name already exists in the\n    suite, replace the previous version of that test case with the newly provided version.\n    :param test_case: The test case to be merged into the test suite.\n    :param is_baseline: Optionally specify whether or not this test case should be considered as a part of the\n        baseline for this test suite. When not specified, the previous value for `is_baseline` for this test\n        case in this test suite is propagated forward. Defaults to false if the test case does not already exist\n        in this suite.\n    \"\"\"\nself.add(test_case, is_baseline)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.create","title":"<code>create(name, description=None, baseline_test_cases=None, non_baseline_test_cases=None)</code>  <code>classmethod</code>","text":"<p>Create a new test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new test suite to create.</p> required <code>description</code> <code>Optional[str]</code> <p>Optional free-form description of the test suite to create.</p> <code>None</code> <code>baseline_test_cases</code> <code>Optional[List[TestCase]]</code> <p>Optionally specify a list of test cases to use as baseline for the test suite.</p> <code>None</code> <code>non_baseline_test_cases</code> <code>Optional[List[TestCase]]</code> <p>Optionally specify a list of test cases to populate the test suite.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestSuite</code> <p>The newly created test suite.</p> Source code in <code>kolena/fr/test_suite.py</code> <pre><code>@classmethod\ndef create(\ncls,\nname: str,\ndescription: Optional[str] = None,\nbaseline_test_cases: Optional[List[TestCase]] = None,\nnon_baseline_test_cases: Optional[List[TestCase]] = None,\n) -&gt; \"TestSuite\":\n\"\"\"\n    Create a new test suite with the provided name.\n    :param name: The name of the new test suite to create.\n    :param description: Optional free-form description of the test suite to create.\n    :param baseline_test_cases: Optionally specify a list of test cases to use as baseline for the test suite.\n    :param non_baseline_test_cases: Optionally specify a list of test cases to populate the test suite.\n    :return: The newly created test suite.\n    \"\"\"\nrequest = API.CreateRequest(name=name, description=description or \"\")\nres = krequests.post(endpoint_path=API.Path.CREATE.value, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\ndata = from_dict(data_class=API.EntityData, data=res.json())\nobj = cls._create_from_data(data)\nlog.info(f\"created test suite '{name}' (v{obj.version}) ({get_test_suite_url(obj._id)})\")\nobj._hydrate(baseline_test_cases, non_baseline_test_cases)\nreturn obj\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.load","title":"<code>load(name, version=None)</code>  <code>classmethod</code>","text":"<p>Load an existing test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>the name of the test suite to load.</p> required <code>version</code> <code>Optional[int]</code> <p>optionally specify a particular version of the test suite to load. Defaults to the latest version when unset.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestSuite</code> <p>the loaded test suite.</p> Source code in <code>kolena/fr/test_suite.py</code> <pre><code>@classmethod\ndef load(cls, name: str, version: Optional[int] = None) -&gt; \"TestSuite\":\n\"\"\"\n    Load an existing test suite with the provided name.\n    :param name: the name of the test suite to load.\n    :param version: optionally specify a particular version of the test suite to load. Defaults to the latest\n        version when unset.\n    :return: the loaded test suite.\n    \"\"\"\nreturn cls._load_by_name(name, version)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.load_by_name","title":"<code>load_by_name(name, version=None)</code>  <code>classmethod</code>","text":"<p>Retrieve the existing test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the test suite to retrieve.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify the version of the named test suite to retrieve. When absent the latest version of the test suite is returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestSuite</code> <p>The retrieved test suite.</p> <p>Raises:</p> Type Description <code>NotFoundError</code> <p>If the test suite with the provided name doesn't exist.</p> Source code in <code>kolena/fr/test_suite.py</code> <pre><code>@classmethod\n@deprecated(details=\"use :meth:`load` instead\", deprecated_in=\"0.57.0\")\ndef load_by_name(cls, name: str, version: Optional[int] = None) -&gt; \"TestSuite\":\n\"\"\"\n    Retrieve the existing test suite with the provided name.\n    :param name: Name of the test suite to retrieve.\n    :param version: Optionally specify the version of the named test suite to retrieve. When absent the latest\n        version of the test suite is returned.\n    :return: The retrieved test suite.\n    :raises NotFoundError: If the test suite with the provided name doesn't exist.\n    \"\"\"\nreturn cls.load(name, version)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_suite.TestSuite.edit","title":"<code>edit(reset=False)</code>","text":"<p>Context-managed way to perform many modification options on a test suite and commit the results when the context is exited, resulting in a single version bump.</p> Source code in <code>kolena/fr/test_suite.py</code> <pre><code>@contextmanager\ndef edit(self, reset: bool = False) -&gt; Iterator[Editor]:\n\"\"\"\n    Context-managed way to perform many modification options on a test suite and commit the results when the context\n    is exited, resulting in a single version bump.\n    \"\"\"\neditor = TestSuite.Editor(self.description, reset)\nif not reset:\nfor baseline_test_case in self.baseline_test_cases:\neditor.add(baseline_test_case, is_baseline=True)\nfor non_baseline_test_case in self.non_baseline_test_cases:\neditor.add(non_baseline_test_case, is_baseline=False)\neditor._edited = False\nyield editor\n# no-op contexts have no effect, do not bump version\nif not editor._edited:\nreturn\nlog.info(f\"editing test suite '{self.name}' (v{self.version})\")\nrequest = API.EditRequest(\ntest_suite_id=self._id,\ncurrent_version=self.version,\nname=self.name,\ndescription=editor._description,\nbaseline_test_case_ids=list(editor._baseline_test_cases.values()),\nnon_baseline_test_case_ids=list(editor._non_baseline_test_cases.values()),\n)\nres = krequests.post(endpoint_path=API.Path.EDIT.value, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\ntest_suite_data = from_dict(data_class=API.EntityData, data=res.json())\nself._populate_from_other(self._create_from_data(test_suite_data))\nlog.success(f\"edited test suite '{self.name}' (v{self.version}) ({get_test_suite_url(self._id)})\")\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_run.TestRun","title":"<code>TestRun(model, test_suite, reset=False)</code>","text":"<p>         Bases: <code>ABC</code>, <code>Frozen</code>, <code>WithTelemetry</code></p> <p>Interface to test a <code>Model</code> on a <code>TestSuite</code>. Any in-progress tests for this model on this test suite is resumed.</p> <p>For a streamlined interface, see <code>test</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model being tested.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>reset</code> <code>bool</code> <p>Overwrites existing inferences if set.</p> <code>False</code> Source code in <code>kolena/fr/test_run.py</code> <pre><code>def __init__(self, model: Model, test_suite: TestSuite, reset: bool = False):\nif reset:\nlog.warn(\"overwriting existing inferences from this model (reset=True)\")\nelse:\nlog.info(\"not overwriting any existing inferences from this model (reset=False)\")\nrequest = API.CreateOrRetrieveRequest(model_id=model.data.id, test_suite_ids=[test_suite._id], reset=reset)\nres = krequests.post(\nendpoint_path=API.Path.CREATE_OR_RETRIEVE.value,\ndata=json.dumps(dataclasses.asdict(request)),\n)\nkrequests.raise_for_status(res)\nresponse = from_dict(data_class=TestRun.Data, data=res.json())\nself.data = response\nself._id = response.id\nself._model = model\nself._test_suite = test_suite\nself._reset = reset\nself._freeze()\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_run.TestRun.create_or_retrieve","title":"<code>create_or_retrieve(model, test_suite, reset=False)</code>  <code>classmethod</code>","text":"<p>Deprecated: since <code>0.57.0</code></p> <p>Use the <code>TestRun</code> constructor instead.</p> <p>Create a new test run for the provided <code>Model</code> on the provided <code>TestSuite</code>. If a test run for this model on this suite already exists, it is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model being tested.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>reset</code> <code>bool</code> <p>Overwrites existing inferences if set.</p> <code>False</code> <p>Returns:</p> Type Description <code>TestRun</code> <p>The created or retrieved test run.</p> Source code in <code>kolena/fr/test_run.py</code> <pre><code>@classmethod\n@deprecated(details=\"use initializer :class:`kolena.fr.TestRun` directly\", deprecated_in=\"0.58.0\")\ndef create_or_retrieve(cls, model: Model, test_suite: TestSuite, reset: bool = False) -&gt; \"TestRun\":\n\"\"\"\n    !!! warning \"Deprecated: since `0.57.0`\"\n        Use the [`TestRun`][kolena.fr.TestRun] constructor instead.\n    Create a new test run for the provided [`Model`][kolena.fr.Model] on the provided\n    [`TestSuite`][kolena.fr.TestSuite]. If a test run for this model on this suite already exists, it is returned.\n    :param model: The model being tested.\n    :param test_suite: The test suite on which to test the model.\n    :param reset: Overwrites existing inferences if set.\n    :return: The created or retrieved test run.\n    \"\"\"\nreturn TestRun(model, test_suite, reset=reset)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_run.TestRun.load_remaining_images","title":"<code>load_remaining_images(batch_size=10000000)</code>","text":"<p>Load a DataFrame containing records for each of the images in the configured test suite that does not yet have results from the configured model.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Optionally specify the maximum number of image records to return.</p> <code>10000000</code> <p>Returns:</p> Type Description <code>ImageDataFrame</code> <p>DataFrame containing records for each of the images that must be processed.</p> <p>Raises:</p> Type Description <code>InputValidationError</code> <p>The requested <code>batch_size</code> failed validation.</p> <code>RemoteError</code> <p>Images could not be loaded for any reason.</p> Source code in <code>kolena/fr/test_run.py</code> <pre><code>@validate_arguments\ndef load_remaining_images(self, batch_size: int = 10_000_000) -&gt; ImageDataFrame:\n\"\"\"\n    Load a DataFrame containing records for each of the images in the configured test suite that does not yet have\n    results from the configured model.\n    :param batch_size: Optionally specify the maximum number of image records to return.\n    :return: DataFrame containing records for each of the images that must be processed.\n    :raises InputValidationError: The requested `batch_size` failed validation.\n    :raises RemoteError: Images could not be loaded for any reason.\n    \"\"\"\nif batch_size &lt;= 0:\nraise InputValidationError(f\"invalid batch_size '{batch_size}': expected positive integer\")\nlog.info(\"loading remaining images for test run\")\ninit_request = API.InitLoadRemainingImagesRequest(\ntest_run_id=self.data.id,\nbatch_size=batch_size,\nload_all=self._reset,\n)\nwith krequests.put(\nendpoint_path=API.Path.INIT_LOAD_REMAINING_IMAGES.value,\ndata=json.dumps(dataclasses.asdict(init_request)),\nstream=True,\n) as init_res:\nkrequests.raise_for_status(init_res)\nload_uuid = None\ntry:\ndfs: List[ImageDataFrame] = []\nfor line in init_res.iter_lines():\npartial_response = from_dict(data_class=LoadAPI.InitDownloadPartialResponse, data=json.loads(line))\nload_uuid = partial_response.uuid\ndfs.append(_BatchedLoader.load_path(partial_response.path, ImageDataFrame))\nlog.info(\"loaded remaining images for test run\")\nreturn _BatchedLoader.concat(dfs, ImageDataFrame)\nfinally:\n_BatchedLoader.complete_load(load_uuid)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_run.TestRun.upload_image_results","title":"<code>upload_image_results(df_image_result)</code>","text":"<p>Upload inference results for a batch of images.</p> <p>All columns except for <code>image_id</code> and <code>embedding</code> are optional. An empty <code>embedding</code> cell in a record indicates a failure to enroll. The <code>failure_reason</code> column can optionally be specified for failures to enroll.</p> <p>To provide more than one embedding extracted from a given image, include multiple records with the same <code>image_id</code> in <code>df_image_result</code> (one for each embedding extracted). Records for a given <code>image_id</code> must be submitted in the same <code>df_image_result</code> DataFrame, and not across multiple calls to <code>upload_image_results</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df_image_result</code> <code>ImageResultDataFrame</code> <p>DataFrame of any size containing records describing inference results for an image.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of records successfully uploaded.</p> <p>Raises:</p> Type Description <code>TypeValidationError</code> <p>The DataFrame failed type validation.</p> <code>RemoteError</code> <p>The DataFrame was unable to be successfully ingested for any reason.</p> Source code in <code>kolena/fr/test_run.py</code> <pre><code>def upload_image_results(self, df_image_result: ImageResultDataFrame) -&gt; int:\n\"\"\"\n    Upload inference results for a batch of images.\n    All columns except for `image_id` and `embedding` are optional. An empty `embedding` cell in a record\n    indicates a failure to enroll. The `failure_reason` column can optionally be specified for failures to enroll.\n    To provide more than one embedding extracted from a given image, include multiple records with the same\n    `image_id` in `df_image_result` (one for each embedding extracted). Records for a given `image_id` must\n    be submitted in the same `df_image_result` DataFrame, and **not** across multiple calls to\n    `upload_image_results`.\n    :param df_image_result: DataFrame of any size containing records describing inference results for an image.\n    :return: Number of records successfully uploaded.\n    :raises TypeValidationError: The DataFrame failed type validation.\n    :raises RemoteError: The DataFrame was unable to be successfully ingested for any reason.\n    \"\"\"\nlog.info(\"uploading inference results for test run\")\ninit_response = init_upload()\nasset_config_res = krequests.get(endpoint_path=AssetAPI.Path.CONFIG.value)\nkrequests.raise_for_status(asset_config_res)\nasset_config = from_dict(data_class=AssetAPI.Config, data=asset_config_res.json())\nasset_path_mapper = AssetPathMapper(asset_config)\ndf_validated = ImageResultDataFrame(validate_df_schema(df_image_result, ImageResultDataFrameSchema))\nvalidate_df_record_count(df_validated)\ndf_image_chips = _ImageChipsDataFrame.from_image_result_data_frame(\ntest_run_id=self.data.id,\nload_uuid=init_response.uuid,\ndf=df_validated,\n)\nupload_image_chips(df_image_chips)\ndf_result_stage = _ResultStageFrame.from_image_result_data_frame(\ntest_run_id=self.data.id,\nload_uuid=init_response.uuid,\ndf=df_validated,\npath_mapper=asset_path_mapper,\n)\nupload_data_frame(df_result_stage, BatchSize.UPLOAD_RECORDS.value, init_response.uuid)\nrequest = API.UploadImageResultsRequest(uuid=init_response.uuid, test_run_id=self.data.id, reset=self._reset)\nfinalize_res = krequests.put(\nendpoint_path=API.Path.COMPLETE_UPLOAD_IMAGE_RESULTS.value,\ndata=json.dumps(dataclasses.asdict(request)),\n)\nkrequests.raise_for_status(finalize_res)\nresponse = from_dict(data_class=API.UploadImageResultsResponse, data=finalize_res.json())\nlog.success(\"uploaded inference results for test run\")\nreturn response.n_uploaded\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_run.TestRun.load_remaining_pairs","title":"<code>load_remaining_pairs(batch_size=10000000)</code>","text":"<p>Load DataFrames containing computed embeddings and records for each of the image pairs in the configured test suite that have not yet had similarity scores computed.</p> <p>This method should not be called until all images in the <code>TestRun</code> have been processed.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Optionally specify the maximum number of image pair records to return.</p> <code>10000000</code> <p>Returns:</p> Type Description <code>Tuple[EmbeddingDataFrame, PairDataFrame]</code> <p>Two DataFrames, one containing embeddings computed in the previous step (<code>df_embedding</code>) and one containing records for each of the image pairs that must be computed (<code>df_pair</code>). See documentation on <code>EmbeddingDataFrameSchema</code> for expected format when multiple embeddings were uploaded from a single image in <code>TestRun.upload_image_results</code>.</p> <p>Raises:</p> Type Description <code>InputValidationError</code> <p>The requested <code>batch_size</code> failed validation.</p> <code>RemoteError</code> <p>Pairs could not be loaded for any reason.</p> Source code in <code>kolena/fr/test_run.py</code> <pre><code>@validate_arguments\ndef load_remaining_pairs(self, batch_size: int = 10_000_000) -&gt; Tuple[EmbeddingDataFrame, PairDataFrame]:\n\"\"\"\n    Load DataFrames containing computed embeddings and records for each of the image pairs in the configured test\n    suite that have not yet had similarity scores computed.\n    This method should not be called until all images in the [`TestRun`][kolena.fr.TestRun] have been processed.\n    :param batch_size: Optionally specify the maximum number of image pair records to return.\n    :return: Two DataFrames, one containing embeddings computed in the previous step (`df_embedding`) and one\n        containing records for each of the image pairs that must be computed (`df_pair`). See documentation on\n        [`EmbeddingDataFrameSchema`][kolena.fr.datatypes.EmbeddingDataFrameSchema] for expected format when multiple\n        embeddings were uploaded from a single image in\n        [`TestRun.upload_image_results`][kolena.fr.TestRun.upload_image_results].\n    :raises InputValidationError: The requested `batch_size` failed validation.\n    :raises RemoteError: Pairs could not be loaded for any reason.\n    \"\"\"\nif batch_size &lt;= 0:\nraise InputValidationError(f\"invalid batch_size '{batch_size}': expected positive integer\")\nlog.info(\"loading batch of image pairs for test run\")\ninit_request = API.InitLoadRemainingPairsRequest(\ntest_run_id=self.data.id,\nbatch_size=batch_size,\nload_all=self._reset,\n)\nwith krequests.put(\nendpoint_path=API.Path.INIT_LOAD_REMAINING_PAIRS.value,\ndata=json.dumps(dataclasses.asdict(init_request)),\nstream=True,\n) as init_res:\nkrequests.raise_for_status(init_res)\nload_uuid_embedding = None\nload_uuid_pair = None\ntry:\ndfs_embedding = []\ndfs_pair = []\nfor line in init_res.iter_lines():\npartial_response = from_dict(\ndata_class=API.InitLoadRemainingPairsPartialResponse,\ndata=json.loads(line),\n)\nload_uuid_embedding = partial_response.embeddings.uuid\ndfs_embedding.append(_BatchedLoader.load_path(partial_response.embeddings.path, EmbeddingDataFrame))\nload_uuid_pair = partial_response.pairs.uuid\ndfs_pair.append(_BatchedLoader.load_path(partial_response.pairs.path, PairDataFrame))\ndf_embedding = _BatchedLoader.concat(dfs_embedding, EmbeddingDataFrame)\ndf_pair = _BatchedLoader.concat(dfs_pair, PairDataFrame)\nlog.info(\"loaded batch of image pairs for test run\")\nreturn df_embedding, df_pair\nfinally:\nfor uuid in [load_uuid_embedding, load_uuid_pair]:\n_BatchedLoader.complete_load(uuid)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_run.TestRun.upload_pair_results","title":"<code>upload_pair_results(df_pair_result)</code>","text":"<p>Upload image pair similarity results for a batch of pairs.</p> <p>This method should not be called until all images in the TestRun have been processed.</p> <p>All columns except for <code>image_pair_id</code> and <code>similarity</code> are optional. An empty <code>similarity</code> cell in a record indicates a pair failure (i.e. one or more of the images in the pair failed to enroll).</p> <p>For image pairs containing images with more than one embedding, a single record may be provided with the highest similarity score, or <code>M x N</code> records may be provided for each embeddings combination in the pair, when there are <code>M</code> embeddings from <code>image_a</code> and <code>N</code> embeddings from <code>image_b</code>.</p> <p>When providing multiple records for a given image pair, use the <code>embedding_a_index</code> and <code>embedding_b_index</code> columns to indicate which embeddings were used to compute a given similarity score. Records for a given image pair must be submitted in the same <code>df_pair_result</code> DataFrame, and not across multiple calls to <code>upload_pair_results</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df_pair_result</code> <code>PairResultDataFrame</code> <p>DataFrame containing records describing the similarity score of a pair of images.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of records successfully uploaded.</p> <p>Raises:</p> Type Description <code>TypeValidationError</code> <p>The DataFrame failed type validation.</p> <code>RemoteError</code> <p>The DataFrame was unable to be successfully ingested for any reason.</p> Source code in <code>kolena/fr/test_run.py</code> <pre><code>def upload_pair_results(self, df_pair_result: PairResultDataFrame) -&gt; int:\n\"\"\"\n    Upload image pair similarity results for a batch of pairs.\n    This method should not be called until all images in the TestRun have been processed.\n    All columns except for `image_pair_id` and `similarity` are optional. An empty `similarity` cell in a\n    record indicates a pair failure (i.e. one or more of the images in the pair failed to enroll).\n    For image pairs containing images with more than one embedding, a single record may be provided with the highest\n    similarity score, or `M x N` records may be provided for each embeddings combination in the pair, when there\n    are `M` embeddings from `image_a` and `N` embeddings from `image_b`.\n    When providing multiple records for a given image pair, use the `embedding_a_index` and `embedding_b_index`\n    columns to indicate which embeddings were used to compute a given similarity score. Records for a given image\n    pair must be submitted in the same `df_pair_result` DataFrame, and **not** across multiple calls to\n    `upload_pair_results`.\n    :param df_pair_result: DataFrame containing records describing the similarity score of a pair of images.\n    :return: Number of records successfully uploaded.\n    :raises TypeValidationError: The DataFrame failed type validation.\n    :raises RemoteError: The DataFrame was unable to be successfully ingested for any reason.\n    \"\"\"\nlog.info(\"uploading pair results for test run\")\ninit_response = init_upload()\ndf_validated = validate_df_schema(df_pair_result, PairResultDataFrameSchema)\nvalidate_df_record_count(df_validated)\nupload_data_frame(df_validated, BatchSize.UPLOAD_RECORDS.value, init_response.uuid)\nrequest = API.UploadPairResultsRequest(uuid=init_response.uuid, test_run_id=self.data.id, reset=self._reset)\nfinalize_res = krequests.put(\nendpoint_path=API.Path.COMPLETE_UPLOAD_PAIR_RESULTS.value,\ndata=json.dumps(dataclasses.asdict(request)),\n)\nkrequests.raise_for_status(finalize_res)\nlog.success(\"uploaded pair results for test run\")\nresponse = from_dict(data_class=API.UploadPairResultsResponse, data=finalize_res.json())\nreturn response.n_uploaded\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.test_run.test","title":"<code>test(model, test_suite, reset=False)</code>","text":"<p>Test the provided <code>InferenceModel</code> on the provided <code>TestSuite</code>. Any tests already in progress for this model on these suites are resumed.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>InferenceModel</code> <p>The model being tested, implementing both <code>extract</code> and <code>compare</code> methods.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>reset</code> <code>bool</code> <p>Overwrites existing inferences if set.</p> <code>False</code> Source code in <code>kolena/fr/test_run.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef test(model: InferenceModel, test_suite: TestSuite, reset: bool = False) -&gt; None:\n\"\"\"\n    Test the provided [`InferenceModel`][kolena.fr.InferenceModel] on the provided [`TestSuite`][kolena.fr.TestSuite].\n    Any tests already in progress for this model on these suites are resumed.\n    :param model: The model being tested, implementing both `extract` and `compare` methods.\n    :param test_suite: The test suite on which to test the model.\n    :param reset: Overwrites existing inferences if set.\n    \"\"\"\ntest_run = TestRun(model, test_suite, reset=reset)\ntry:\nlog.info(\"starting test run\")\ndf_image = test_run.load_remaining_images(int(1e12))\ndf_image[\"embedding\"] = [\nmodel.extract(record.locator)\nfor record in tqdm(df_image.itertuples(), position=0, leave=True, total=len(df_image))\n]\nempty_columns = [  # columns not populated in this implementation\n\"bounding_box\",\n\"landmarks_input_image\",\n\"landmarks\",\n\"quality_input_image\",\n\"quality\",\n\"acceptability\",\n\"fr_input_image\",\n\"failure_reason\",\n]\ndf_image[empty_columns] = None\ndf_image_result = df_image[[\"image_id\", \"embedding\", *empty_columns]]\nif len(df_image_result) &gt; 0:  # only attempt to upload if this step has not been completed\ntest_run.upload_image_results(df_image_result)\ndf_embedding, df_pair = test_run.load_remaining_pairs(int(1e12))\nembedding_by_id = {record.image_id: record.embedding for record in df_embedding.itertuples()}\ndf_pair[\"similarity\"] = [\nmodel.compare(embedding_by_id[record.image_a_id], embedding_by_id[record.image_b_id])\nfor record in tqdm(df_pair.itertuples(), position=0, leave=True, total=len(df_pair))\n]\ndf_pair_result = df_pair[[\"image_pair_id\", \"similarity\"]]\nif len(df_pair_result) &gt; 0:\ntest_run.upload_pair_results(df_pair_result)\nlog.success(\"completed test run\")\nexcept Exception as e:\nreport_crash(test_run.data.id, API.Path.MARK_CRASHED.value)\nraise e\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.model.Model","title":"<code>Model</code>","text":"<p>         Bases: <code>Uninstantiable['Model.Data']</code></p> <p>The descriptor for your model within the Kolena platform.</p>"},{"location":"reference/built-in/fr/#kolena.fr.model.Model.create","title":"<code>create(name, metadata)</code>  <code>classmethod</code>","text":"<p>Create a new model with the provided name and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name of the new model to create.</p> required <code>metadata</code> <code>Dict[str, Any]</code> <p>Unstructured metadata to associate with the model.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>The newly created model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>A model by the provided name already exists.</p> Source code in <code>kolena/fr/model.py</code> <pre><code>@classmethod\ndef create(cls, name: str, metadata: Dict[str, Any]) -&gt; \"Model\":\n\"\"\"\n    Create a new model with the provided name and metadata.\n    :param name: Unique name of the new model to create.\n    :param metadata: Unstructured metadata to associate with the model.\n    :return: The newly created model.\n    :raises ValueError: A model by the provided name already exists.\n    \"\"\"\nrequest = API.CreateRequest(name=name, metadata=metadata)\nres = krequests.post(endpoint_path=API.Path.CREATE.value, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\nobj = Model.__factory__(from_dict(data_class=Model.Data, data=res.json()))\nlog.info(f\"created model '{name}' ({get_model_url(obj.data.id)})\")\nreturn obj\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.model.Model.load_by_name","title":"<code>load_by_name(name)</code>  <code>classmethod</code>","text":"<p>Retrieve the existing model with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model to retrieve.</p> required <p>Returns:</p> Type Description <code>Model</code> <p>The retrieved model.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no model with the provided name exists.</p> Source code in <code>kolena/fr/model.py</code> <pre><code>@classmethod\ndef load_by_name(cls, name: str) -&gt; \"Model\":\n\"\"\"\n    Retrieve the existing model with the provided name.\n    :param name: Name of the model to retrieve.\n    :return: The retrieved model.\n    :raises KeyError: If no model with the provided name exists.\n    \"\"\"\nrequest = API.LoadByNameRequest(name=name)\nres = krequests.put(endpoint_path=API.Path.LOAD_BY_NAME.value, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\nobj = Model.__factory__(from_dict(data_class=Model.Data, data=res.json()))\nlog.info(f\"loaded model '{name}' ({get_model_url(obj.data.id)})\")\nreturn obj\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.model.Model.load_pair_results","title":"<code>load_pair_results(test_object)</code>","text":"<p>Load previously stored pair results for this model on the provided test case or test suite. If this model has not been run on the provided test object, a zero-length response is returned. Partial results are returned when testing on the requested test case or test suite is incomplete.</p> <p>The returned DataFrame has the following relevant fields:</p> <ul> <li><code>locator_a</code>: the locator pointing to the left image in the pair</li> <li><code>locator_b</code>: the locator pointing to the right image in the pair</li> <li><code>is_same</code>: boolean indicating if the two images depict the same person or a different person</li> <li><code>image_a_fte</code>: boolean indicating that the left image failed to enroll (FTE)</li> <li><code>image_b_fte</code>: boolean indicating that the right image failed to enroll (FTE)</li> <li><code>similarity</code>: float similarity score between the left and right images. <code>NaN</code> if either image failed to   enroll. When multiple similarity scores were provided for a given image pair, only the highest similarity   score is returned</li> </ul> <p>Parameters:</p> Name Type Description Default <code>test_object</code> <code>Union[TestSuite, TestSuite.Data, TestCase, TestCase.Data]</code> <p>The <code>TestSuite</code> or <code>TestCase</code> to load pair results from.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>An invalid test object was provided.</p> <code>RemoteError</code> <p>The pair results could not be loaded for any reason.</p> Source code in <code>kolena/fr/model.py</code> <pre><code>def load_pair_results(\nself,\ntest_object: Union[TestSuite, TestSuite.Data, TestCase, TestCase.Data],\n) -&gt; LoadedPairResultDataFrame:\n\"\"\"\n    Load previously stored pair results for this model on the provided test case or test suite. If this model has\n    not been run on the provided test object, a zero-length response is returned. Partial results are returned when\n    testing on the requested test case or test suite is incomplete.\n    The returned DataFrame has the following relevant fields:\n    - `locator_a`: the locator pointing to the left image in the pair\n    - `locator_b`: the locator pointing to the right image in the pair\n    - `is_same`: boolean indicating if the two images depict the same person or a different person\n    - `image_a_fte`: boolean indicating that the left image failed to enroll (FTE)\n    - `image_b_fte`: boolean indicating that the right image failed to enroll (FTE)\n    - `similarity`: float similarity score between the left and right images. `NaN` if either image failed to\n      enroll. When multiple similarity scores were provided for a given image pair, only the highest similarity\n      score is returned\n    :param test_object: The [`TestSuite`][kolena.fr.TestSuite] or [`TestCase`][kolena.fr.TestCase] to load pair\n        results from.\n    :raises ValueError: An invalid test object was provided.\n    :raises RemoteError: The pair results could not be loaded for any reason.\n    \"\"\"\nreturn _BatchedLoader.concat(self.iter_pair_results(test_object), LoadedPairResultDataFrame)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.model.Model.iter_pair_results","title":"<code>iter_pair_results(test_object, batch_size=10000000)</code>","text":"<p>Iterator over DataFrames of previously stored pair results for this model on the provided test case or test suite, grouped in batches. If this model has not been run on the provided test object, a zero-length response is returned. Partial results are returned when testing on the requested test case or test suite is incomplete.</p> <p>See <code>Model.load_pair_results</code> for details on the returned DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>test_object</code> <code>Union[TestSuite, TestSuite.Data, TestCase, TestCase.Data]</code> <p>The <code>TestSuite</code> or <code>TestCase</code> to load pair results from.</p> required <code>batch_size</code> <code>int</code> <p>Optionally specify maximum number of rows to be returned in a single DataFrame.</p> <code>10000000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>An invalid test object was provided.</p> <code>RemoteError</code> <p>The pair results could not be loaded for any reason.</p> Source code in <code>kolena/fr/model.py</code> <pre><code>def iter_pair_results(\nself,\ntest_object: Union[TestSuite, TestSuite.Data, TestCase, TestCase.Data],\nbatch_size: int = 10_000_000,\n) -&gt; Iterator[LoadedPairResultDataFrame]:\n\"\"\"\n    Iterator over DataFrames of previously stored pair results for this model on the provided test case or test\n    suite, grouped in batches. If this model has not been run on the provided test object, a zero-length response\n    is returned. Partial results are returned when testing on the requested test case or test suite is incomplete.\n    See [`Model.load_pair_results`][kolena.fr.Model.load_pair_results] for details on the returned DataFrame.\n    :param test_object: The [`TestSuite`][kolena.fr.TestSuite] or [`TestCase`][kolena.fr.TestCase] to load pair\n        results from.\n    :param batch_size: Optionally specify maximum number of rows to be returned in a single DataFrame.\n    :raises ValueError: An invalid test object was provided.\n    :raises RemoteError: The pair results could not be loaded for any reason.\n    \"\"\"\ndisplay_type = \"test case\" if isinstance(test_object, (TestCase, TestCase.Data)) else \"test suite\"\ndisplay_name = test_object.data.name if isinstance(test_object, (TestCase, TestSuite)) else test_object.name\ntest_object_display_name = f\"{display_type} '{display_name}'\"\nlog.info(f\"loading pair results from model '{self.data.name}' on {test_object_display_name}\")\nbase_load_request = dataclasses.asdict(self._get_load_pair_results_request(test_object))\ninit_request = API.InitLoadPairResultsRequest(batch_size=batch_size, **base_load_request)\nyield from _BatchedLoader.iter_data(\ninit_request=init_request,\nendpoint_path=API.Path.INIT_LOAD_PAIR_RESULTS.value,\ndf_class=LoadedPairResultDataFrame,\n)\nlog.info(f\"loaded pair results from model '{self.data.name}' on {test_object_display_name}\")\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.model.InferenceModel","title":"<code>InferenceModel</code>","text":"<p>         Bases: <code>Model</code></p> <p>A <code>Model</code> capable of running tests via <code>test</code>.</p> <p>Currently supports extracting a single embedding per image. To extract multiple embeddings per image, see <code>TestRun</code>.</p>"},{"location":"reference/built-in/fr/#kolena.fr.model.InferenceModel.create","title":"<code>create(name, extract, compare, metadata)</code>  <code>classmethod</code>","text":"<p>Create a new model with the provided name and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Unique name of the new model to create.</p> required <code>extract</code> <code>Callable[[str], Optional[np.ndarray]]</code> <p>A function implementing embeddings extraction for this model.</p> required <code>compare</code> <code>Callable[[np.ndarray, np.ndarray], float]</code> <p>A function implementing embeddings similarity comparison for this model.</p> required <code>metadata</code> <code>Dict[str, Any]</code> <p>Unstructured metadata to associate with the model.</p> required <p>Returns:</p> Type Description <code>InferenceModel</code> <p>The newly created model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>A model by the provided name already exists.</p> Source code in <code>kolena/fr/model.py</code> <pre><code>@classmethod\ndef create(\ncls,\nname: str,\nextract: Callable[[str], Optional[np.ndarray]],\ncompare: Callable[[np.ndarray, np.ndarray], float],\nmetadata: Dict[str, Any],\n) -&gt; \"InferenceModel\":\n\"\"\"\n    Create a new model with the provided name and metadata.\n    :param name: Unique name of the new model to create.\n    :param extract: A function implementing embeddings extraction for this model.\n    :param compare: A function implementing embeddings similarity comparison for this model.\n    :param metadata: Unstructured metadata to associate with the model.\n    :return: The newly created model.\n    :raises ValueError: A model by the provided name already exists.\n    \"\"\"\nbase_model = super().create(name, metadata)\nreturn cls._from_base(base_model, extract, compare)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.model.InferenceModel.load_by_name","title":"<code>load_by_name(name, extract, compare)</code>  <code>classmethod</code>","text":"<p>Load an existing model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the model to load.</p> required <code>extract</code> <code>Callable[[str], Optional[np.ndarray]]</code> <p>A function implementing embeddings extraction for this model.</p> required <code>compare</code> <code>Callable[[np.ndarray, np.ndarray], float]</code> <p>A function implementing embeddings similarity comparison for this model.</p> required <p>Returns:</p> Type Description <code>InferenceModel</code> <p>The loaded model.</p> Source code in <code>kolena/fr/model.py</code> <pre><code>@classmethod\ndef load_by_name(\ncls,\nname: str,\nextract: Callable[[str], Optional[np.ndarray]],\ncompare: Callable[[np.ndarray, np.ndarray], float],\n) -&gt; \"InferenceModel\":\n\"\"\"\n    Load an existing model.\n    :param name: The name of the model to load.\n    :param extract: A function implementing embeddings extraction for this model.\n    :param compare: A function implementing embeddings similarity comparison for this model.\n    :return: The loaded model.\n    \"\"\"\nbase_model = super().load_by_name(name)\nreturn cls._from_base(base_model, extract, compare)\n</code></pre>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.TestImageDataFrameSchema","title":"<code>TestImageDataFrameSchema</code>","text":"<p>         Bases: <code>pa.SchemaModel</code></p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.TestImageDataFrameSchema.image_id","title":"<code>image_id: Series[pa.typing.Int64] = pa.Field(coerce=True)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Internal ID corresponding to this image.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.TestImageDataFrameSchema.locator","title":"<code>locator: Series[pa.typing.String] = pa.Field(coerce=True, _validate_locator=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>External locator pointing to image in bucket.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.TestImageDataFrameSchema.data_source","title":"<code>data_source: Series[pa.typing.String] = pa.Field(coerce=True, nullable=True)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Specify source dataset, e.g. \"CIFAR-10\".</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.TestImageDataFrameSchema.width","title":"<code>width: Series[pa.typing.Int64] = pa.Field(coerce=True, _validate_optional_dimension=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Width of the image, in pixels. The value <code>-1</code> is used to specify \"unspecified\".</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.TestImageDataFrameSchema.height","title":"<code>height: Series[pa.typing.Int64] = pa.Field(coerce=True, _validate_optional_dimension=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Height of the image, in pixels. The value <code>-1</code> is used to specify \"unspecified\".</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.TestImageDataFrameSchema.original_locator","title":"<code>original_locator: Series[pa.typing.String] = pa.Field(coerce=True, nullable=True, _validate_locator=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Specify that this image is an augmented version of another (registered) image.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.TestImageDataFrameSchema.augmentation_spec","title":"<code>augmentation_spec: Series[JSONObject] = pa.Field(coerce=True, nullable=True, _validate_json_object=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Free-form specification describing the augmentation applied to the image, e.g. <code>{\"rotate\": 90}</code>. Should not be specified unless original_locator is present.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.TestImageDataFrameSchema.bounding_box","title":"<code>bounding_box: Series[BoundingBox] = pa.Field(coerce=True, nullable=True, _validate_bounding_box=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Ground truth bounding box. If absent, no ground truth will be available for display.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.TestImageDataFrameSchema.landmarks","title":"<code>landmarks: Series[Landmarks] = pa.Field(coerce=True, nullable=True, _validate_landmarks=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Ground truth landmarks. If absent, no ground truth will be available for display.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.TestImageDataFrameSchema.tags","title":"<code>tags: Series[JSONObject] = pa.Field(coerce=True, _validate_tags=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Specify a set of tags to apply to this object in the form <code>{category: value}</code>. Note that this format intentionally restricts tags to a single value per category.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.ImageResultDataFrameSchema","title":"<code>ImageResultDataFrameSchema</code>","text":"<p>         Bases: <code>pa.SchemaModel</code></p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.ImageResultDataFrameSchema.image_id","title":"<code>image_id: Series[pa.typing.Int64] = pa.Field(coerce=True)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Required] The ID of the image corresponding to this record.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.ImageResultDataFrameSchema.bounding_box","title":"<code>bounding_box: Optional[Series[BoundingBox]] = pa.Field(coerce=True, nullable=True, _validate_bounding_box=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Optional] A bounding box around the face detected in this image.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.ImageResultDataFrameSchema.landmarks_input_image","title":"<code>landmarks_input_image: Optional[Series[RGBImage]] = pa.Field(coerce=True, nullable=True, _validate_rgb_image=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Optional] RGB image (<code>np.ndarray</code> with cells of type <code>np.uint8</code>) corresponding to the input to the \"landmarks\" model in the face recognition pipeline.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.ImageResultDataFrameSchema.landmarks","title":"<code>landmarks: Optional[Series[Landmarks]] = pa.Field(coerce=True, nullable=True, _validate_landmarks=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Optional] A 10-element array with <code>(x, y)</code> coordinates corresponding to the left eye, right eye, nose tip, left mouth corner, right mouth corner of the detected face.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.ImageResultDataFrameSchema.quality_input_image","title":"<code>quality_input_image: Optional[Series[RGBImage]] = pa.Field(coerce=True, nullable=True, _validate_rgb_image=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Optional] RGB image (<code>np.ndarray</code> with cells of type <code>np.uint8</code>) corresponding to the input to the \"quality\" model in the face recognition pipeline.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.ImageResultDataFrameSchema.quality","title":"<code>quality: Optional[Series[pa.typing.Float64]] = pa.Field(coerce=True, nullable=True)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Optional] Score produced by the \"quality\" model in the face recognition pipeline.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.ImageResultDataFrameSchema.acceptability","title":"<code>acceptability: Optional[Series[pa.typing.Float64]] = pa.Field(coerce=True, nullable=True)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Optional] Score produced by the \"acceptability\" model in the face recognition pipeline.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.ImageResultDataFrameSchema.fr_input_image","title":"<code>fr_input_image: Optional[Series[RGBImage]] = pa.Field(coerce=True, nullable=True, _validate_rgb_image=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Optional] RGB image (<code>np.ndarray</code> with cells of type <code>np.uint8</code>) corresponding to the input to the facial embeddings extraction model in the face recognition pipeline.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.ImageResultDataFrameSchema.embedding","title":"<code>embedding: Series[EmbeddingVector] = pa.Field(coerce=True, nullable=True, _validate_embedding_vector=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Optional] Embedding vector (<code>np.ndarray</code>) extracted representing the face detected in the image. An empty cell (no array is provided) indicates a failure to enroll for the image, i.e. no face detected.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.ImageResultDataFrameSchema.failure_reason","title":"<code>failure_reason: Optional[Series[pa.typing.String]] = pa.Field(coerce=True, nullable=True)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Optional] The reason why the image was a failure to enroll.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.EmbeddingDataFrameSchema","title":"<code>EmbeddingDataFrameSchema</code>","text":"<p>         Bases: <code>pa.SchemaModel</code></p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.EmbeddingDataFrameSchema.image_id","title":"<code>image_id: Series[pa.typing.Int64] = pa.Field(coerce=True)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The provided ID of the image.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.EmbeddingDataFrameSchema.embedding","title":"<code>embedding: Series[EmbeddingVector] = pa.Field(nullable=True, coerce=True, _validate_embedding_vector=())</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The extracted embedding(s) corresponding to the <code>image_id</code>. A missing embedding indicates a failure to enroll for the image.</p> <p>For images with only one extracted embedding, the <code>embedding</code> is a one-dimensional <code>np.ndarray</code> with length matching the length of the extracted embedding. When multiple embeddings were extracted from a single image, the first dimension represents the index of the extracted embedding. For example, for an image with 3 extracted embeddings and embeddings of length 256, the <code>embedding</code> is an array of shape (3, 256).</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.PairResultDataFrameSchema","title":"<code>PairResultDataFrameSchema</code>","text":"<p>         Bases: <code>pa.SchemaModel</code></p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.PairResultDataFrameSchema.image_pair_id","title":"<code>image_pair_id: Series[pa.typing.Int64] = pa.Field(coerce=True)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Required] The ID of the image corresponding to this record.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.PairResultDataFrameSchema.similarity","title":"<code>similarity: Series[pa.typing.Float64] = pa.Field(nullable=True, coerce=True)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Optional] The similarity score computed between the two embeddings in this image pair. Should be left empty when either image in the pair is a failure to enroll.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.PairResultDataFrameSchema.embedding_a_index","title":"<code>embedding_a_index: Optional[Series[pa.typing.Int64]] = pa.Field(nullable=True, coerce=True)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Optional] Index of the embedding in <code>image_a</code> corresponding to this similarity score. Required when multiple embeddings are extracted per image and multiple similarity scores are computed per image pair.</p>"},{"location":"reference/built-in/fr/#kolena.fr.datatypes.PairResultDataFrameSchema.embedding_b_index","title":"<code>embedding_b_index: Optional[Series[pa.typing.Int64]] = pa.Field(nullable=True, coerce=True)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>[Optional] Index of the embedding in <code>image_b</code> corresponding to this similarity score. Required when multiple embeddings are extracted per image and multiple similarity scores are computed per image pair.</p>"},{"location":"reference/workflow/annotation/","title":"Annotations","text":"<p>Annotations are visualized in the Kolena web platform as overlays on top of <code>TestSample</code> objects.</p> <p>For example, when viewing images in the Studio, any annotations (such as lists of <code>BoundingBox</code> objects) present in the <code>TestSample</code>, <code>GroundTruth</code>, <code>Inference</code>, or <code>MetricsTestSample</code> objects are rendered on top of the image.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Annotation","title":"<code>Annotation</code>","text":"<p>         Bases: <code>TypedDataObject[_AnnotationType]</code></p> <p>The base class for all annotation types.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox","title":"<code>BoundingBox</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox.top_left","title":"<code>top_left: Tuple[float, float]</code>  <code>instance-attribute</code>","text":"<p>The top left vertex (in <code>(x, y)</code> image coordinates) of this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox.bottom_right","title":"<code>bottom_right: Tuple[float, float]</code>  <code>instance-attribute</code>","text":"<p>The bottom right vertex (in <code>(x, y)</code> image coordinates) of this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox","title":"<code>LabeledBoundingBox</code>","text":"<p>         Bases: <code>BoundingBox</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices and a string label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox","title":"<code>ScoredBoundingBox</code>","text":"<p>         Bases: <code>BoundingBox</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox","title":"<code>ScoredLabeledBoundingBox</code>","text":"<p>         Bases: <code>BoundingBox</code></p> <p>Rectangular bounding box specified with pixel coordinates of the top left and bottom right vertices, a string label, and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polygon","title":"<code>Polygon</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polygon.points","title":"<code>points: List[Tuple[float, float]]</code>  <code>instance-attribute</code>","text":"<p>The sequence of <code>(x, y)</code> points comprising the boundary of this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledPolygon","title":"<code>LabeledPolygon</code>","text":"<p>         Bases: <code>Polygon</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates and a string label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledPolygon.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredPolygon","title":"<code>ScoredPolygon</code>","text":"<p>         Bases: <code>Polygon</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredPolygon.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledPolygon","title":"<code>ScoredLabeledPolygon</code>","text":"<p>         Bases: <code>Polygon</code></p> <p>Arbitrary polygon specified by three or more pixel coordinates with a string label and a float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledPolygon.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label (e.g. model classification) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledPolygon.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score (e.g. model confidence) associated with this polygon.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Keypoints","title":"<code>Keypoints</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Array of any number of keypoints specified in pixel coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Keypoints.points","title":"<code>points: List[Tuple[float, float]]</code>  <code>instance-attribute</code>","text":"<p>The sequence of discrete <code>(x, y)</code> points comprising this keypoints annotation.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polyline","title":"<code>Polyline</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Polyline with any number of vertices specified in pixel coordinates.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.Polyline.points","title":"<code>points: List[Tuple[float, float]]</code>  <code>instance-attribute</code>","text":"<p>The sequence of connected <code>(x, y)</code> points comprising this polyline.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D","title":"<code>BoundingBox3D</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Three-dimensional cuboid bounding box in a right-handed coordinate system.</p> <p>Specified by <code>(x, y, z)</code> coordinates for the <code>center</code> of the cuboid, <code>(x, y, z)</code> <code>dimensions</code>, and a <code>rotation</code> parameter specifying the degrees of rotation about each axis <code>(x, y, z)</code> ranging <code>[-\u03c0, \u03c0]</code>.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D.center","title":"<code>center: Tuple[float, float, float]</code>  <code>instance-attribute</code>","text":"<p><code>(x, y, z)</code> coordinates specifying the center of the bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D.dimensions","title":"<code>dimensions: Tuple[float, float, float]</code>  <code>instance-attribute</code>","text":"<p><code>(x, y, z)</code> measurements specifying the dimensions of the bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BoundingBox3D.rotations","title":"<code>rotations: Tuple[float, float, float]</code>  <code>instance-attribute</code>","text":"<p>Rotations in degrees about each <code>(x, y, z)</code> axis.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox3D","title":"<code>LabeledBoundingBox3D</code>","text":"<p>         Bases: <code>BoundingBox3D</code></p> <p><code>BoundingBox3D</code> with an additional string label.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.LabeledBoundingBox3D.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox3D","title":"<code>ScoredBoundingBox3D</code>","text":"<p>         Bases: <code>BoundingBox3D</code></p> <p><code>BoundingBox3D</code> with an additional float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredBoundingBox3D.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox3D","title":"<code>ScoredLabeledBoundingBox3D</code>","text":"<p>         Bases: <code>BoundingBox3D</code></p> <p><code>BoundingBox3D</code> with an additional string label and float score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox3D.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>The label associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredLabeledBoundingBox3D.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>The score associated with this 3D bounding box.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.SegmentationMask","title":"<code>SegmentationMask</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Raster segmentation mask. The <code>locator</code> is the URL to the image file representing the segmentation mask.</p> <p>The segmentation mask must be rendered as a single-channel, 8-bit-depth (grayscale) image. For the best results, use a lossless file format such as PNG. Each pixel's value is the numerical ID of its class label, as specified in the <code>labels</code> map. Any pixel value not present in the <code>labels</code> map is rendered as part of the background.</p> <p>For example, <code>labels = {255: \"object\"}</code> will highlight all pixels with the value of 255 as <code>\"object\"</code>. Every other pixel value will be transparent.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.SegmentationMask.labels","title":"<code>labels: Dict[int, str]</code>  <code>instance-attribute</code>","text":"<p>Mapping of unique label IDs (pixel values) to unique label values.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.SegmentationMask.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL of the segmentation mask image.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BitmapMask","title":"<code>BitmapMask</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Arbitrary bitmap mask. The <code>locator</code> is the URL to the image file representing the mask.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.BitmapMask.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL of the bitmap data.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ClassificationLabel","title":"<code>ClassificationLabel</code>","text":"<p>         Bases: <code>Annotation</code></p> <p>Label of classification.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ClassificationLabel.label","title":"<code>label: str</code>  <code>instance-attribute</code>","text":"<p>String label for this classification.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredClassificationLabel","title":"<code>ScoredClassificationLabel</code>","text":"<p>         Bases: <code>ClassificationLabel</code></p> <p>Classification label with accompanying score.</p>"},{"location":"reference/workflow/annotation/#kolena.workflow.annotation.ScoredClassificationLabel.score","title":"<code>score: float</code>  <code>instance-attribute</code>","text":"<p>Score associated with this label.</p>"},{"location":"reference/workflow/asset/","title":"Assets: <code>kolena.workflow.asset</code>","text":"<p>Assets are additional files linked to the <code>TestSample</code>, <code>GroundTruth</code>, or <code>Inference</code> objects for your workflow. Assets can be visualized in the Kolena Studio when exploring your test cases or model results.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.Asset","title":"<code>Asset</code>","text":"<p>         Bases: <code>TypedDataObject[_AssetType]</code></p> <p>Base class for all asset types.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.ImageAsset","title":"<code>ImageAsset</code>","text":"<p>         Bases: <code>Asset</code></p> <p>An image in a cloud bucket.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.ImageAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this image in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-image-asset.png</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PlainTextAsset","title":"<code>PlainTextAsset</code>","text":"<p>         Bases: <code>Asset</code></p> <p>A plain text file in a cloud bucket.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PlainTextAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this text file in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-text-asset.txt</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BinaryAsset","title":"<code>BinaryAsset</code>","text":"<p>         Bases: <code>Asset</code></p> <p>A binary file in a cloud bucket.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PointCloudAsset","title":"<code>PointCloudAsset</code>","text":"<p>         Bases: <code>Asset</code></p> <p>A three-dimensional point cloud located in a cloud bucket. Points are assumed to be specified in a right-handed, Z-up coordinate system with the origin around the sensor that captured the point cloud.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.PointCloudAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>The location of this point cloud in a cloud bucket, e.g. <code>s3://my-bucket/path/to/my-point-cloud.pcd</code>.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BaseVideoAsset","title":"<code>BaseVideoAsset</code>","text":"<p>         Bases: <code>Asset</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.BaseVideoAsset.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset","title":"<code>VideoAsset</code>","text":"<p>         Bases: <code>BaseVideoAsset</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.thumbnail","title":"<code>thumbnail: Optional[ImageAsset] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally provide asset locator for custom video thumbnail image.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.start","title":"<code>start: Optional[float] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally specify start time of video snippet, in seconds.</p>"},{"location":"reference/workflow/asset/#kolena.workflow.asset.VideoAsset.end","title":"<code>end: Optional[float] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally specify end time of video snippet, in seconds.</p>"},{"location":"reference/workflow/define_workflow/","title":"<code>kolena.workflow.define_workflow</code>","text":""},{"location":"reference/workflow/define_workflow/#kolena.workflow.define_workflow.define_workflow","title":"<code>define_workflow(name, test_sample_type, ground_truth_type, inference_type)</code>","text":"<p>Define a new workflow, specifying its test sample, ground truth, and inference types.</p> <pre><code>_, TestCase, TestSuite, Model = define_workflow(\n\"My Workflow\",\nMyTestSample,   # extends e.g. kolena.workflow.Image (or uses directly)\nMyGroundTruth,  # extends kolena.workflow.GroundTruth\nMyInference,    # extends kolena.workflow.Inference\n)\n</code></pre> <p><code>define_workflow</code> is provided as a convenience method to create the <code>TestCase</code>, <code>TestSuite</code>, and <code>Model</code> objects for a new workflow. These objects can also be defined manually by subclassing them and binding the <code>workflow</code> class variable:</p> <pre><code>from kolena.workflow import TestCase\nfrom my_code import my_workflow\nclass MyTestCase(TestCase):\nworkflow = my_workflow\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the workflow.</p> required <code>test_sample_type</code> <code>Type[TestSample]</code> <p>The type of the <code>TestSample</code> for this workflow.</p> required <code>ground_truth_type</code> <code>Type[GroundTruth]</code> <p>The type of the <code>GroundTruth</code> for this workflow.</p> required <code>inference_type</code> <code>Type[Inference]</code> <p>The type of the <code>Inference</code> for this workflow.</p> required <p>Returns:</p> Type Description <code>Tuple[Workflow, Type[TestCase], Type[TestSuite], Type[Model]]</code> <p>The <code>Workflow</code> object for this workflow along with the <code>TestCase</code>, <code>TestSuite</code>, and <code>Model</code> objects to use when creating and running tests for this workflow.</p> Source code in <code>kolena/workflow/define_workflow.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef define_workflow(\nname: str,\ntest_sample_type: Type[TestSample],\nground_truth_type: Type[GroundTruth],\ninference_type: Type[Inference],\n) -&gt; Tuple[Workflow, Type[TestCase], Type[TestSuite], Type[Model]]:\n\"\"\"\n    Define a new workflow, specifying its test sample, ground truth, and inference types.\n    ```python\n    _, TestCase, TestSuite, Model = define_workflow(\n        \"My Workflow\",\n        MyTestSample,   # extends e.g. kolena.workflow.Image (or uses directly)\n        MyGroundTruth,  # extends kolena.workflow.GroundTruth\n        MyInference,    # extends kolena.workflow.Inference\n    )\n    ```\n    `define_workflow` is provided as a convenience method to create the [`TestCase`][kolena.workflow.TestCase],\n    [`TestSuite`][kolena.workflow.TestSuite], and [`Model`][kolena.workflow.Model] objects\n    for a new workflow. These objects can also be defined manually by subclassing them and binding the `workflow`\n    class variable:\n    ```python\n    from kolena.workflow import TestCase\n    from my_code import my_workflow\n    class MyTestCase(TestCase):\n        workflow = my_workflow\n    ```\n    :param name: The name of the workflow.\n    :param test_sample_type: The type of the [`TestSample`][kolena.workflow.TestSample] for this workflow.\n    :param ground_truth_type: The type of the [`GroundTruth`][kolena.workflow.GroundTruth] for this workflow.\n    :param inference_type: The type of the [`Inference`][kolena.workflow.Inference] for this workflow.\n    :return: The `Workflow` object for this workflow along with the [`TestCase`][kolena.workflow.TestCase],\n        [`TestSuite`][kolena.workflow.TestSuite], and [`Model`][kolena.workflow.Model] objects to use when creating and\n        running tests for this workflow.\n    \"\"\"\nworkflow = Workflow(\nname=name,\ntest_sample_type=test_sample_type,\nground_truth_type=ground_truth_type,\ninference_type=inference_type,\n)\ntest_case = type(\"TestCase\", (TestCase,), {\"workflow\": workflow})\ntest_suite = type(\"TestSuite\", (TestSuite,), {\"workflow\": workflow, \"_test_case_type\": test_case})\nmodel = type(\"Model\", (Model,), {\"workflow\": workflow})\nreturn workflow, cast(Type[TestCase], test_case), cast(Type[TestSuite], test_suite), cast(Type[Model], model)\n</code></pre>"},{"location":"reference/workflow/evaluator/","title":"<code>kolena.workflow.Evaluator</code>","text":"<p>Simplified interface for <code>Evaluator</code> implementations.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.NumberSeries","title":"<code>NumberSeries = Sequence[Union[float, int]]</code>  <code>module-attribute</code>","text":"<p>A sequence of numeric values.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.NullableNumberSeries","title":"<code>NullableNumberSeries = Sequence[Union[float, int, None]]</code>  <code>module-attribute</code>","text":"<p>A sequence of numeric values or <code>None</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestSample","title":"<code>MetricsTestSample</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Test-sample-level metrics produced by an <code>Evaluator</code>.</p> <p>This class should be subclassed with the relevant fields for a given workflow.</p> <p>Examples here may include the number of true positive detections on an image, the mean IOU of inferred polygon(s) with ground truth polygon(s), etc.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestCase","title":"<code>MetricsTestCase</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Test-case-level metrics produced by an <code>Evaluator</code>.</p> <p>This class should be subclassed with the relevant fields for a given workflow.</p> <p>Test-case-level metrics are aggregate metrics like Precision, Recall, and F1 score. Any and all aggregate metrics that fit a workflow should be defined here.</p> <p><code>MetricsTestCase</code> supports nesting metrics objects, for e.g. reporting class-level metrics within a test case that contains multiple classes. Example usage:</p> <pre><code>@dataclass(frozen=True)\nclass PerClassMetrics(MetricsTestCase):\nClass: str\nPrecision: float\nRecall: float\nF1: float\nAP: float\n@dataclass(frozen=True)\nclass TestCaseMetrics(MetricsTestCase):\nmacro_Precision: float\nmacro_Recall: float\nmacro_F1: float\nmAP: float\nPerClass: List[PerClassMetrics]\n</code></pre>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.MetricsTestSuite","title":"<code>MetricsTestSuite</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Test-suite-level metrics produced by an <code>Evaluator</code>.</p> <p>This class should be subclassed with the relevant fields for a given workflow.</p> <p>Test-suite-level metrics typically measure performance across test cases, e.g. penalizing variance across different subsets of a benchmark.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.AxisConfig","title":"<code>AxisConfig</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Configuration for the format of a given axis on a plot.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.AxisConfig.type","title":"<code>type: Literal[linear, log]</code>  <code>instance-attribute</code>","text":"<p>Type of axis to display. Supported options are <code>linear</code> and <code>log</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Plot","title":"<code>Plot</code>","text":"<p>         Bases: <code>TypedDataObject[_PlotType]</code></p> <p>A data visualization shown when exploring model results in the web platform.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Curve","title":"<code>Curve</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>A single series on a <code>CurvePlot</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Curve.x","title":"<code>x: NumberSeries</code>  <code>instance-attribute</code>","text":"<p>The <code>x</code> coordinates of this curve. Length must match the provided <code>y</code> coordinates.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Curve.y","title":"<code>y: NumberSeries</code>  <code>instance-attribute</code>","text":"<p>The <code>y</code> coordinates of this curve. Length must match the provided <code>x</code> coordinates.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Curve.label","title":"<code>label: Optional[str] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally specify an additional label (in addition to the associated test case) to apply to this curve, for use when e.g. there are multiple curves generated per test case.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.CurvePlot","title":"<code>CurvePlot</code>","text":"<p>         Bases: <code>Plot</code></p> <p>A plot visualizing one or more curves per test case.</p> <p>Examples include Receiver Operating Characteristic (ROC) curves, Precision versus Recall (PR) curves, Detection-Error Tradeoff (DET) curves, etc.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.CurvePlot.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The title for the plot.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.CurvePlot.x_label","title":"<code>x_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>x</code> axis.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.CurvePlot.y_label","title":"<code>y_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>y</code> axis.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.CurvePlot.curves","title":"<code>curves: List[Curve]</code>  <code>instance-attribute</code>","text":"<p>A test case may generate zero or more curves on a given plot. However, under most circumstances, a single curve per test case is desirable.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.CurvePlot.x_config","title":"<code>x_config: Optional[AxisConfig] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>x</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.CurvePlot.y_config","title":"<code>y_config: Optional[AxisConfig] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>y</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Histogram","title":"<code>Histogram</code>","text":"<p>         Bases: <code>Plot</code></p> <p>A plot visualizing distribution of one or more continuous values, e.g. distribution of an error metric across all samples within a test case.</p> <p>For visualization of discrete values, see <code>BarPlot</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Histogram.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The title for the plot.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Histogram.x_label","title":"<code>x_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>x</code> axis.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Histogram.y_label","title":"<code>y_label: str</code>  <code>instance-attribute</code>","text":"<p>The label describing the plot's <code>y</code> axis.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Histogram.buckets","title":"<code>buckets: NumberSeries</code>  <code>instance-attribute</code>","text":"<p>A Histogram requires intervals to bucket the data. For <code>n</code> buckets, <code>n+1</code> consecutive bounds must be specified in increasing order.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Histogram.frequency","title":"<code>frequency: Union[NumberSeries, Sequence[NumberSeries]]</code>  <code>instance-attribute</code>","text":"<p>For <code>n</code> buckets, there are <code>n</code> frequencies corresponding to the height of each bucket. The frequency at index <code>i</code> corresponds to the bucket with bounds (<code>i</code>, <code>i+1</code>) in <code>buckets</code>.</p> <p>To specify multiple distributions for a given test case, multiple frequency series can be provided, corresponding e.g. to the distribution for a given class within a test case, with name specified in <code>labels</code>.</p> <p>Specify a list of labels corresponding to the different <code>frequency</code> series when multiple series are provided. Can be omitted when a single <code>frequency</code> series is provided.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Histogram.labels","title":"<code>labels: Optional[List[str]] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Specify the label corresponding to a given distribution when multiple are specified in <code>frequency</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Histogram.x_config","title":"<code>x_config: Optional[AxisConfig] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>x</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Histogram.y_config","title":"<code>y_config: Optional[AxisConfig] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Custom options to allow for control over the display of the plot <code>y</code> axis. See <code>AxisConfig</code> for details.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.BarPlot","title":"<code>BarPlot</code>","text":"<p>         Bases: <code>Plot</code></p> <p>A plot visualizing a set of bars per test case.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.BarPlot.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The plot title.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.BarPlot.x_label","title":"<code>x_label: str</code>  <code>instance-attribute</code>","text":"<p>Axis label for the axis along which the bars are laid out (<code>labels</code>).</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.BarPlot.y_label","title":"<code>y_label: str</code>  <code>instance-attribute</code>","text":"<p>Axis label for the axis corresponding to bar height (<code>values</code>).</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.BarPlot.labels","title":"<code>labels: Sequence[Union[str, int, float]]</code>  <code>instance-attribute</code>","text":"<p>Labels for each bar with corresponding height specified in <code>values</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.BarPlot.values","title":"<code>values: NullableNumberSeries</code>  <code>instance-attribute</code>","text":"<p>Values for each bar with corresponding label specified in <code>labels</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.BarPlot.config","title":"<code>config: Optional[AxisConfig] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Custom format options to allow for control over the display of the numerical plot axis (<code>values</code>).</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.ConfusionMatrix","title":"<code>ConfusionMatrix</code>","text":"<p>         Bases: <code>Plot</code></p> <p>A confusion matrix. Example:</p> <pre><code>ConfusionMatrix(\ntitle=\"Cat and Dog Confusion\",\nlabels=[\"Cat\", \"Dog\"],\nmatrix=[[90, 10], [5, 95]],\n)\n</code></pre> <p>Yields a confusion matrix of the form:</p> <pre><code>            Predicted\n\n            Cat   Dog\n           +----+----+\n       Cat | 90 | 10 |\nActual     +----+----+\n       Dog |  5 | 95 |\n           +----+----+\n</code></pre>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.ConfusionMatrix.title","title":"<code>title: str</code>  <code>instance-attribute</code>","text":"<p>The plot title.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.ConfusionMatrix.labels","title":"<code>labels: List[str]</code>  <code>instance-attribute</code>","text":"<p>The labels corresponding to each entry in the square <code>matrix</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.ConfusionMatrix.matrix","title":"<code>matrix: Sequence[NullableNumberSeries]</code>  <code>instance-attribute</code>","text":"<p>A square matrix, typically representing the number of matches between class <code>i</code> and class <code>j</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.ConfusionMatrix.x_label","title":"<code>x_label: str = 'Predicted'</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The label for the <code>x</code> axis of the confusion matrix.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.ConfusionMatrix.y_label","title":"<code>y_label: str = 'Actual'</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>The label for the <code>y</code> axis of the confusion matrix.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.EvaluatorConfiguration","title":"<code>EvaluatorConfiguration</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>Configuration for an <code>Evaluator</code>.</p> <p>Example evaluator configurations may specify:</p> <ul> <li>Fixed confidence thresholds at which detections are discarded.</li> <li>Different algorithms/strategies used to compute confidence thresholds     (e.g. \"accuracy optimal\" for a classification-type workflow).</li> </ul>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator","title":"<code>Evaluator(configurations=None)</code>","text":"<p>An <code>Evaluator</code> transforms inferences into metrics.</p> <p>Metrics are computed at the individual test sample level (<code>MetricsTestSample</code>), in aggregate at the test case level (<code>MetricsTestCase</code>), and across populations at the test suite level (<code>MetricsTestSuite</code>).</p> <p>Test-case-level plots (<code>Plot</code>) may also be computed.</p> Source code in <code>kolena/workflow/evaluator.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, configurations: Optional[List[EvaluatorConfiguration]] = None):\nif configurations is not None and len(configurations) == 0:\nraise ValueError(\"empty configuration list provided, at least one configuration required or 'None'\")\nself.configurations = configurations or []\nif len({configuration.display_name() for configuration in self.configurations}) &lt; len(self.configurations):\nraise ValueError(\"all configurations must have distinct display names\")\n</code></pre>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_sample_metrics","title":"<code>compute_test_sample_metrics(test_case, inferences, configuration=None)</code>  <code>abstractmethod</code>","text":"<p>Compute metrics for every test sample in a test case.</p> <p>Must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>the test case to which the provided test samples and ground truths belong.</p> required <code>inferences</code> <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>the test samples, ground truths, and inferences for all entries in a test case.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>the evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[TestSample, MetricsTestSample]]</code> <p>test-sample-level metrics for each provided test sample.</p> Source code in <code>kolena/workflow/evaluator.py</code> <pre><code>@abstractmethod\ndef compute_test_sample_metrics(\nself,\ntest_case: TestCase,\ninferences: List[Tuple[TestSample, GroundTruth, Inference]],\nconfiguration: Optional[EvaluatorConfiguration] = None,\n) -&gt; List[Tuple[TestSample, MetricsTestSample]]:\n\"\"\"\n    Compute metrics for every test sample in a test case.\n    Must be implemented.\n    :param test_case: the test case to which the provided test samples and ground truths belong.\n    :param inferences: the test samples, ground truths, and inferences for all entries in a test case.\n    :param configuration: the evaluator configuration to use. Empty for implementations that are not configured.\n    :return: test-sample-level metrics for each provided test sample.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_case_metrics","title":"<code>compute_test_case_metrics(test_case, inferences, metrics, configuration=None)</code>  <code>abstractmethod</code>","text":"<p>Compute aggregate metrics across a test case.</p> <p>Must be implemented.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>the test case in question.</p> required <code>inferences</code> <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>the test samples, ground truths, and inferences for all entries in a test case.</p> required <code>metrics</code> <code>List[MetricsTestSample]</code> <p>the test-sample-level metrics computed by :meth:<code>Evaluator.compute_test_sample_metrics</code>. Provided in the same order as <code>inferences</code>.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>the evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>MetricsTestCase</code> <p>test-case-level metrics for the provided test case.</p> Source code in <code>kolena/workflow/evaluator.py</code> <pre><code>@abstractmethod\ndef compute_test_case_metrics(\nself,\ntest_case: TestCase,\ninferences: List[Tuple[TestSample, GroundTruth, Inference]],\nmetrics: List[MetricsTestSample],\nconfiguration: Optional[EvaluatorConfiguration] = None,\n) -&gt; MetricsTestCase:\n\"\"\"\n    Compute aggregate metrics across a test case.\n    Must be implemented.\n    :param test_case: the test case in question.\n    :param inferences: the test samples, ground truths, and inferences for all entries in a test case.\n    :param metrics: the test-sample-level metrics computed by :meth:`Evaluator.compute_test_sample_metrics`.\n        Provided in the same order as ``inferences``.\n    :param configuration: the evaluator configuration to use. Empty for implementations that are not configured.\n    :return: test-case-level metrics for the provided test case.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_case_plots","title":"<code>compute_test_case_plots(test_case, inferences, metrics, configuration=None)</code>","text":"<p>Optionally compute any number of plots to visualize the results for a test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>the test case in question</p> required <code>inferences</code> <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>the test samples, ground truths, and inferences for all entries in a test case.</p> required <code>metrics</code> <code>List[MetricsTestSample]</code> <p>the test-sample-level metrics computed by :meth:<code>Evaluator.compute_test_sample_metrics</code>. Provided in the same order as <code>inferences</code>.</p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>the evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[List[Plot]]</code> <p>zero or more plots for this test case at this configuration.</p> Source code in <code>kolena/workflow/evaluator.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef compute_test_case_plots(\nself,\ntest_case: TestCase,\ninferences: List[Tuple[TestSample, GroundTruth, Inference]],\nmetrics: List[MetricsTestSample],\nconfiguration: Optional[EvaluatorConfiguration] = None,\n) -&gt; Optional[List[Plot]]:\n\"\"\"\n    Optionally compute any number of plots to visualize the results for a test case.\n    :param test_case: the test case in question\n    :param inferences: the test samples, ground truths, and inferences for all entries in a test case.\n    :param metrics: the test-sample-level metrics computed by :meth:`Evaluator.compute_test_sample_metrics`.\n        Provided in the same order as ``inferences``.\n    :param configuration: the evaluator configuration to use. Empty for implementations that are not configured.\n    :return: zero or more plots for this test case at this configuration.\n    \"\"\"\nreturn None  # not required\n</code></pre>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator.Evaluator.compute_test_suite_metrics","title":"<code>compute_test_suite_metrics(test_suite, metrics, configuration=None)</code>","text":"<p>Optionally compute test-suite-level metrics.</p> <p>Parameters:</p> Name Type Description Default <code>test_suite</code> <code>TestSuite</code> <p>the test suite in question</p> required <code>metrics</code> <code>List[Tuple[TestCase, MetricsTestCase]]</code> <p>the test-case-level metrics computed by :meth:<code>Evaluator.compute_test_case_metrics</code></p> required <code>configuration</code> <code>Optional[EvaluatorConfiguration]</code> <p>the evaluator configuration to use. Empty for implementations that are not configured.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[MetricsTestSuite]</code> <p>the test-suite-level metrics for this test suite</p> Source code in <code>kolena/workflow/evaluator.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef compute_test_suite_metrics(\nself,\ntest_suite: TestSuite,\nmetrics: List[Tuple[TestCase, MetricsTestCase]],\nconfiguration: Optional[EvaluatorConfiguration] = None,\n) -&gt; Optional[MetricsTestSuite]:\n\"\"\"\n    Optionally compute test-suite-level metrics.\n    :param test_suite: the test suite in question\n    :param metrics: the test-case-level metrics computed by :meth:`Evaluator.compute_test_case_metrics`\n    :param configuration: the evaluator configuration to use. Empty for implementations that are not configured.\n    :return: the test-suite-level metrics for this test suite\n    \"\"\"\nreturn None  # not required\n</code></pre>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.BasicEvaluatorFunction","title":"<code>BasicEvaluatorFunction = Union[ConfiguredEvaluatorFunction, UnconfiguredEvaluatorFunction]</code>  <code>module-attribute</code>","text":"<p><code>BasicEvaluatorFunction</code> provides a function-based evaluator interface that takes the inferences for all test samples in a test suite and a <code>TestCases</code> as input and computes the corresponding test-sample-level, test-case-level, and test-suite-level metrics (and optionally plots) as output.</p> <p>Example implementation, relying on <code>compute_per_sample</code> and <code>compute_aggregate</code> functions implemented elsewhere:</p> <pre><code>def evaluate(\ntest_samples: List[TestSample],\nground_truths: List[GroundTruth],\ninferences: List[Inference],\ntest_cases: TestCases,\n# configuration: EvaluatorConfiguration,  # uncomment when configuration is used\n) -&gt; EvaluationResults:\n# compute per-sample metrics for each test sample\nper_sample_metrics = [compute_per_sample(gt, inf) for gt, inf in zip(ground_truths, inferences)]\n# compute aggregate metrics across all test cases using `test_cases.iter(...)`\naggregate_metrics: List[Tuple[TestCase, MetricsTestCase]] = []\nfor test_case, *s in test_cases.iter(test_samples, ground_truths, inferences, per_sample_metrics):\n# subset of `test_samples`/`ground_truths`/`inferences`/`test_sample_metrics` in given test case\ntc_test_samples, tc_ground_truths, tc_inferences, tc_per_sample_metrics = s\naggregate_metrics.append((test_case, compute_aggregate(tc_per_sample_metrics)))\n# if desired, compute and add `plots_test_case` and `metrics_test_suite`\nreturn EvaluationResults(\nmetrics_test_sample=list(zip(test_samples, per_sample_metrics)),\nmetrics_test_case=aggregate_metrics,\n)\n</code></pre> <p>The control flow is in general more streamlined than with <code>Evaluator</code>, but requires a couple of assumptions to hold:</p> <ul> <li>Test-sample-level metrics do not vary by test case</li> <li>Ground truths corresponding to a given test sample do not vary by test case</li> </ul> <p>This <code>BasicEvaluatorFunction</code> is provided to the test run at runtime, and is expected to have the following signature:</p> <p>Parameters:</p> Name Type Description Default <code>test_samples</code> <code>List[TestSample]</code> <p>A list of distinct <code>TestSample</code> values that correspond to all test samples in the test run.</p> required <code>ground_truths</code> <code>List[GroundTruth]</code> <p>A list of <code>GroundTruth</code> values corresponding to and sequenced in the same order as <code>test_samples</code>.</p> required <code>inferences</code> <code>List[Inference]</code> <p>A list of <code>Inference</code> values corresponding to and sequenced in the same order as <code>test_samples</code>.</p> required <code>test_cases</code> <code>TestCases</code> <p>An instance of <code>TestCases</code>, used to provide iteration groupings for evaluating test-case-level metrics.</p> required <code>evaluator_configuration</code> <code>EvaluatorConfiguration</code> <p>The <code>EvaluatorConfiguration</code> to use when performing the evaluation. This parameter may be omitted in the function definition for implementations that do not use any configuration object.</p> required <p>Returns:</p> Type Description <code>EvaluationResults</code> <p>An <code>EvaluationResults</code> object tracking the test-sample-level, test-case-level and test-suite-level metrics and plots for the input collection of test samples.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.TestCases","title":"<code>TestCases</code>","text":"<p>Provides an iterator method for grouping test-sample-level metric results with the test cases that they belong to.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.TestCases.iter","title":"<code>iter(test_samples, ground_truths, inferences, metrics_test_sample)</code>  <code>abstractmethod</code>","text":"<p>Matches test sample metrics to the corresponding test cases that they belong to.</p> <p>Parameters:</p> Name Type Description Default <code>test_samples</code> <code>List[TestSample]</code> <p>All unique test samples within the test run, sequenced in the same order as the other parameters.</p> required <code>ground_truths</code> <code>List[GroundTruth]</code> <p>Ground truths corresponding to <code>test_samples</code>, sequenced in the same order.</p> required <code>inferences</code> <code>List[Inference]</code> <p>Inferences corresponding to <code>test_samples</code>, sequenced in the same order.</p> required <code>metrics_test_sample</code> <code>List[MetricsTestSample]</code> <p>Test-sample-level metrics corresponding to <code>test_samples</code>, sequenced in the same order.</p> required <p>Returns:</p> Type Description <code>Iterator[Tuple[TestCase, List[TestSample], List[GroundTruth], List[Inference], List[MetricsTestSample]]]</code> <p>Iterator that groups each test case in the test run to the lists of member test samples, inferences, and test-sample-level metrics.</p> Source code in <code>kolena/workflow/evaluator_function.py</code> <pre><code>@abstractmethod\ndef iter(\nself,\ntest_samples: List[TestSample],\nground_truths: List[GroundTruth],\ninferences: List[Inference],\nmetrics_test_sample: List[MetricsTestSample],\n) -&gt; Iterator[Tuple[TestCase, List[TestSample], List[GroundTruth], List[Inference], List[MetricsTestSample]]]:\n\"\"\"\n    Matches test sample metrics to the corresponding test cases that they belong to.\n    :param test_samples: All unique test samples within the test run, sequenced in the same order as the other\n        parameters.\n    :param ground_truths: Ground truths corresponding to `test_samples`, sequenced in the same order.\n    :param inferences: Inferences corresponding to `test_samples`, sequenced in the same order.\n    :param metrics_test_sample: Test-sample-level metrics corresponding to `test_samples`, sequenced in the\n        same order.\n    :return: Iterator that groups each test case in the test run to the lists of member test samples, inferences,\n        and test-sample-level metrics.\n    \"\"\"\nraise NotImplementedError\n</code></pre>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults","title":"<code>EvaluationResults</code>","text":"<p>A bundle of metrics computed for a test run grouped at the test-sample-level, test-case-level, and test-suite-level. Optionally includes <code>Plot</code>s at the test-case-level.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.metrics_test_sample","title":"<code>metrics_test_sample: List[Tuple[BaseTestSample, BaseMetricsTestSample]]</code>  <code>instance-attribute</code>","text":"<p>Sample-level metrics, extending <code>MetricsTestSample</code>, for every provided test sample.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.metrics_test_case","title":"<code>metrics_test_case: List[Tuple[TestCase, MetricsTestCase]]</code>  <code>instance-attribute</code>","text":"<p>Aggregate metrics, extending <code>MetricsTestCase</code>, computed across each test case yielded from <code>TestCases.iter</code>.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.plots_test_case","title":"<code>plots_test_case: List[Tuple[TestCase, List[Plot]]] = field(default_factory=list)</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optional test-case-level plots.</p>"},{"location":"reference/workflow/evaluator/#kolena.workflow.evaluator_function.EvaluationResults.metrics_test_suite","title":"<code>metrics_test_suite: Optional[MetricsTestSuite] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optional test-suite-level metrics, extending <code>MetricsTestSuite</code>.</p>"},{"location":"reference/workflow/ground_truth/","title":"<code>kolena.workflow.GroundTruth</code>","text":"<p>The ground truth associated with a <code>TestSample</code>. Typically, a ground truth will represent the expected output of a model when given a test sample and will be manually annotated by a human.</p> <p>A <code>TestCase</code> holds a list of test samples (model inputs) paired with ground truths (expected outputs).</p>"},{"location":"reference/workflow/ground_truth/#kolena.workflow.ground_truth.GroundTruth","title":"<code>GroundTruth</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>The ground truth against which a model is evaluated.</p> <p>A test case contains one or more <code>TestSample</code> objects each paired with a ground truth object. During evaluation, these test samples, ground truths, and your model's inferences are provided to the <code>Evaluator</code> implementation.</p> <p>This object may contain any combination of scalars (e.g. <code>str</code>, <code>float</code>), <code>Annotation</code> objects, or lists of these objects.</p> <p>For <code>Composite</code>, each object can contain multiple basic test sample elements. To associate a set of attributes and/or annotations as the ground truth to a target test sample element, declare annotations by extending <code>DataObject</code> and use the same attribute name as used in the <code>Composite</code> test sample.</p> <p>Continue with the example given in <code>Composite</code>, where the <code>FacePairSample</code> test sample type is defined using a pair of images under the <code>source</code> and <code>target</code> members, we can design a corresponding ground truth type with image-level annotations defined in the <code>FaceRegion</code> object:</p> <pre><code>from dataclasses import dataclass\nfrom kolena.workflow import DataObject, GroundTruth\nfrom kolena.workflow.annotation import BoundingBox, Keypoints\n@dataclass(frozen=True)\nclass FaceRegion(DataObject):\nbounding_box: BoundingBox\nkeypoints: Keypoints\n@dataclass(frozen=True)\nclass FacePair(GroundTruth):\nsource: FaceRegion\ntarget: FaceRegion\nis_same_person: bool\n</code></pre> <p>This way, it is clear which bounding boxes and keypoints are associated to which image in the test sample.</p>"},{"location":"reference/workflow/inference/","title":"<code>kolena.workflow.Inference</code>","text":"<p>The output from a <code>Model</code>. In other words, a model is a deterministic transformation from a <code>TestSample</code> to an <code>Inference</code>.</p>"},{"location":"reference/workflow/inference/#kolena.workflow.inference.Inference","title":"<code>Inference</code>","text":"<p>         Bases: <code>DataObject</code></p> <p>The inference produced by a model.</p> <p>Typically the structure of this object closely mirrors the structure of the <code>GroundTruth</code> for a workflow, but this is not a requirement.</p> <p>During evaluation, the <code>TestSample</code> objects, ground truth objects, and these inference objects are provided to the <code>Evaluator</code> implementation to compute metrics.</p> <p>This object may contain any combination of scalars (e.g. <code>str</code>, <code>float</code>), <code>Annotation</code> objects, or lists of these objects.</p> <p>A model processing a <code>Composite</code> test sample can produce an inference result for each of its elements. To associate an inference result to each test sample element, put the attributes and/or annotations inside a <code>DataObject</code> and use the same attribute name as that used in the <code>Composite</code> test sample.</p> <p>Continue with the example given in <code>Composite</code>, where the <code>FacePairSample</code> test sample type is defined using a pair of images under the <code>source</code> and <code>target</code> members, we can design a corresponding inference type with image-level annotations defined in the <code>FaceRegion</code> object:</p> <pre><code>from dataclasses import dataclass\nfrom kolena.workflow import DataObject, Inference\nfrom kolena.workflow.annotation import BoundingBox, Keypoints\n@dataclass(frozen=True)\nclass FaceRegion(DataObject):\nbounding_box: BoundingBox\nkeypoints: Keypoints\n@dataclass(frozen=True)\nclass FacePair(Inference):\nsource: FaceRegion\ntarget: FaceRegion\nsimilarity: float\n</code></pre> <p>This way, it is clear which bounding boxes and keypoints are associated to which image in the test sample.</p>"},{"location":"reference/workflow/metrics/","title":"<code>kolena.workflow.metrics</code>","text":""},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.InferenceMatches","title":"<code>InferenceMatches</code>","text":"<p>         Bases: <code>Generic[GT, Inf]</code></p> <p>The result of <code>match_inferences</code>, providing lists of matches between ground truth and inference objects, unmatched ground truths, and unmatched inferences. After applying some confidence threshold on returned inference objects, <code>InferenceMatches</code> can be used to calculate metrics such as precision and recall.</p> <p>Objects are of type <code>BoundingBox</code> or <code>Polygon</code>, depending on the type of inputs provided to <code>match_inferences</code>.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.InferenceMatches.matched","title":"<code>matched: List[Tuple[GT, Inf]]</code>  <code>instance-attribute</code>","text":"<p>Pairs of matched ground truth and inference objects above the IOU threshold. Considered as true positive detections after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.InferenceMatches.unmatched_gt","title":"<code>unmatched_gt: List[GT]</code>  <code>instance-attribute</code>","text":"<p>Unmatched ground truth objects. Considered as false negatives.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.InferenceMatches.unmatched_inf","title":"<code>unmatched_inf: List[Inf]</code>  <code>instance-attribute</code>","text":"<p>Unmatched inference objects. Considered as false positives after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.MulticlassInferenceMatches","title":"<code>MulticlassInferenceMatches</code>","text":"<p>         Bases: <code>Generic[GT_Multiclass, Inf_Multiclass]</code></p> <p>The result of <code>match_inferences_multiclass</code>, providing lists of matches between ground truth and inference objects, unmatched ground truths, and unmatched inferences.</p> <p>Unmatched ground truths may be matched with an inference of a different class when no inference of its own class is suitable, i.e. a \"confused\" match. <code>MultiClassInferenceMatches</code> can be used to calculate metrics such as precision and recall per class, after applying some confidence threshold on the returned inference objects.</p> <p>Objects are of type <code>LabeledBoundingBox</code> or <code>LabeledPolygon</code>, depending on the type of inputs provided to <code>match_inferences_multiclass</code>.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.MulticlassInferenceMatches.matched","title":"<code>matched: List[Tuple[GT_Multiclass, Inf_Multiclass]]</code>  <code>instance-attribute</code>","text":"<p>Pairs of matched ground truth and inference objects above the IOU threshold. Considered as true positive detections after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.MulticlassInferenceMatches.unmatched_gt","title":"<code>unmatched_gt: List[Tuple[GT_Multiclass, Optional[Inf_Multiclass]]]</code>  <code>instance-attribute</code>","text":"<p>Pairs of unmatched ground truth objects with its confused inference object (i.e. IOU above threshold with mismatching <code>label</code>), if such an inference exists. Considered as false negatives and \"confused\" detections.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics._geometry.MulticlassInferenceMatches.unmatched_inf","title":"<code>unmatched_inf: List[Inf_Multiclass]</code>  <code>instance-attribute</code>","text":"<p>Unmatched inference objects. Considered as false positives after applying some confidence threshold.</p>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.iou","title":"<code>iou(a, b)</code>","text":"<p>Compute the Intersection Over Union (IOU) of two geometries.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Union[BoundingBox, Polygon]</code> <p>The first geometry in computation.</p> required <code>b</code> <code>Union[BoundingBox, Polygon]</code> <p>The second geometry in computation.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The value of the IOU between geometries <code>a</code> and <code>b</code>.</p> Source code in <code>kolena/workflow/metrics/_geometry.py</code> <pre><code>def iou(a: Union[BoundingBox, Polygon], b: Union[BoundingBox, Polygon]) -&gt; float:\n\"\"\"\n    Compute the Intersection Over Union (IOU) of two geometries.\n    :param a: The first geometry in computation.\n    :param b: The second geometry in computation.\n    :return: The value of the IOU between geometries `a` and `b`.\n    \"\"\"\nif isinstance(a, BoundingBox) and isinstance(b, BoundingBox):\nreturn _iou_bbox(a, b)\ndef as_shapely_polygon(obj: Union[BoundingBox, Polygon]) -&gt; ShapelyPolygon:\nif isinstance(obj, BoundingBox):\n(tlx, tly), (brx, bry) = obj.top_left, obj.bottom_right\nreturn make_valid(ShapelyPolygon([(tlx, tly), (brx, tly), (brx, bry), (tlx, bry)]))\nreturn make_valid(ShapelyPolygon(obj.points))\npolygon_a = as_shapely_polygon(a)\npolygon_b = as_shapely_polygon(b)\nunion = polygon_a.union(polygon_b).area\nreturn polygon_a.intersection(polygon_b).area / union if union &gt; 0 else 0\n</code></pre>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.match_inferences","title":"<code>match_inferences(ground_truths, inferences, *, ignored_ground_truths=None, mode='pascal', iou_threshold=0.5)</code>","text":"<p>Matches model inferences with annotated ground truths using the provided configuration.</p> <p>This matcher does not consider labels, which is appropriate for single class object matching. To match with multiple classes (i.e. heeding <code>label</code> classifications), use the multiclass matcher <code>match_inferences_multiclass</code>.</p> <p>Available modes:</p> <ul> <li><code>pascal</code> (PASCAL VOC): For every inference by order of highest confidence, the ground truth of highest IOU is   its match. Multiple inferences are able to match with the same ignored ground truth. See the   PASCAL VOC paper for more information.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[Geometry]</code> <p>A list of <code>BoundingBox</code> or <code>Polygon</code> ground truths.</p> required <code>inferences</code> <code>List[ScoredGeometry]</code> <p>A list of <code>ScoredBoundingBox</code> or <code>ScoredPolygon</code> inferences.</p> required <code>ignored_ground_truths</code> <code>Optional[List[Geometry]]</code> <p>Optionally specify a list of <code>BoundingBox</code> or <code>Polygon</code> ground truths to ignore. These ignored ground truths and any inferences matched with them are omitted from the returned <code>InferenceMatches</code>.</p> <code>None</code> <code>mode</code> <code>Literal['pascal']</code> <p>The matching methodology to use. See available modes above.</p> <code>'pascal'</code> <code>iou_threshold</code> <code>float</code> <p>The IOU (intersection over union, see <code>iou</code>) threshold for valid matches.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>InferenceMatches[GT, Inf]</code> <p><code>InferenceMatches</code> containing the matches (true positives), unmatched ground truths (false negatives) and unmatched inferences (false positives).</p> Source code in <code>kolena/workflow/metrics/_geometry.py</code> <pre><code>def match_inferences(\nground_truths: List[GT],\ninferences: List[Inf],\n*,\nignored_ground_truths: Optional[List[GT]] = None,\nmode: Literal[\"pascal\"] = \"pascal\",\niou_threshold: float = 0.5,\n) -&gt; InferenceMatches[GT, Inf]:\n\"\"\"\n    Matches model inferences with annotated ground truths using the provided configuration.\n    This matcher does not consider labels, which is appropriate for single class object matching. To match with multiple\n    classes (i.e. heeding `label` classifications), use the multiclass matcher\n    [`match_inferences_multiclass`][kolena.workflow.metrics.match_inferences_multiclass].\n    Available modes:\n    - `pascal` (PASCAL VOC): For every inference by order of highest confidence, the ground truth of highest IOU is\n      its match. Multiple inferences are able to match with the same ignored ground truth. See the\n      [PASCAL VOC paper](https://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf) for more information.\n    :param List[Geometry] ground_truths: A list of [`BoundingBox`][kolena.workflow.annotation.BoundingBox] or\n        [`Polygon`][kolena.workflow.annotation.Polygon] ground truths.\n    :param List[ScoredGeometry] inferences: A list of\n        [`ScoredBoundingBox`][kolena.workflow.annotation.ScoredBoundingBox] or\n        [`ScoredPolygon`][kolena.workflow.annotation.ScoredPolygon] inferences.\n    :param Optional[List[Geometry]] ignored_ground_truths: Optionally specify a list of\n        [`BoundingBox`][kolena.workflow.annotation.BoundingBox] or [`Polygon`][kolena.workflow.annotation.Polygon]\n        ground truths to ignore. These ignored ground truths and any inferences matched with them are\n        omitted from the returned [`InferenceMatches`][kolena.workflow.metrics.InferenceMatches].\n    :param mode: The matching methodology to use. See available modes above.\n    :param iou_threshold: The IOU (intersection over union, see [`iou`][kolena.workflow.metrics.iou]) threshold for\n        valid matches.\n    :return: [`InferenceMatches`][kolena.workflow.metrics.InferenceMatches] containing the matches (true positives),\n        unmatched ground truths (false negatives) and unmatched inferences (false positives).\n    \"\"\"\nif mode == \"pascal\":\nreturn _match_inferences_single_class_pascal_voc(\nground_truths,\ninferences,\nignored_ground_truths=ignored_ground_truths,\niou_threshold=iou_threshold,\n)\nraise InputValidationError(f\"Mode: '{mode}' is not a valid mode.\")\n</code></pre>"},{"location":"reference/workflow/metrics/#kolena.workflow.metrics.match_inferences_multiclass","title":"<code>match_inferences_multiclass(ground_truths, inferences, *, ignored_ground_truths=None, mode='pascal', iou_threshold=0.5)</code>","text":"<p>Matches model inferences with annotated ground truths using the provided configuration.</p> <p>This matcher considers <code>label</code> values matching per class. After matching inferences and ground truths with equivalent <code>label</code> values, unmatched inferences and unmatched ground truths are matched once more to identify confused matches, where localization succeeded (i.e. IOU above <code>iou_threshold</code>) but classification failed (i.e. mismatching <code>label</code> values).</p> <p>Available modes:</p> <ul> <li><code>pascal</code> (PASCAL VOC): For every inference by order of highest confidence, the ground truth of highest IOU is   its match. Multiple inferences are able to match with the same ignored ground truth. See the   PASCAL VOC paper for more information.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>ground_truths</code> <code>List[LabeledGeometry]</code> <p>A list of <code>LabeledBoundingBox</code> or <code>LabeledPolygon</code> ground truths.</p> required <code>inferences</code> <code>List[ScoredLabeledGeometry]</code> <p>A list of <code>ScoredLabeledBoundingBox</code> or <code>ScoredLabeledPolygon</code> inferences.</p> required <code>ignored_ground_truths</code> <code>Optional[List[LabeledGeometry]]</code> <p>Optionally specify a list of <code>LabeledBoundingBox</code> or <code>LabeledPolygon</code> ground truths to ignore. These ignored ground truths and any inferences matched with them are omitted from the returned <code>MulticlassInferenceMatches</code>.</p> <code>None</code> <code>mode</code> <code>Literal['pascal']</code> <p>The matching methodology to use. See available modes above.</p> <code>'pascal'</code> <code>iou_threshold</code> <code>float</code> <p>The IOU threshold cutoff for valid matches.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>MulticlassInferenceMatches[GT_Multiclass, Inf_Multiclass]</code> <p><code>MulticlassInferenceMatches</code> containing the matches (true positives), unmatched ground truths (false negatives), and unmatched inferences (false positives).</p> Source code in <code>kolena/workflow/metrics/_geometry.py</code> <pre><code>def match_inferences_multiclass(\nground_truths: List[GT_Multiclass],\ninferences: List[Inf_Multiclass],\n*,\nignored_ground_truths: Optional[List[GT_Multiclass]] = None,\nmode: Literal[\"pascal\"] = \"pascal\",\niou_threshold: float = 0.5,\n) -&gt; MulticlassInferenceMatches[GT_Multiclass, Inf_Multiclass]:\n\"\"\"\n    Matches model inferences with annotated ground truths using the provided configuration.\n    This matcher considers `label` values matching per class. After matching inferences and ground truths with\n    equivalent `label` values, unmatched inferences and unmatched ground truths are matched once more to identify\n    confused matches, where localization succeeded (i.e. IOU above `iou_threshold`) but classification failed (i.e.\n    mismatching `label` values).\n    Available modes:\n    - `pascal` (PASCAL VOC): For every inference by order of highest confidence, the ground truth of highest IOU is\n      its match. Multiple inferences are able to match with the same ignored ground truth. See the\n      [PASCAL VOC paper](https://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf) for more information.\n    :param List[LabeledGeometry] ground_truths: A list of\n        [`LabeledBoundingBox`][kolena.workflow.annotation.LabeledBoundingBox] or\n        [`LabeledPolygon`][kolena.workflow.annotation.LabeledPolygon] ground truths.\n    :param List[ScoredLabeledGeometry] inferences: A list of\n        [`ScoredLabeledBoundingBox`][kolena.workflow.annotation.ScoredLabeledBoundingBox] or\n        [`ScoredLabeledPolygon`][kolena.workflow.annotation.ScoredLabeledPolygon] inferences.\n    :param Optional[List[LabeledGeometry]] ignored_ground_truths: Optionally specify a list of\n        [`LabeledBoundingBox`][kolena.workflow.annotation.LabeledBoundingBox] or\n        [`LabeledPolygon`][kolena.workflow.annotation.LabeledPolygon] ground truths to ignore. These ignored ground\n        truths and any inferences matched with them are omitted from the returned\n        [`MulticlassInferenceMatches`][kolena.workflow.metrics.MulticlassInferenceMatches].\n    :param mode: The matching methodology to use. See available modes above.\n    :param iou_threshold: The IOU threshold cutoff for valid matches.\n    :return:\n        [`MulticlassInferenceMatches`][kolena.workflow.metrics.MulticlassInferenceMatches] containing the matches\n        (true positives), unmatched ground truths (false negatives), and unmatched inferences (false positives).\n    \"\"\"\nmatched: List[Tuple[GT_Multiclass, Inf_Multiclass]] = []\nunmatched_gt: List[GT_Multiclass] = []\nunmatched_inf: List[Inf_Multiclass] = []\ngts_by_class: Dict[str, List[GT_Multiclass]] = defaultdict(list)\ninfs_by_class: Dict[str, List[Inf_Multiclass]] = defaultdict(list)\nignored_gts_by_class: Dict[str, List[GT_Multiclass]] = defaultdict(list)\nall_labels: Set[str] = set()\nif mode == \"pascal\":\nmatching_function = _match_inferences_single_class_pascal_voc\nelse:\nraise InputValidationError(f\"Mode: '{mode}' is not a valid mode.\")\n# collect all unique labels, store gts and infs of the same label together\nfor gt in ground_truths:\ngts_by_class[gt.label].append(gt)\nall_labels.add(gt.label)\nfor inf in inferences:\ninfs_by_class[inf.label].append(inf)\nall_labels.add(inf.label)\nif ignored_ground_truths:\nfor ignored_gt in ignored_ground_truths:\nignored_gts_by_class[ignored_gt.label].append(ignored_gt)\nfor label in sorted(all_labels):\nground_truths_single = gts_by_class[label]\ninferences_single = infs_by_class[label]\nignored_ground_truths_single = ignored_gts_by_class[label]\nsingle_matches: InferenceMatches = matching_function(\nground_truths_single,\ninferences_single,\nignored_ground_truths=ignored_ground_truths_single,\niou_threshold=iou_threshold,\n)\nmatched += single_matches.matched\nunmatched_gt += single_matches.unmatched_gt\nunmatched_inf += single_matches.unmatched_inf\nconfused_matches = matching_function(\nunmatched_gt,\nunmatched_inf,\nignored_ground_truths=ignored_ground_truths,\niou_threshold=iou_threshold,\n)\nconfused = []\nfor gt, inf in confused_matches.matched:\nif gt.label != inf.label:\nconfused.append((gt, inf))\nunmatched_gt.remove(gt)\nreturn MulticlassInferenceMatches(\nmatched=matched,\nunmatched_gt=confused + [(gt, None) for gt in unmatched_gt],\nunmatched_inf=unmatched_inf,\n)\n</code></pre>"},{"location":"reference/workflow/model/","title":"<code>kolena.worfklow.Model</code>","text":""},{"location":"reference/workflow/model/#kolena.workflow.model.Model","title":"<code>Model(name, infer=None, metadata=None)</code>","text":"<p>         Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>The descriptor of a model tested on Kolena.</p> Source code in <code>kolena/workflow/model.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nname: str,\ninfer: Optional[Callable[[TestSample], Inference]] = None,\nmetadata: Optional[Dict[str, Any]] = None,\n):\nif type(self) == Model:\nraise Exception(\"&lt;Model&gt; must be subclassed.\")\ntry:\nloaded = self.load(name, infer)\nif len(loaded.metadata.keys()) &gt; 0 and loaded.metadata != metadata:\nlog.warn(f\"mismatch in model metadata, using loaded metadata (loaded: {loaded.metadata})\")\nexcept NotFoundError:\nloaded = self.create(name, infer, metadata)\nself._populate_from_other(loaded)\n</code></pre>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":"<p>The workflow of this model. Automatically populated when constructing via the model type returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>Unique name of the model.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.metadata","title":"<code>metadata: Dict[str, Any]</code>  <code>instance-attribute</code>","text":"<p>Unstructured metadata associated with the model.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.infer","title":"<code>infer: Optional[Callable[[TestSample], Inference]]</code>  <code>instance-attribute</code>","text":"<p>Function transforming a <code>TestSample</code> for a workflow into an <code>Inference</code> object. Required when using <code>test</code> or <code>TestRun.run</code>.</p>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.create","title":"<code>create(name, infer=None, metadata=None)</code>  <code>classmethod</code>","text":"<p>Create a new model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The unique name of the new model to create.</p> required <code>infer</code> <code>Optional[Callable[[TestSample], Inference]]</code> <p>Optional inference function for this model.</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Optional unstructured metadata to store with this model.</p> <code>None</code> <p>Returns:</p> Type Description <code>Model</code> <p>The newly created model.</p> Source code in <code>kolena/workflow/model.py</code> <pre><code>@classmethod\ndef create(\ncls,\nname: str,\ninfer: Optional[Callable[[TestSample], Inference]] = None,\nmetadata: Optional[Dict[str, Any]] = None,\n) -&gt; \"Model\":\n\"\"\"\n    Create a new model.\n    :param name: The unique name of the new model to create.\n    :param infer: Optional inference function for this model.\n    :param metadata: Optional unstructured metadata to store with this model.\n    :return: The newly created model.\n    \"\"\"\nmetadata = metadata or {}\nrequest = CoreAPI.CreateRequest(name=name, metadata=metadata, workflow=cls.workflow.name)\nres = krequests.post(endpoint_path=API.Path.CREATE.value, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\nobj = cls._from_data_with_infer(from_dict(data_class=CoreAPI.EntityData, data=res.json()), infer)\nlog.info(f\"created model '{name}' ({get_model_url(obj._id)})\")\nreturn obj\n</code></pre>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.load","title":"<code>load(name, infer=None)</code>  <code>classmethod</code>","text":"<p>Load an existing model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the model to load.</p> required <code>infer</code> <code>Optional[Callable[[TestSample], Inference]]</code> <p>Optional inference function for this model.</p> <code>None</code> Source code in <code>kolena/workflow/model.py</code> <pre><code>@classmethod\ndef load(cls, name: str, infer: Optional[Callable[[TestSample], Inference]] = None) -&gt; \"Model\":\n\"\"\"\n    Load an existing model.\n    :param name: The name of the model to load.\n    :param infer: Optional inference function for this model.\n    \"\"\"\nrequest = CoreAPI.LoadByNameRequest(name=name)\nres = krequests.put(endpoint_path=API.Path.LOAD.value, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\nobj = cls._from_data_with_infer(from_dict(data_class=CoreAPI.EntityData, data=res.json()), infer)\nlog.info(f\"loaded model '{name}' ({get_model_url(obj._id)})\")\nreturn obj\n</code></pre>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.load_inferences","title":"<code>load_inferences(test_case)</code>","text":"<p>Load all inferences stored for this model on the provided test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case for which to load inferences.</p> required <p>Returns:</p> Type Description <code>List[Tuple[TestSample, GroundTruth, Inference]]</code> <p>The ground truths and inferences for all test samples in the test case.</p> Source code in <code>kolena/workflow/model.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef load_inferences(self, test_case: TestCase) -&gt; List[Tuple[TestSample, GroundTruth, Inference]]:\n\"\"\"\n    Load all inferences stored for this model on the provided test case.\n    :param test_case: The test case for which to load inferences.\n    :return: The ground truths and inferences for all test samples in the test case.\n    \"\"\"\nreturn list(self.iter_inferences(test_case))\n</code></pre>"},{"location":"reference/workflow/model/#kolena.workflow.model.Model.iter_inferences","title":"<code>iter_inferences(test_case)</code>","text":"<p>Iterate over all inferences stored for this model on the provided test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case over which to iterate inferences.</p> required <p>Returns:</p> Type Description <code>Iterator[Tuple[TestSample, GroundTruth, Inference]]</code> <p>Iterator exposing the ground truths and inferences for all test samples in the test case.</p> Source code in <code>kolena/workflow/model.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef iter_inferences(self, test_case: TestCase) -&gt; Iterator[Tuple[TestSample, GroundTruth, Inference]]:\n\"\"\"\n    Iterate over all inferences stored for this model on the provided test case.\n    :param test_case: The test case over which to iterate inferences.\n    :return: Iterator exposing the ground truths and inferences for all test samples in the test case.\n    \"\"\"\nlog.info(f\"loading inferences from model '{self.name}' on test case '{test_case.name}'\")\nassert_workflows_match(self.workflow.name, test_case.workflow.name)\nfor df_batch in _BatchedLoader.iter_data(\ninit_request=API.LoadInferencesRequest(\nmodel_id=self._id,\ntest_case_id=test_case._id,\nbatch_size=BatchSize.LOAD_SAMPLES.value,\n),\nendpoint_path=API.Path.LOAD_INFERENCES.value,\ndf_class=TestSampleDataFrame,\n):\nfor record in df_batch.itertuples():\ntest_sample = self.workflow.test_sample_type._from_dict(record.test_sample)\nground_truth = self.workflow.ground_truth_type._from_dict(record.ground_truth)\ninference = self.workflow.inference_type._from_dict(record.inference)\nyield test_sample, ground_truth, inference\nlog.info(f\"loaded inferences from model '{self.name}' on test case '{test_case.name}'\")\n</code></pre>"},{"location":"reference/workflow/test_case/","title":"<code>kolena.workflow.TestCase</code>","text":""},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase","title":"<code>TestCase(name, version=None, description=None, test_samples=None, reset=False)</code>","text":"<p>         Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A test case holds a set of images to compute aggregate performance metrics against.</p> Source code in <code>kolena/workflow/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nname: str,\nversion: Optional[int] = None,\ndescription: Optional[str] = None,\ntest_samples: Optional[List[Tuple[TestSample, GroundTruth]]] = None,\nreset: bool = False,\n):\nif type(self) == TestCase:\nraise Exception(\"&lt;TestCase&gt; must be subclassed.\")\nself._validate_test_samples(test_samples)\ntry:\nself._populate_from_other(self.load(name, version))\nif description is not None and self.description != description and not reset:\nlog.warn(\"test case already exists, not updating description when reset=False\")\nif test_samples is not None:\nif self.version &gt; 0 and not reset:\nlog.warn(\"not updating test samples for test case that has already been edited when reset=False\")\nelse:\nself._hydrate(test_samples, description)\nexcept NotFoundError:\nself._populate_from_other(self.create(name, description, test_samples))\nself._freeze()\n</code></pre>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":"<p>The workflow of this test case. Automatically populated when constructing via test case type returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>The unique name of this test case. Cannot be changed after creation.</p>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of this test case. A test case's version is automatically incremented whenever it is edited via <code>TestCase.edit</code>.</p>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form, human-readable description of this test case. Can be edited at any time via <code>TestCase.edit</code>.</p>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.Editor","title":"<code>Editor(description, reset)</code>","text":"Source code in <code>kolena/workflow/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, description: str, reset: bool) -&gt; None:\nself._edits = []\nself._reset = reset\nself._description = description\nself._initial_description = description\n</code></pre>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.Editor.description","title":"<code>description(description)</code>","text":"<p>Update the description of the test case.</p> Source code in <code>kolena/workflow/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef description(self, description: str) -&gt; None:\n\"\"\"Update the description of the test case.\"\"\"\nself._description = description\n</code></pre>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.Editor.add","title":"<code>add(test_sample, ground_truth)</code>","text":"<p>Add a test sample to the test case. When the test sample already exists in the test case, its ground truth is overwritten with the ground truth provided here.</p> <p>Parameters:</p> Name Type Description Default <code>test_sample</code> <code>TestSample</code> <p>The test sample to add.</p> required <code>ground_truth</code> <code>GroundTruth</code> <p>The ground truth for the test sample.</p> required Source code in <code>kolena/workflow/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef add(self, test_sample: TestSample, ground_truth: GroundTruth) -&gt; None:\n\"\"\"\n    Add a test sample to the test case. When the test sample already exists in the test case, its ground truth\n    is overwritten with the ground truth provided here.\n    :param test_sample: The test sample to add.\n    :param ground_truth: The ground truth for the test sample.\n    \"\"\"\nself._edits.append(self._Edit(test_sample, ground_truth=ground_truth))\n</code></pre>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.Editor.remove","title":"<code>remove(test_sample)</code>","text":"<p>Remove a test sample from the test case. Does nothing if the test sample is not in the test case.</p> <p>Parameters:</p> Name Type Description Default <code>test_sample</code> <code>TestSample</code> <p>The test sample to remove.</p> required Source code in <code>kolena/workflow/test_case.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef remove(self, test_sample: TestSample) -&gt; None:\n\"\"\"\n    Remove a test sample from the test case. Does nothing if the test sample is not in the test case.\n    :param test_sample: The test sample to remove.\n    \"\"\"\nself._edits.append(self._Edit(test_sample, remove=True))\n</code></pre>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.create","title":"<code>create(name, description=None, test_samples=None)</code>  <code>classmethod</code>","text":"<p>Create a new test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new test case to create.</p> required <code>description</code> <code>Optional[str]</code> <p>Optional free-form description of the test case to create.</p> <code>None</code> <code>test_samples</code> <code>Optional[List[Tuple[TestSample, GroundTruth]]]</code> <p>Optionally specify a set of test samples and ground truths to populate the test case.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestCase</code> <p>The newly created test case.</p> Source code in <code>kolena/workflow/test_case.py</code> <pre><code>@classmethod\ndef create(\ncls,\nname: str,\ndescription: Optional[str] = None,\ntest_samples: Optional[List[Tuple[TestSample, GroundTruth]]] = None,\n) -&gt; \"TestCase\":\n\"\"\"\n    Create a new test case with the provided name.\n    :param name: The name of the new test case to create.\n    :param description: Optional free-form description of the test case to create.\n    :param test_samples: Optionally specify a set of test samples and ground truths to populate the test case.\n    :return: The newly created test case.\n    \"\"\"\ncls._validate_test_samples(test_samples)\nrequest = CoreAPI.CreateRequest(name=name, description=description or \"\", workflow=cls.workflow.name)\nres = krequests.post(endpoint_path=API.Path.CREATE.value, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\ndata = from_dict(data_class=CoreAPI.EntityData, data=res.json())\nobj = cls._create_from_data(data)\nlog.info(f\"created test case '{name}' (v{obj.version})\")\nif test_samples is not None:\nobj._hydrate(test_samples)\nreturn obj\n</code></pre>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.load","title":"<code>load(name, version=None)</code>  <code>classmethod</code>","text":"<p>Load an existing test case with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test case to load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify a particular version of the test case to load. Defaults to the latest version when unset.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestCase</code> <p>The loaded test case.</p> Source code in <code>kolena/workflow/test_case.py</code> <pre><code>@classmethod\ndef load(cls, name: str, version: Optional[int] = None) -&gt; \"TestCase\":\n\"\"\"\n    Load an existing test case with the provided name.\n    :param name: The name of the test case to load.\n    :param version: Optionally specify a particular version of the test case to load. Defaults to the latest version\n        when unset.\n    :return: The loaded test case.\n    \"\"\"\nrequest = CoreAPI.LoadByNameRequest(name=name, version=version)\nres = krequests.put(endpoint_path=API.Path.LOAD.value, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\ndata = from_dict(data_class=CoreAPI.EntityData, data=res.json())\nlog.info(f\"loaded test case '{name}' (v{data.version})\")\nreturn cls._create_from_data(data)\n</code></pre>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.load_test_samples","title":"<code>load_test_samples()</code>","text":"<p>Load all test samples and ground truths in this test case.</p> Source code in <code>kolena/workflow/test_case.py</code> <pre><code>def load_test_samples(self) -&gt; List[Tuple[TestSample, GroundTruth]]:\n\"\"\"Load all test samples and ground truths in this test case.\"\"\"\nreturn list(self.iter_test_samples())\n</code></pre>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.iter_test_samples","title":"<code>iter_test_samples()</code>","text":"<p>Iterate through all test samples and ground truths in this test case.</p> Source code in <code>kolena/workflow/test_case.py</code> <pre><code>def iter_test_samples(self) -&gt; Iterator[Tuple[TestSample, GroundTruth]]:\n\"\"\"Iterate through all test samples and ground truths in this test case.\"\"\"\nlog.info(f\"loading test samples in test case '{self.name}' (v{self.version})\")\ntest_sample_type = self.workflow.test_sample_type\nground_truth_type = self.workflow.ground_truth_type\ninit_request = CoreAPI.InitLoadContentsRequest(batch_size=BatchSize.LOAD_SAMPLES.value, test_case_id=self._id)\nfor df in _BatchedLoader.iter_data(\ninit_request=init_request,\nendpoint_path=API.Path.INIT_LOAD_TEST_SAMPLES.value,\ndf_class=TestSampleDataFrame,\n):\nhas_metadata = \"test_sample_metadata\" in df.columns\nfor record in df.itertuples():\nmetadata_field = record.test_sample_metadata if has_metadata else {}\ntest_sample = test_sample_type._from_dict({**record.test_sample, _METADATA_KEY: metadata_field})\nground_truth = ground_truth_type._from_dict(record.ground_truth)\nyield test_sample, ground_truth\nlog.info(f\"loaded test samples in test case '{self.name}' (v{self.version})\")\n</code></pre>"},{"location":"reference/workflow/test_case/#kolena.workflow.test_case.TestCase.edit","title":"<code>edit(reset=False)</code>","text":"<p>Edit this test case in a context:</p> <pre><code>with test_case.edit() as editor:\n# perform as many editing actions as desired\neditor.add(...)\neditor.remove(...)\n</code></pre> <p>Changes are committed to the Kolena platform when the context is exited.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>Clear any and all test samples currently in the test case.</p> <code>False</code> Source code in <code>kolena/workflow/test_case.py</code> <pre><code>@contextmanager\ndef edit(self, reset: bool = False) -&gt; Iterator[Editor]:\n\"\"\"\n    Edit this test case in a context:\n    ```python\n    with test_case.edit() as editor:\n        # perform as many editing actions as desired\n        editor.add(...)\n        editor.remove(...)\n    ```\n    Changes are committed to the Kolena platform when the context is exited.\n    :param reset: Clear any and all test samples currently in the test case.\n    \"\"\"\neditor = self.Editor(self.description, reset)\nyield editor\nif not editor._edited():\nreturn\nlog.info(f\"editing test case '{self.name}' (v{self.version})\")\ninit_response = init_upload()\ndf_serialized = editor._to_data_frame().as_serializable()\nupload_data_frame(df=df_serialized, batch_size=BatchSize.UPLOAD_RECORDS.value, load_uuid=init_response.uuid)\nrequest = CoreAPI.CompleteEditRequest(\ntest_case_id=self._id,\ncurrent_version=self.version,\ndescription=editor._description,\nreset=editor._reset,\nuuid=init_response.uuid,\n)\ncomplete_res = krequests.put(\nendpoint_path=API.Path.COMPLETE_EDIT.value,\ndata=json.dumps(dataclasses.asdict(request)),\n)\nkrequests.raise_for_status(complete_res)\ntest_case_data = from_dict(data_class=CoreAPI.EntityData, data=complete_res.json())\nself._populate_from_other(self._create_from_data(test_case_data))\nlog.success(f\"edited test case '{self.name}' (v{self.version})\")\n</code></pre>"},{"location":"reference/workflow/test_run/","title":"<code>kolena.workflow.test</code>","text":""},{"location":"reference/workflow/test_run/#kolena.workflow.test_run.TestRun","title":"<code>TestRun(model, test_suite, evaluator=None, configurations=None, reset=False)</code>","text":"<p>         Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A <code>Model</code> tested on a <code>TestSuite</code> using a specific <code>Evaluator</code> implementation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model being tested.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>evaluator</code> <code>Union[Evaluator, BasicEvaluatorFunction, None]</code> <p>An optional evaluator implementation. Requires a previously configured server-side evaluator to default to if omitted. (Please see <code>BasicEvaluatorFunction</code> for type definition.)</p> <code>None</code> <code>configurations</code> <code>Optional[List[EvaluatorConfiguration]]</code> <p>a list of configurations to use when running the evaluator.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>overwrites existing inferences if set.</p> <code>False</code> Source code in <code>kolena/workflow/test_run.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nmodel: Model,\ntest_suite: TestSuite,\nevaluator: Union[Evaluator, BasicEvaluatorFunction, None] = None,\nconfigurations: Optional[List[EvaluatorConfiguration]] = None,\nreset: bool = False,\n):\nif configurations is None:\nconfigurations = []\nif model.workflow != test_suite.workflow:\nraise WorkflowMismatchError(\nf\"model workflow ({model.workflow}) does not match test suite workflow ({test_suite.workflow})\",\n)\nif reset:\nlog.warn(\"overwriting existing inferences from this model (reset=True)\")\nelse:\nlog.info(\"not overwriting any existing inferences from this model (reset=False)\")\nself.model = model\nself.test_suite = test_suite\nself.evaluator = evaluator\nself.configurations = self.evaluator.configurations if isinstance(evaluator, Evaluator) else configurations\nself.reset = reset\nevaluator_display_name = (\nNone\nif evaluator is None\nelse evaluator.display_name()\nif isinstance(evaluator, Evaluator)\nelse evaluator.__name__\n)\napi_configurations = (\n[_maybe_evaluator_configuration_to_api(config) for config in self.configurations]\nif self.configurations is not None\nelse None\n)\nrequest = API.CreateOrRetrieveRequest(\nmodel_id=model._id,\ntest_suite_id=test_suite._id,\nevaluator=evaluator_display_name,\nconfigurations=api_configurations,\n)\nres = krequests.put(\nendpoint_path=API.Path.CREATE_OR_RETRIEVE.value,\ndata=json.dumps(dataclasses.asdict(request)),\n)\nkrequests.raise_for_status(res)\nresponse = from_dict(data_class=API.CreateOrRetrieveResponse, data=res.json())\nself._id = response.test_run_id\nself._freeze()\n</code></pre>"},{"location":"reference/workflow/test_run/#kolena.workflow.test_run.TestRun.run","title":"<code>run()</code>","text":"<p>Run the testing process, first extracting inferences for all test samples in the test suite then performing evaluation.</p> Source code in <code>kolena/workflow/test_run.py</code> <pre><code>def run(self) -&gt; None:\n\"\"\"\n    Run the testing process, first extracting inferences for all test samples in the test suite then performing\n    evaluation.\n    \"\"\"\ntry:\ninferences = []\nfor ts in log.progress_bar(self.iter_test_samples(), desc=\"performing inference\"):\nif self.model.infer is None:  # only fail when `infer` is necessary\nraise ValueError(\"model must implement `infer`\")\ninferences.append((ts, self.model.infer(ts)))\nif len(inferences) &gt; 0:\nlog.success(f\"performed inference on {len(inferences)} test samples\")\nlog.info(\"uploading inferences\")\nself.upload_inferences(inferences)\nself.evaluate()\nexcept Exception as e:\nreport_crash(self._id, API.Path.MARK_CRASHED.value)\nraise e\n</code></pre>"},{"location":"reference/workflow/test_run/#kolena.workflow.test_run.TestRun.load_test_samples","title":"<code>load_test_samples()</code>","text":"<p>Load the test samples in the test suite that do not yet have inferences uploaded.</p> <p>Returns:</p> Type Description <code>List[TestSample]</code> <p>a list of all test samples in the test suite still requiring inferences.</p> Source code in <code>kolena/workflow/test_run.py</code> <pre><code>def load_test_samples(self) -&gt; List[TestSample]:\n\"\"\"\n    Load the test samples in the test suite that do not yet have inferences uploaded.\n    :return: a list of all test samples in the test suite still requiring inferences.\n    \"\"\"\nreturn list(self.iter_test_samples())\n</code></pre>"},{"location":"reference/workflow/test_run/#kolena.workflow.test_run.TestRun.iter_test_samples","title":"<code>iter_test_samples()</code>","text":"<p>Iterate through the test samples in the test suite that do not yet have inferences uploaded.</p> <p>Returns:</p> Type Description <code>Iterator[TestSample]</code> <p>an iterator over each test sample still requiring inferences.</p> Source code in <code>kolena/workflow/test_run.py</code> <pre><code>def iter_test_samples(self) -&gt; Iterator[TestSample]:\n\"\"\"\n    Iterate through the test samples in the test suite that do not yet have inferences uploaded.\n    :return: an iterator over each test sample still requiring inferences.\n    \"\"\"\ntest_sample_type = self.model.workflow.test_sample_type\nfor df_batch in self._iter_test_samples_batch():\nfor record in df_batch.itertuples():\nyield test_sample_type._from_dict(record.test_sample)\n</code></pre>"},{"location":"reference/workflow/test_run/#kolena.workflow.test_run.TestRun.upload_inferences","title":"<code>upload_inferences(inferences)</code>","text":"<p>Upload inferences from a model.</p> <p>Parameters:</p> Name Type Description Default <code>inferences</code> <code>List[Tuple[TestSample, Inference]]</code> <p>the inferences, paired with their corresponding test samples, to upload.</p> required Source code in <code>kolena/workflow/test_run.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef upload_inferences(self, inferences: List[Tuple[TestSample, Inference]]) -&gt; None:\n\"\"\"\n    Upload inferences from a model.\n    :param inferences: the inferences, paired with their corresponding test samples, to upload.\n    \"\"\"\nif len(inferences) == 0:\nreturn\ninference_dicts = [(ts._to_dict(), inf._to_dict()) for ts, inf in inferences]\ndf = pd.DataFrame(inference_dicts, columns=[\"test_sample\", \"inference\"])\ndf_validated = TestSampleDataFrame(validate_df_schema(df, TestSampleDataFrameSchema, trusted=True))\ndf_serializable = df_validated.as_serializable()\ninit_response = init_upload()\nupload_data_frame_chunk(df_serializable, init_response.uuid)\nrequest = API.UploadInferencesRequest(uuid=init_response.uuid, test_run_id=self._id, reset=self.reset)\nres = krequests.put(\nendpoint_path=API.Path.UPLOAD_INFERENCES.value,\ndata=json.dumps(dataclasses.asdict(request)),\n)\nkrequests.raise_for_status(res)\n</code></pre>"},{"location":"reference/workflow/test_run/#kolena.workflow.test_run.TestRun.evaluate","title":"<code>evaluate()</code>","text":"<p>Perform evaluation by computing metrics for individual test samples, in aggregate across test cases, and across the complete test suite at each :class:<code>kolena.workflow.EvaluatorConfiguration</code>.</p> Source code in <code>kolena/workflow/test_run.py</code> <pre><code>def evaluate(self) -&gt; None:\n\"\"\"\n    Perform evaluation by computing metrics for individual test samples, in aggregate across test cases, and across\n    the complete test suite at each :class:`kolena.workflow.EvaluatorConfiguration`.\n    \"\"\"\nif self.evaluator is None:\nlog.info(\"commencing server side metrics evaluation\")\nself._start_server_side_evaluation()\nreturn\n# TODO: assert that testing is complete?\nt0 = time.time()\nlog.info(\"commencing evaluation\")\nif isinstance(self.evaluator, Evaluator):\nself._perform_evaluation(self.evaluator)\nelse:\nself._perform_streamlined_evaluation(self.evaluator)\nlog.success(f\"completed evaluation in {time.time() - t0:0.1f} seconds\")\nlog.success(f\"results: {get_results_url(self.model.workflow.name, self.model._id, self.test_suite._id)}\")\n</code></pre>"},{"location":"reference/workflow/test_run/#kolena.workflow.test_run.test","title":"<code>test(model, test_suite, evaluator=None, configurations=None, reset=False)</code>","text":"<p>Test a <code>Model</code> on a <code>TestSuite</code> using a specific <code>Evaluator</code> implementation.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Model</code> <p>The model being tested, implementing the <code>infer</code> method.</p> required <code>test_suite</code> <code>TestSuite</code> <p>The test suite on which to test the model.</p> required <code>evaluator</code> <code>Union[Evaluator, BasicEvaluatorFunction, None]</code> <p>An optional evaluator implementation. Requires a previously configured server-side evaluator to default to if omitted. (Please see <code>BasicEvaluatorFunction</code> for type definition.)</p> <code>None</code> <code>configurations</code> <code>Optional[List[EvaluatorConfiguration]]</code> <p>A list of configurations to use when running the evaluator.</p> <code>None</code> <code>reset</code> <code>bool</code> <p>Overwrites existing inferences if set.</p> <code>False</code> Source code in <code>kolena/workflow/test_run.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef test(\nmodel: Model,\ntest_suite: TestSuite,\nevaluator: Union[Evaluator, BasicEvaluatorFunction, None] = None,\nconfigurations: Optional[List[EvaluatorConfiguration]] = None,\nreset: bool = False,\n) -&gt; None:\n\"\"\"\n    Test a [`Model`][kolena.workflow.Model] on a [`TestSuite`][kolena.workflow.TestSuite] using a specific\n    [`Evaluator`][kolena.workflow.Evaluator] implementation.\n    :param model: The model being tested, implementing the `infer` method.\n    :param test_suite: The test suite on which to test the model.\n    :param evaluator: An optional evaluator implementation.\n        Requires a previously configured server-side evaluator to default to if omitted.\n        (Please see [`BasicEvaluatorFunction`][kolena.workflow.evaluator_function.BasicEvaluatorFunction] for type\n        definition.)\n    :param configurations: A list of configurations to use when running the evaluator.\n    :param reset: Overwrites existing inferences if set.\n    \"\"\"\nTestRun(model, test_suite, evaluator, configurations, reset).run()\n</code></pre>"},{"location":"reference/workflow/test_sample/","title":"<code>kolena.workflow.TestSample</code>","text":"<p>Test samples are the inputs to your models when testing.</p> <p>For example, for a model that processes specific regions within a larger image, its test sample may be defined:</p> <pre><code>from dataclasses import dataclass\nfrom kolena.workflow import Image\nfrom kolena.workflow.annotation import BoundingBox\n@dataclass(frozen=True)\nclass ImageWithRegion(Image):\nregion: BoundingBox\nexample = ImageWithRegion(\nlocator=\"s3://my-bucket/example-image.png\",  # field from Image base class\nregion=BoundingBox(top_left=(0, 0), bottom_right=(100, 100)),\n)\n</code></pre>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.Metadata","title":"<code>Metadata = Dict[str, Union[None, StrictStr, StrictFloat, StrictInt, StrictBool, str, float, int, bool, List[Union[None, StrictStr, StrictFloat, StrictInt, StrictBool, str, float, int, bool]]]]</code>  <code>module-attribute</code>","text":"<p>Type of the <code>metadata</code> field that can be included on <code>TestSample</code> definitions. String (<code>str</code>) keys and scalar values (<code>int</code>, <code>float</code>, <code>str</code>, <code>bool</code>, <code>None</code>) as well as scalar list values are permitted.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.TestSample","title":"<code>TestSample</code>","text":"<p>         Bases: <code>TypedDataObject[_TestSampleType]</code></p> <p>The inputs to a model.</p> <p>Test samples can be customized as necessary for a workflow by extending this class or one of the built-in test sample types.</p> <p>Extensions to the <code>TestSample</code> class may define a <code>metadata</code> field of type <code>Metadata</code> containing a dictionary of scalar properties associated with the test sample, intended for use when sorting or filtering test samples.</p> <p>Kolena handles the <code>metadata</code> field differently from other test sample fields. Updates to the <code>metadata</code> object for a given test sample are merged with previously uploaded metadata. As such, <code>metadata</code> for a given test sample within a test case is not immutable, and should not be relied on when an implementation of <code>Model</code> computes inferences, or when an implementation of <code>Evaluator</code> evaluates metrics.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.Composite","title":"<code>Composite</code>","text":"<p>         Bases: <code>TestSample</code></p> <p>A test sample composed of multiple basic <code>TestSample</code> elements.</p> <p>An example application would be each test sample is a pair of face images, and the goal is to predict whether the two images are of the same person. For this use-case the test sample can be defined as:</p> <pre><code>class FacePairSample(Composite):\nsource: Image\ntarget: Image\n</code></pre> <p>To facilitate visualization for this kind of use cases, see usage of <code>GroundTruth</code> and <code>Inference</code>.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.Image","title":"<code>Image</code>","text":"<p>         Bases: <code>TestSample</code></p> <p>An image located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.ImagePair","title":"<code>ImagePair</code>","text":"<p>         Bases: <code>Composite</code></p> <p>Two images.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.Text","title":"<code>Text</code>","text":"<p>         Bases: <code>TestSample</code></p> <p>A text snippet.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.ImageText","title":"<code>ImageText</code>","text":"<p>         Bases: <code>Composite</code></p> <p>An image paired with a text snippet.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.BaseVideo","title":"<code>BaseVideo</code>","text":"<p>         Bases: <code>TestSample</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.BaseVideo.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the video file.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.Video","title":"<code>Video</code>","text":"<p>         Bases: <code>BaseVideo</code></p> <p>A video clip located in a cloud bucket or served at a URL.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.Video.thumbnail","title":"<code>thumbnail: Optional[ImageAsset] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally provide asset locator for custom video thumbnail.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.Video.start","title":"<code>start: Optional[float] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally specify start time of video snippet, in seconds.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.Video.end","title":"<code>end: Optional[float] = None</code>  <code>instance-attribute</code> <code>class-attribute</code>","text":"<p>Optionally specify end time of video snippet, in seconds.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.Document","title":"<code>Document</code>","text":"<p>         Bases: <code>TestSample</code></p> <p>A remotely linked document, e.g. PDF or TXT file.</p>"},{"location":"reference/workflow/test_sample/#kolena.workflow.test_sample.Document.locator","title":"<code>locator: str</code>  <code>instance-attribute</code>","text":"<p>URL (e.g. S3, HTTPS) of the document.</p>"},{"location":"reference/workflow/test_suite/","title":"<code>kolena.workflow.TestSuite</code>","text":""},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite","title":"<code>TestSuite(name, version=None, description=None, test_cases=None, reset=False, tags=None)</code>","text":"<p>         Bases: <code>Frozen</code>, <code>WithTelemetry</code></p> <p>A test suite groups together one or more test cases.</p> Source code in <code>kolena/workflow/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(\nself,\nname: str,\nversion: Optional[int] = None,\ndescription: Optional[str] = None,\ntest_cases: Optional[List[TestCase]] = None,\nreset: bool = False,\ntags: Optional[Set[str]] = None,\n):\nself._validate_workflow()\nself._validate_test_cases(test_cases)\ntry:\nother = self.load(name, version)\nexcept NotFoundError:\nother = self.create(name, description, test_cases, tags)\nself._populate_from_other(other)\nshould_update_test_cases = test_cases is not None and test_cases != self.test_cases\ncan_update_test_cases = reset or self.version == 0\nif should_update_test_cases and not can_update_test_cases:\nlog.warn(f\"reset=False, not updating test cases on test suite '{self.name}' (v{self.version})\")\nto_update = [\n*([\"test cases\"] if should_update_test_cases and can_update_test_cases else []),\n*([\"description\"] if description is not None and description != self.description else []),\n*([\"tags\"] if tags is not None and tags != self.tags else []),\n]\nif len(to_update) &gt; 0:\nlog.info(f\"updating {', '.join(to_update)} on test suite '{self.name}' (v{self.version})\")\nupdated_test_cases = test_cases or self.test_cases if can_update_test_cases else self.test_cases\nself._hydrate(updated_test_cases, description=description, tags=tags)\nself._freeze()\n</code></pre>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.workflow","title":"<code>workflow: Workflow</code>  <code>instance-attribute</code>","text":"<p>The workflow of this test suite. Automatically populated when constructing via test suite type returned from <code>define_workflow</code>.</p>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":"<p>The unique name of this test suite.</p>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.version","title":"<code>version: int</code>  <code>instance-attribute</code>","text":"<p>The version of this test suite. A test suite's version is automatically incremented whenever it is edited via <code>TestSuite.edit</code>.</p>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.description","title":"<code>description: str</code>  <code>instance-attribute</code>","text":"<p>Free-form, human-readable description of this test suite. Can be edited at any time via <code>TestSuite.edit</code>.</p>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.test_cases","title":"<code>test_cases: List[TestCase]</code>  <code>instance-attribute</code>","text":"<p>The <code>TestCase</code> objects belonging to this test suite.</p>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.tags","title":"<code>tags: Set[str]</code>  <code>instance-attribute</code>","text":"<p>The tags associated with this test suite.</p>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.Editor","title":"<code>Editor(test_cases, description, tags, reset)</code>","text":"Source code in <code>kolena/workflow/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef __init__(self, test_cases: List[TestCase], description: str, tags: Set[str], reset: bool):\nself._test_cases = test_cases if not reset else []\nself._reset = reset\nself._description = description\nself._initial_test_case_ids = [tc._id for tc in test_cases]\nself._initial_description = description\nself._initial_tags = tags\nself.tags = tags\n</code></pre>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.Editor.description","title":"<code>description(description)</code>","text":"<p>Update the description of the test suite.</p> Source code in <code>kolena/workflow/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef description(self, description: str) -&gt; None:\n\"\"\"Update the description of the test suite.\"\"\"\nself._description = description\n</code></pre>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.Editor.add","title":"<code>add(test_case)</code>","text":"<p>Add a test case to this test suite. If a different version of the test case already exists in this test suite, it is replaced.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case to add to the test suite.</p> required Source code in <code>kolena/workflow/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef add(self, test_case: TestCase) -&gt; None:\n\"\"\"\n    Add a test case to this test suite. If a different version of the test case already exists in this test\n    suite, it is replaced.\n    :param test_case: The test case to add to the test suite.\n    \"\"\"\nself._test_cases = [*(tc for tc in self._test_cases if tc.name != test_case.name), test_case]\n</code></pre>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.Editor.remove","title":"<code>remove(test_case)</code>","text":"<p>Remove a test case from this test suite. Does nothing if the test case is not in the test suite.</p> <p>Parameters:</p> Name Type Description Default <code>test_case</code> <code>TestCase</code> <p>The test case to remove.</p> required Source code in <code>kolena/workflow/test_suite.py</code> <pre><code>@validate_arguments(config=ValidatorConfig)\ndef remove(self, test_case: TestCase) -&gt; None:\n\"\"\"\n    Remove a test case from this test suite. Does nothing if the test case is not in the test suite.\n    :param test_case: The test case to remove.\n    \"\"\"\nself._test_cases = [tc for tc in self._test_cases if tc.name != test_case.name]\n</code></pre>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.create","title":"<code>create(name, description=None, test_cases=None, tags=None)</code>  <code>classmethod</code>","text":"<p>Create a new test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the new test suite to create.</p> required <code>description</code> <code>Optional[str]</code> <p>Optional free-form description of the test suite to create.</p> <code>None</code> <code>test_cases</code> <code>Optional[List[TestCase]]</code> <p>Optionally specify a list of test cases to populate the test suite.</p> <code>None</code> <code>tags</code> <code>Optional[Set[str]]</code> <p>Optionally specify a set of tags to attach to the test suite.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestSuite</code> <p>The newly created test suite.</p> Source code in <code>kolena/workflow/test_suite.py</code> <pre><code>@classmethod\ndef create(\ncls,\nname: str,\ndescription: Optional[str] = None,\ntest_cases: Optional[List[TestCase]] = None,\ntags: Optional[Set[str]] = None,\n) -&gt; \"TestSuite\":\n\"\"\"\n    Create a new test suite with the provided name.\n    :param name: The name of the new test suite to create.\n    :param description: Optional free-form description of the test suite to create.\n    :param test_cases: Optionally specify a list of test cases to populate the test suite.\n    :param tags: Optionally specify a set of tags to attach to the test suite.\n    :return: The newly created test suite.\n    \"\"\"\ncls._validate_workflow()\ncls._validate_test_cases(test_cases)\nrequest = CoreAPI.CreateRequest(name=name, description=description or \"\", workflow=cls.workflow.name, tags=tags)\nres = krequests.post(endpoint_path=API.Path.CREATE, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\ndata = from_dict(data_class=CoreAPI.EntityData, data=res.json())\nobj = cls._create_from_data(data)\nlog.info(f\"created test suite '{name}' (v{obj.version}) ({get_test_suite_url(obj._id)})\")\nif test_cases is not None:\nobj._hydrate(test_cases)\nreturn obj\n</code></pre>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.load","title":"<code>load(name, version=None)</code>  <code>classmethod</code>","text":"<p>Load an existing test suite with the provided name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the test suite to load.</p> required <code>version</code> <code>Optional[int]</code> <p>Optionally specify a particular version of the test suite to load. Defaults to the latest version when unset.</p> <code>None</code> <p>Returns:</p> Type Description <code>TestSuite</code> <p>The loaded test suite.</p> Source code in <code>kolena/workflow/test_suite.py</code> <pre><code>@classmethod\ndef load(cls, name: str, version: Optional[int] = None) -&gt; \"TestSuite\":\n\"\"\"\n    Load an existing test suite with the provided name.\n    :param name: The name of the test suite to load.\n    :param version: Optionally specify a particular version of the test suite to load. Defaults to the latest\n        version when unset.\n    :return: The loaded test suite.\n    \"\"\"\ncls._validate_workflow()\nrequest = CoreAPI.LoadByNameRequest(name=name, version=version)\nres = krequests.put(endpoint_path=API.Path.LOAD, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\ndata = from_dict(data_class=CoreAPI.EntityData, data=res.json())\nobj = cls._create_from_data(data)\nlog.info(f\"loaded test suite '{name}' (v{obj.version}) ({get_test_suite_url(obj._id)})\")\nreturn obj\n</code></pre>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.load_all","title":"<code>load_all(*, tags=None)</code>  <code>classmethod</code>","text":"<p>Load the latest version of all non-archived test suites with this workflow.</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>Optional[Set[str]]</code> <p>Optionally specify a set of tags to apply as a filter. The loaded test suites will include only test suites with tags matching each of these specified tags, i.e. <code>test_suite.tags.intersection(tags) == tags</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[TestSuite]</code> <p>The latest version of all non-archived test suites, with matching tags when specified.</p> Source code in <code>kolena/workflow/test_suite.py</code> <pre><code>@classmethod\ndef load_all(cls, *, tags: Optional[Set[str]] = None) -&gt; List[\"TestSuite\"]:\n\"\"\"\n    Load the latest version of all non-archived test suites with this workflow.\n    :param tags: Optionally specify a set of tags to apply as a filter. The loaded test suites will include only\n        test suites with tags matching each of these specified tags, i.e.\n        `test_suite.tags.intersection(tags) == tags`.\n    :return: The latest version of all non-archived test suites, with matching tags when specified.\n    \"\"\"\ncls._validate_workflow()\nrequest = CoreAPI.LoadAllRequest(workflow=cls.workflow.name, tags=tags)\nres = krequests.put(endpoint_path=API.Path.LOAD_ALL.value, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\ndata = from_dict(data_class=CoreAPI.LoadAllResponse, data=res.json())\nobjs = [cls._create_from_data(test_suite) for test_suite in data.test_suites]\ntags_quoted = {f\"'{t}'\" for t in tags or {}}\ntags_message = f\" with tag{'s' if len(tags) &gt; 1 else ''} {', '.join(tags_quoted)}\" if tags else \"\"\nlog.info(f\"loaded {len(objs)} '{cls.workflow.name}' test suites{tags_message}\")\nreturn objs\n</code></pre>"},{"location":"reference/workflow/test_suite/#kolena.workflow.test_suite.TestSuite.edit","title":"<code>edit(reset=False)</code>","text":"<p>Edit this test suite in a context:</p> <pre><code>with test_suite.edit() as editor:\n# perform as many editing actions as desired\neditor.add(...)\neditor.remove(...)\n</code></pre> <p>Changes are committed to the Kolena platform when the context is exited.</p> <p>Parameters:</p> Name Type Description Default <code>reset</code> <code>bool</code> <p>Clear any and all test cases currently in the test suite.</p> <code>False</code> Source code in <code>kolena/workflow/test_suite.py</code> <pre><code>@contextmanager\ndef edit(self, reset: bool = False) -&gt; Iterator[Editor]:\n\"\"\"\n    Edit this test suite in a context:\n    ```python\n    with test_suite.edit() as editor:\n        # perform as many editing actions as desired\n        editor.add(...)\n        editor.remove(...)\n    ```\n    Changes are committed to the Kolena platform when the context is exited.\n    :param reset: Clear any and all test cases currently in the test suite.\n    \"\"\"\neditor = self.Editor(self.test_cases, self.description, self.tags, reset)\nyield editor\nif not editor._edited():\nreturn\nlog.info(f\"editing test suite '{self.name}' (v{self.version})\")\nrequest = CoreAPI.EditRequest(\ntest_suite_id=self._id,\ncurrent_version=self.version,\nname=self.name,\ndescription=editor._description,\ntest_case_ids=[tc._id for tc in editor._test_cases],\ntags=list(editor.tags),\n)\nres = krequests.post(endpoint_path=API.Path.EDIT, data=json.dumps(dataclasses.asdict(request)))\nkrequests.raise_for_status(res)\ntest_suite_data = from_dict(data_class=CoreAPI.EntityData, data=res.json())\nself._populate_from_other(self._create_from_data(test_suite_data))\nlog.success(f\"edited test suite '{self.name}' (v{self.version}) ({get_test_suite_url(self._id)})\")\n</code></pre>"}]}